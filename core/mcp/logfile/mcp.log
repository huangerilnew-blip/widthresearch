2026-02-22 20:37:57,026 - __main__ - INFO - call_tool: name=wikipedia_search, args={'query': 'LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些'}
2026-02-22 20:37:57,027 - __main__ - INFO - handle_search: searcher=WikipediaSearcher, query=LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些, search_type=None
2026-02-22 20:37:57,073 - __main__ - INFO - call_tool: name=tavily_search, args={'query': 'LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理'}
2026-02-22 20:37:57,078 - __main__ - INFO - handle_search: searcher=TavilySearch, query=LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理, search_type=None
2026-02-22 20:37:57,148 - __main__ - INFO - call_tool: name=tavily_search, args={'query': 'LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些'}
2026-02-22 20:37:57,148 - __main__ - INFO - handle_search: searcher=TavilySearch, query=LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些, search_type=None
2026-02-22 20:37:57,160 - __main__ - INFO - call_tool: name=wikipedia_search, args={'query': 'LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理'}
2026-02-22 20:37:57,166 - __main__ - INFO - handle_search: searcher=WikipediaSearcher, query=LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理, search_type=None
2026-02-22 20:37:57,243 - __main__ - INFO - call_tool: name=exa_context_search, args={'query': 'LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复'}
2026-02-22 20:37:57,243 - __main__ - INFO - handle_search: searcher=ExaSearcherContext, query=LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复, search_type=None
2026-02-22 20:37:57,282 - __main__ - INFO - call_tool: name=exa_context_search, args={'query': 'LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些'}
2026-02-22 20:37:57,282 - __main__ - INFO - handle_search: searcher=ExaSearcherContext, query=LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些, search_type=None
2026-02-22 20:37:57,397 - __main__ - INFO - call_tool: name=exa_context_search, args={'query': 'LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理'}
2026-02-22 20:37:57,398 - __main__ - INFO - handle_search: searcher=ExaSearcherContext, query=LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理, search_type=None
2026-02-22 20:37:57,398 - __main__ - INFO - call_tool: name=wikipedia_search, args={'query': 'LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复'}
2026-02-22 20:37:57,398 - __main__ - INFO - handle_search: searcher=WikipediaSearcher, query=LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复, search_type=None
2026-02-22 20:37:57,460 - __main__ - INFO - call_tool: name=tavily_search, args={'query': 'LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复'}
2026-02-22 20:37:57,461 - __main__ - INFO - handle_search: searcher=TavilySearch, query=LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复, search_type=None
2026-02-22 20:37:57,671 - __main__ - WARNING - handle_search: returned=0 for query=LangGraph 中人机协作模式和错误重试策略的最佳实践有哪些
2026-02-22 20:37:57,672 - __main__ - INFO - call_tool payload: source_tool=wikipedia_search, result_type=papers, count=0
2026-02-22 20:37:57,672 - __main__ - INFO - call_tool: name=wikipedia_search, result_type=papers, count=0
2026-02-22 20:37:57,905 - __main__ - WARNING - handle_search: returned=0 for query=LangGraph 的内存架构如何支持短期记忆和跨会话长期记忆的协同管理
2026-02-22 20:37:57,905 - __main__ - INFO - call_tool payload: source_tool=wikipedia_search, result_type=papers, count=0
2026-02-22 20:37:57,905 - __main__ - INFO - call_tool: name=wikipedia_search, result_type=papers, count=0
2026-02-22 20:37:58,099 - __main__ - WARNING - handle_search: returned=0 for query=LangGraph 的状态持久化机制和检查点策略如何实现长期运行的工作流恢复
2026-02-22 20:37:58,099 - __main__ - INFO - call_tool payload: source_tool=wikipedia_search, result_type=papers, count=0
2026-02-22 20:37:58,099 - __main__ - INFO - call_tool: name=wikipedia_search, result_type=papers, count=0
2026-02-22 20:37:58,521 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:37:58,521 - __main__ - INFO - call_tool payload: source_tool=exa_context_search, result_type=papers, count=1
2026-02-22 20:37:58,521 - __main__ - INFO - call_tool: name=exa_context_search, result_type=papers, count=1
2026-02-22 20:37:58,522 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '持久化 ¶', 'authors': [], 'abstract': '概述 - LangChain 框架[跳过内容] \n**我们正在壮大，并为 LangChain、LangGraph 和LangSmith 招聘多个职位。[加入我们的团队！] **\n[] # 持久化[¶] \nLangGraph 有一个内置的持久化层，通过检查点器实现。当你使用检查点器编译图时，检查点器会在每个超级步骤保存图状态的`checkpoint`。这些检查点保存到`thread`中，可以在图执行后访问。因为`threads`允许在执行后访问图的状态，所以人机协作、记忆、时间旅行和容错等多种强大功能都成为可能。请参阅[此操作指南] ，了解如何在图中添加和使用检查点器的端到端示例。下面，我们将更详细地讨论这些概念。\n![Checkpoints] \nLangGraph API 自动处理检查点使用LangGraph API 时，你无需手动实现或配置检查点器。API 在幕后为你处理所有持久化基础设施。## 线程[¶] \n线程是分配给检查点器保存的每个检查点的唯一ID 或[线程标识符] 。当使用检查点器调用图时，你**必须**在配置的`configurable`部分中指定`thread\\_id`\n```\n`[]<web_link>{"configurable":{"thread\\_id":"1"}}`\n```\n## 检查点[¶] \n检查点是每个超级步骤保存的图状态的快照，由具有以下关键属性的`StateSnapshot`对象表示\n* `config`：与此检查点关联的配置。\n* `metadata`：与此检查点关联的元数据。\n* `values`：此时状态通道的值。\n* `next`：要在图中接下来执行的节点名称的元组。\n* `tasks`：包含要执行的后续任务信息的`PregelTask`对象元组。如果该步骤之前曾尝试过，它将包含错误信息。如果图在节点内部被[动态] 中断，`tasks`将包含与中断相关的附加数据。\n让我们看看以如下方式调用简单图时保存的检查点*API 参考：[StateGraph] |[START] |[END] |[InMemorySaver] *\n```\n`[]<web_link>fromlanggraph.graphimportStateGraph,START,END[]<web_link>fromlanggraph.checkpoint.memoryimportInMemorySaver[]<web_link>fromtypingimportAnnotated[]<web_link>fromtyping\\_extensionsimportTypedDict[]<web_link>fromoperatorimportadd[]<web_link>[]<web_link>classState(TypedDict):[]<web_link>foo:str[]<web_link>bar:Annotated[list[str],add][]<web_link>[]<web_link>defnode\\_a(state:State):[]<web_link>return{"foo":"a","bar":["a"]}[]<web_link>[]<web_link>defnode\\_b(state:State):[]<web_link>return{"foo":"b","bar":["b"]}[]<web_link>[]<web_link>[]<web_link>workflow=StateGraph(State)[]<web_link>workflow.add\\_node(node\\_a)[]<web_link>workflow.add\\_node(node\\_b)[]<web_link>workflow.add\\_edge(START,"node\\_a")[]<web_link>workflow.add\\_edge("node\\_a","node\\_b")[]<web_link>workflow.add\\_edge("node\\_b",END)[]<web_link>[]<web_link>checkpointer=InMemorySaver()[]<web_link>graph=workflow.compile(checkpointer=checkpointer)[]<web_link>[]<web_link>config={"configurable":{"thread\\_id":"1"}}[]<web_link>graph.invoke({"foo":""},config)`\n```\n运行图后，我们预期会看到恰好4 个检查点* 空检查点，其中`START`是下一个要执行的节点\n* 包含用户输入`{\'foo\': \'\', \'bar\': []}`的检查点，`node\\_a`是下一个要执行的节点\n* 包含`node\\_a`输出`{\'foo\': \'a\', \'bar\': [\'a\']}`的检查点，`node\\_b`是下一个要执行的节点\n* 包含`node\\_b`输出`{\'foo\': \'b\', \'bar\': [\'a\', \'b\']}`的检查点，并且没有要执行的下一个节点\n请注意，`bar`通道的值包含来自两个节点的输出，因为我们为`bar`通道设置了 reducer。\n### 获取状态[¶] \n当与保存的图状态交互时，你**必须**指定一个[线程标识符] 。你可以通过调用`graph.get\\_state(config)`来查看图的\\*最新\\*状态。这将返回一个`StateSnapshot`对象，该对象对应于配置中提供的线程 ID 关联的最新检查点，或者如果提供了检查点ID，则返回与该线程的检查点 ID 关联的检查点。```\n`[] # get the latest state snapshot[] config={"configurable":{"thread\\_id":"1"}}[] graph.get\\_state(config)[] [] # get a state snapshot for a specific checkpoint\\_id[] config={"configurable":{"thread\\_id":"1","checkpoint\\_id":"1ef663ba-28fe-6528-8002-5a559208592c"}}[] graph.get\\_state(config)`\n```\n在我们的示例中，`get\\_state`的输出将如下所示\n```\n`[] StateSnapshot([] values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},[] next=(),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},[] created\\_at=\'2024-08-29T19:19:38.821749+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}}, tasks=()[])`\n```\n### 获取状态历史[¶]<web_link>\n你可以通过调用`graph.get\\_state\\_history(config)`获取给定线程的图执行的完整历史记录。这将返回一个与配置中提供的线程 ID 关联的`StateSnapshot`对象列表。重要的是，检查点将按时间顺序排列，最近的检查点 /`StateSnapshot`位于列表的首位。\n```\n`[] config={"configurable":{"thread\\_id":"1"}}[] list(graph.get\\_state\\_history(config))`\n```\n在我们的示例中，`get\\_state\\_history`的输出将如下所示\n```\n`[] [[] StateSnapshot([] values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},[] next=(),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},[] created\\_at=\'2024-08-29T19:19:38.821749+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},[] tasks=(),[]),[] StateSnapshot([] values={\'foo\': \'a\', \'bar\': [\'a\']}, next=(\'node\\_b\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_a\': {\'foo\': \'a\', \'bar\': [\'a\']}}, \'step\': 1},[] created\\_at=\'2024-08-29T19:19:38.819946+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},[] tasks=(PregelTask(id=\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\', name=\'node\\_b\', error=None, interrupts=()),),[]),[] StateSnapshot([] values={\'foo\': \'\', \'bar\': []},[] next=(\'node\\_a\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},[] metadata={\'source\': \'loop\', \'writes\': None, \'step\': 0},[] created\\_at=\'2024-08-29T19:19:38.817813+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},[] tasks=(PregelTask(id=\'f1b14528-5ee5-579c-949b-23ef9bfbed58\', name=\'node\\_a\', error=None, interrupts=()),),[]),[] StateSnapshot([] values={\'bar\': []},[] next=(\'\\_\\_start\\_\\_\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},[] metadata={\'source\': \'input\', \'writes\': {\'foo\': \'\'}, \'step\': -1},[] created\\_at=\'2024-08-29T19:19:38.816205+00:00\',[] parent\\_config=None,[] tasks=(PregelTask(id=\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\', name=\'\\_\\_start\\_\\_\', error=None, interrupts=()),),[])[]]`\n```\n![State]<image_link>\n### 回放[¶]<web_link>\n也可以回放之前的图执行。如果我们使用`thread\\_id`和`checkpoint\\_id`调用图，那么我们将\\*回放\\*`checkpoint\\_id`对应的检查点\\*之前\\*已执行的步骤，并且只执行检查点\\*之后\\*的步骤。\n* `thread\\_id`是线程的 ID。\n* `checkpoint\\_id`是指线程中特定检查点的标识符。\n在调用图时，你必须将这些作为配置的`configurable`部分传入\n```\n`[] config={"configurable":{"thread\\_id":"1","checkpoint\\_id":"0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}[] graph.invoke(None,config=config)`\n```\n重要的是，LangGraph 知道特定步骤是否以前执行过。如果执行过，LangGraph 只会简单地在图中\\*回放\\*该特定步骤，而不会重新执行该步骤，但这仅适用于所提供的`checkpoint\\_id`\\*之前\\*的步骤。`checkpoint\\_id`\\*之后\\*的所有步骤都将被执行（即，一个新的分支），即使它们以前执行过。请参阅此[时间旅行操作指南，了解有关回放的更多信息]<web_link>。\n![Replay]<image_link>\n### 更新状态[¶]<web_link>\n除了从特定的`checkpoints`回放图之外，我们还可以\\*编辑\\*图的状态。我们使用`graph.update\\_state()`来实现。此方法接受三个不同的参数\n#### `config`[¶]<web_link>\n配置应包含`thread\\_id`，指定要更新哪个线程。当只传递`thread\\_id`时，我们更新（或分叉）当前状态。可选地，如果我们包含`checkpoint\\_id`字段，那么我们分叉该选定的检查点。\n#### `values`[¶]<web_link>\n这些是将用于更新状态的值。请注意，此更新的处理方式与来自节点的任何更新完全相同。这意味着如果为图状态中的某些通道定义了reducer 函数，这些值将传递给[reducer]<web_link>函数。这意味着`update\\_state`不会自动覆盖每个通道的通道值，而只覆盖没有 reducer 的通道。让我们通过一个例子来了解。假设你已经使用以下模式定义了图的状态（参见上面的完整示例）```\n`[]<web_link>fromtypingimportAnnotated[]<web_link>fromtyping\\_extensionsimportTypedDict[]<web_link>fromoperatorimportadd[]<web_link>[]<web_link>classState(TypedDict):[]<web_link>foo:int[]<web_link>bar:Annotated[list[str],add]`\n```\n现在假设图的当前状态是```\n`[] {"foo": 1, "bar": ["a"]}`\n```\n如果你如下更新状态```\n`[]<web_link>graph.update\\_state(config, {"foo": 2, "bar": ["b"]})`\n```\n那么图的新状态将是```\n`[] {"foo": 2, "bar": ["a", "b"]}`\n```\n`foo`键（通道）完全改变了（因为没有为该通道指定 reducer，所以`update\\_state`会覆盖它）。然而，为`bar`键指定了一个 reducer，因此它将`"b"`附加到`bar`的状态。\n#### `as\\_node`[¶]<web_link>\n调用`update\\_state`时，你可以选择指定的最后一项是`as\\_node`。如果你提供了它，更新将像来自节点`as\\_node`一样应用。如果未提供`as\\_node`，则在不模糊的情况下，它将被设置为最后更新状态的节点。这之所以重要，是因为接下来要执行的步骤取决于最后给出更新的节点，因此这可用于控制哪个节点接下来执行。请参阅此[时间旅行操作指南，了解有关分叉状态的更多信息]<web_link>。\n![Update]<image_link>\n## 记忆存储[¶]<web_link>\n![Model of shared state]<image_link>\n[状态模式]<web_link>指定了一组在图执行时填充的键。如上所述，状态可以通过检查点器在每个图步骤写入线程，从而实现状态持久化。\n但是，如果我们要\\*跨线程\\*保留一些信息怎么办？考虑一个聊天机器人的情况，我们希望在与该用户进行的所有聊天对话（例如，线程）中保留有关该用户的特定信息！\n仅凭检查点器，我们无法跨线程共享信息。这促使了对[`Store`]<web_link>接口的需求。为了说明这一点，我们可以定义一个`InMemoryStore`来存储用户在所有线程中的信息。我们只需像以前一样，使用检查点器和我们新的`in\\_memory\\_store`变量来编译我们的图。\nLangGraph API 自动处理存储使用LangGraph API 时，你无需手动实现或配置存储。API 在幕后为你处理所有存储基础设施。### 基本用法[¶]<web_link>\n首先，让我们在不使用LangGraph 的情况下单独展示这一点。```\n`[]<web_link>fromlanggraph.store.memoryimportInMemoryStore[]<web_link>in\\_memory\\_store=InMemoryStore()`\n```\n记忆通过`tuple`进行命名空间管理，在此特定示例中为`(&lt;&lt;user\\_id&gt;&gt;, "memories")`。命名空间可以是任意长度，并表示任何内容，不必是用户特定的。\n```\n`[]<web_link>user\\_id="1"[]<web_link>namespace\\_for\\_memory=(user\\_id,"memories")`\n```\n我们使用`store.put`方法将记忆保存到存储中的命名空间。当我们这样做时，我们指定命名空间（如上定义），以及记忆的键值对：键是记忆的唯一标识符（`memory\\_id`），值（一个字典）是记忆本身。\n```\n`[]<web_link>memory\\_id=str(uuid.uuid4())[]<web_link>memory={"food\\_preference":"I like pizza"}[]<web_link>in\\_memory\\_store.put(namespace\\_for\\_memory,memory\\_id,memory)`\n```\n我们可以使用`store.search`方法读取命名空间中的记忆，该方法将以列表形式返回给定用户的所有记忆。最近的记忆在列表的末尾。\n```\n`[]<web_link>memories=in\\_memory\\_store.search(namespace\\_for\\_memory)[]<web_link>memories[-1].dict()[]<web_link>{\'value\':{\'food\\_preference\':\'I like pizza\'},[]<web_link>\'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\',[]<web_link>\'namespace\':[\'1\',\'memories\'],[]<web_link>\'created\\_at\':\'2024-10-02T17:22:31.590602+00:00\',[]<web_link>\'updated\\_at\':\'2024-10-02T17:22:31.590605+00:00\'}`\n```\n每种记忆类型都是一个Python 类([`Item`])，具有特定属性。我们可以像上面那样通过`.dict`转换来将其作为字典访问。它具有的属性是\n* `value`：此记忆的值（本身是一个字典）\n* `key`：此记忆在此命名空间中的唯一键\n* `namespace`：一个字符串列表，此记忆类型的命名空间\n* `created\\_at`：此记忆创建时的时间戳\n* `updated\\_at`：此记忆更新时的时间戳### 语义搜索[¶] \n除了简单检索，该存储还支持语义搜索，允许你根据含义而非精确匹配来查找记忆。要启用此功能，请使用嵌入模型配置存储*API', 'doi': '', 'published_date': '2026-02-22T20:37:58.521593', 'pdf_url': '', 'url': 'https://langgraph.com.cn/concepts/persistence.1.html', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}
2026-02-22 20:37:58,581 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:37:58,582 - __main__ - INFO - call_tool payload: source_tool=exa_context_search, result_type=papers, count=1
2026-02-22 20:37:58,582 - __main__ - INFO - call_tool: name=exa_context_search, result_type=papers, count=1
2026-02-22 20:37:58,582 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '概述 - LangChain 框架', 'authors': [], 'abstract': '概述 - LangChain 教程[跳到内容] \n**LangGraph 平台文档已迁移！**请在新的[LangChain 文档] 网站上查找 LangGraph 平台文档。[] # 人机协作(Human-in-the-loop)[¶] \n要在代理或工作流中审查、编辑和批准工具调用，请[使用 LangGraph 的人机协同（human-in-the-loop）功能] ，以便在工作流的任何时刻进行人工干预。这在大型语言模型 (LLM) 驱动的应用中尤其有用，因为模型输出可能需要验证、修正或额外的上下文。![image] \n提示有关如何使用人机协同的信息，请参阅[启用人工干预] 和[使用服务器 API 实现人机协同] 。\n## 核心功能[¶] \n* **持久化执行状态**：中断功能使用了 LangGraph 的[持久化] 层，该层会保存图的状态，从而可以无限期地暂停图的执行，直到您恢复为止。这是因为 LangGraph 在每一步之后都会为图状态创建检查点，这使得系统能够持久化执行上下文并在之后从中断处继续恢复工作流。这支持了没有时间限制的异步人工审查或输入。有两种暂停图的方法* [动态中断] ：在特定节点内部，根据图的当前状态使用`interrupt`来暂停图。\n* [静态中断] ：使用`interrupt\\_before`和`interrupt\\_after`在预定义的点暂停图，即在节点执行之前或之后。\n![image] 一个由3个顺序步骤组成的示例图，在 step\\_3 之前设有一个断点。* **灵活的集成点**：人机协同逻辑可以引入到工作流的任何一点。这允许有针对性的人工参与，例如批准 API 调用、修正输出或引导对话。## 模式[¶] \n您可以使用`interrupt`和`Command`实现四种典型的设计模式\n* [批准或拒绝] ：在关键步骤（如 API 调用）之前暂停图，以审查和批准该操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。此模式通常涉及根据人工输入来路由图。* [审查和编辑状态] ：暂停图以审查和编辑图状态。这对于纠正错误或用附加信息更新状态很有用。此模式通常涉及用人工输入来更新状态。\n* [审查工具调用] ：在工具执行之前，暂停图以审查和编辑 LLM 请求的工具调用。* [验证人工输入] ：在进入下一步之前，暂停图以验证人工输入。\n回到顶', 'doi': '', 'published_date': '2025-01-01T00:00:00+00:00', 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraph/concepts/human_in_the_loop', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}
2026-02-22 20:37:59,321 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:37:59,321 - __main__ - INFO - call_tool payload: source_tool=exa_context_search, result_type=papers, count=1
2026-02-22 20:37:59,321 - __main__ - INFO - call_tool: name=exa_context_search, result_type=papers, count=1
2026-02-22 20:37:59,321 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': "LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学 | Ningto's Blog", 'authors': [], 'abstract': 'LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学 | Ningto&#x27;s Blog\n[\nNingto&#x27;s Blog\n] \n[Blog] [LLM] [Tags] [CSS] [Projects] [About] \n# LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学\n[AI] [LangChain] [LLM] [Memory-Management] [LangGraph] \n•\n2026年1月22日星期四\n目录\n* [LangChain内存管理的核心概念与设计哲学] \n* [短时内存：会话上下文的守护者] \n* [长时内存：持久化知识与经验的基石] \n* [与LangGraph的集成：构建有状态的、持久的智能代理] \n* [最佳实践与性能优化建议] \n* [总结] \n在构建基于大型语言模型（LLM）的智能应用时，一个核心挑战是如何高效、优雅地管理“记忆”。无论是简单的聊天机器人需要记住上一条消息，还是复杂的个人助理需要从过往数月的交互中检索关键信息，内存管理都是决定应用智能程度与用户体验的关键。LangChain，作为当前最流行的LLM应用开发框架之一，其内存管理系统正是为解决这一系列问题而精心设计的。本文将深入其内部，解析其短时内存与长时内存的架构选择、实现机制，并探讨如何基于LangGraph构建具备持久化记忆的下一代AI代理。\n## [] LangChain内存管理的核心概念与设计哲学\nLangChain将“记忆”抽象为一个核心组件，其设计哲学可以概括为：**标准化、模块化与可组合性**。它不试图提供一个万能的记忆解决方案，而是定义了一套清晰的接口和抽象，允许开发者根据应用场景自由选择和组合不同的记忆策略。\n从宏观上看，LangChain将内存分为两大类别：\n1. **短时内存 (Short-term Memory)**：用于维持单次对话或单个执行流程中的上下文。它通常是易失的，生命周期与当前的会话或代理运行实例绑定。其核心作用是解决LLM有限的上下文窗口问题，通过有选择地保留、总结或压缩历史消息，确保最重要的信息能被模型“看见”。\n2. **长时内存 (Long-term Memory)**：用于跨会话、跨进程甚至跨时间的持久化信息存储与检索。它通常与外部数据库（如向量数据库、SQL数据库）连接，能够存储海量信息，并根据当前查询动态检索最相关的片段。其核心作用是赋予AI代理“经验”和“知识”，实现持续学习和个性化。\n这种划分并非LangChain独创，但它通过清晰的API和与LangGraph的深度集成，将这一理念变成了易于实现的工程实践。\n## [] 短时内存：会话上下文的守护者\n短时内存的核心任务是管理对话历史。最简单的形式就是 `ConversationBufferMemory`，它像一个FIFO队列，忠实地记录所有对话。\n```python\n<span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory\n</span><span class="code-line"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> ChatOpenAI\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 初始化一个简单的对话缓冲内存</span>\n</span><span class="code-line">memory <span class="token operator">=</span> ConversationBufferMemory<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"gpt-4"</span><span class="token punctuation">)</span>\n</span><span class="code-line">conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> memory<span class="token operator">=</span>memory<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 进行对话</span>\n</span><span class="code-line">conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你好，我叫小明。"</span><span class="token punctuation">)</span>\n</span><span class="code-line"><span class="token comment"># 内存中现在保存了：Human: 你好，我叫小明。\\nAI: [模型的回复]</span>\n</span><span class="code-line">conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"我的名字是什么？"</span><span class="token punctuation">)</span>\n</span><span class="code-line"><span class="token comment"># AI能够正确回答“小明”，因为它从内存中读取了历史。</span>\n</span>\n```\n然而，随着对话轮次增加，简单的缓冲会迅速耗尽模型的上下文窗口。为此，LangChain提供了更智能的策略：\n* `ConversationBufferWindowMemory`：只保留最近K轮对话。\n* `ConversationSummaryMemory`：定期（或按需）使用LLM对历史对话进行总结，用总结摘要替代原始长文本，从而大幅节省Token。\n* `ConversationTokenBufferMemory`：基于Token数量进行限制，而非对话轮次，更精确地控制上下文长度。\n* `ConversationSummaryBufferMemory`：结合了总结和缓冲，在Token超限时，将最早的历史进行总结，保留最近的原始对话。\n这些内存组件都实现了统一的 `BaseChatMemory` 接口，可以像乐高积木一样在Chain或Agent中替换，体现了极佳的模块化设计。\n## [] 长时内存：持久化知识与经验的基石\n当应用需要记住超越单次会话的信息时，就需要长时内存。LangChain的长时内存系统通常与 **检索器 (Retriever)** 紧密结合，其标准范式是：**存储时进行向量化嵌入并保存到数据库；检索时根据查询的向量相似度召回最相关的片段**。\n```python\n<span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> VectorStoreRetrieverMemory\n</span><span class="code-line"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> OpenAIEmbeddings\n</span><span class="code-line"><span class="token keyword">from</span> langchain_chroma <span class="token keyword">import</span> Chroma\n</span><span class="code-line"><span class="token keyword">from</span> langchain_core<span class="token punctuation">.</span>documents <span class="token keyword">import</span> Document\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 1. 准备一个支持检索的向量数据库作为记忆后端</span>\n</span><span class="code-line">embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">vectorstore <span class="token operator">=</span> Chroma<span class="token punctuation">(</span>embedding_function<span class="token operator">=</span>embeddings<span class="token punctuation">,</span> collection_name<span class="token operator">=</span><span class="token string">"long_term_memory"</span><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 2. 创建一些“记忆”文档并存储</span>\n</span><span class="code-line">memories <span class="token operator">=</span> <span class="token punctuation">[</span>\n</span><span class="code-line">    Document<span class="token punctuation">(</span>page_content<span class="token operator">=</span><span class="token string">"小明最喜欢的颜色是蓝色。"</span><span class="token punctuation">,</span> metadata<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"speaker"</span><span class="token punctuation">:</span> <span class="token string">"human"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n</span><span class="code-line">    Document<span class="token punctuation">(</span>page_content<span class="token operator">=</span><span class="token string">"小明的生日是7月15日。"</span><span class="token punctuation">,</span> metadata<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"speaker"</span><span class="token punctuation">:</span> <span class="token string">"human"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n</span><span class="code-line"><span class="token punctuation">]</span>\n</span><span class="code-line">vectorstore<span class="token punctuation">.</span>add_documents<span class="token punctuation">(</span>memories<span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 3. 创建基于向量检索的记忆组件</span>\n</span><span class="code-line">retriever <span class="token operator">=</span> vectorstore<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span>search_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"k"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">)</span>\n</span><span class="code-line">memory <span class="token operator">=</span> VectorStoreRetrieverMemory<span class="token punctuation">(</span>retriever<span class="token operator">=</span>retriever<span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 4. 在对话中使用</span>\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChain\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 提示词模板，通过 `{history}` 和 `{input}` 注入记忆和当前输入</span>\n</span><span class="code-line">prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>\n</span><span class="code-line">    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"history"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>\n</span><span class="code-line">    template<span class="token operator">=</span>`你是一个贴心的助手，以下是你已知的关于用户的背景信息：\\n<span class="token punctuation">{</span>history<span class="token punctuation">}</span>\\n\\n当前对话：\\nHuman<span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token builtin">input</span><span class="token punctuation">}</span>\\nAI<span class="token punctuation">:</span>`\n</span><span class="code-line"><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">chain <span class="token operator">=</span> LLMChain<span class="token punctuation"', 'doi': '', 'published_date': '2026-01-22T00:00:00+00:00', 'pdf_url': '', 'url': 'https://www.ningto.com/blog/2026/langchain-memory-management-deep-dive-from-short-term-conversations-to-long-term-memory-architecture-philosophy', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}
2026-02-22 20:37:59,900 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:37:59,901 - __main__ - INFO - call_tool payload: source_tool=tavily_search, result_type=papers, count=1
2026-02-22 20:37:59,901 - __main__ - INFO - call_tool: name=tavily_search, result_type=papers, count=1
2026-02-22 20:37:59,901 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': 'LangGraph 持久化 - 概述- LangChain 教程', 'authors': [], 'abstract': '* [线程](#threads)\n* [检查点](#checkpoints) \n\n  + [获取状态](#get-state)\n  + [获取状态历史](#get-state-history)\n  + [重放](#replay)\n  + [更新状态](#update-state) \n\n    - [config](#config)\n    - [values](#values)\n    - [as\\_node](#as_node)\n* [记忆存储](#memory-store) \n\n  + [基本用法](#basic-usage)\n  + [语义搜索](#semantic-search)\n  + [在 LangGraph 中使用](#using-in-langgraph)\n* [检查点库](#checkpointer-libraries) \n\n  + [检查点接口](#checkpointer-interface)\n  + [序列化器](#serializer) \n\n    - [使用 pickle 进行序列化](#serialization-with-pickle)\n    - [加密](#encryption)\n* [功能](#capabilities) \n\n  + [人工干预](#human-in-the-loop)\n  + [记忆](#memory)\n  + [时光穿梭](#time-travel)\n  + [容错](#fault-tolerance) \n\n    - [待定写入](#pending-writes)\n\n# 持久性[¶](#persistence "Permanent link")\n\nLangGraph 有一个内置的持久化层，通过检查点（checkpointer）实现。当您使用检查点编译一个图时，检查点会在每个超级步骤（super-step）保存图状态的`checkpoint`。这些检查点被保存到一个`thread`（线程）中，可以在图执行后访问。因为`threads`允许在执行后访问图的状态，所以可以实现诸如人在环路、记忆、时间旅行和容错等多种强大功能。下面，我们将更详细地讨论这些概念。\n\nLangGraph API 自动处理检查点\n\n使用 LangGraph API 时，您无需手动实现或配置检查点。API 在后台为您处理所有持久化基础架构。\n\n## 线程[¶](#threads "Permanent link")\n\n线程（thread）是由检查点保存的每个检查点的唯一 ID 或线程标识符。它包含一系列[运行](../assistants/#execution)的累积状态。当一次运行被执行时，助手的底层图的[状态](../low_level/#state)将被持久化到该线程中。\n\n当使用检查点调用一个图时，您**必须**在配置的`configurable`部分指定一个`thread_id`。\n\n```\n{"configurable": {"thread_id": "1"}} {"configurable":{"thread_id": "1"}}\n```\n\n可以检索一个线程的当前和历史状态。为了持久化状态，必须在执行一次运行之前创建一个线程。LangGraph 平台 API 提供了几个用于创建和管理线程及线程状态的端点。更多详情请参见[API 参考](../../cloud/reference/api/api_ref.html#tag/threads)。\n\n## 检查点[¶](#checkpoints "Permanent link")\n\n一个线程在特定时间点的状态称为检查点（checkpoint）。检查点是在每个超级步骤保存的图状态的快照，由一个`StateSnapshot`对象表示，具有以下关键属性：\n\n* `config`: 与此检查点关联的配置。\n* `metadata`: 与此检查点关联的元数据。\n* `values`: 在此时间点的状态通道的值。\n* `next`: 一个元组，包含图中接下来要执行的节点名称。\n* `tasks`: 一个`PregelTask`对象的元组，包含有关接下来要执行的任务的信息。如果该步骤之前尝试过，它将包含错误信息。如果图在节点内部被[动态](../../how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt)中断，任务将包含与中断相关的额外数据。\n\n检查点是持久化的，可以用来在稍后恢复线程的状态。\n\n让我们看看当一个简单的图被如下调用时，会保存哪些检查点：\n\n*API 参考：[StateGraph](https://github.langchain.ac.cn/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://github.langchain.ac.cn/langgraph/reference/constants/#langgraph.constants.START) | [END](https://github.langchain.ac.cn/langgraph/reference/constants/#langgraph.constants.END) | [InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)*\n\n```\nfrom langgraph.graph import StateGraph, START, END from  langgraph.graph  import StateGraph, START, ENDfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaver from typing import Annotated from  typing  import Annotated from typing_extensions import TypedDict from  typing_extensions  import TypedDict from operator import add from  operator  import add  class State(TypedDict): class  State(TypedDict): foo: str foo: str bar: Annotated[list[str], add] bar: Annotated[list[str], add]  def node_a(state: State): def  node_a(state: State): return {"foo": "a", "bar": ["a"]} return{"foo": "a", "bar":["a"]}  def node_b(state: State): def  node_b(state: State): return {"foo": "b", "bar": ["b"]} return{"foo": "b", "bar":["b"]}   workflow = StateGraph(State) workflow = StateGraph(State)workflow.add_node(node_a) workflow. add_node(node_a)workflow.add_node(node_b) workflow. add_node(node_b)workflow.add_edge(START, "node_a") workflow. add_edge(START, "node_a")workflow.add_edge("node_a", "node_b") workflow. add_edge("node_a", "node_b")workflow.add_edge("node_b", END) workflow. add_edge("node_b", END)  checkpointer = InMemorySaver() checkpointer = InMemorySaver()graph = workflow.compile(checkpointer=checkpointer) graph = workflow. compile(checkpointer = checkpointer)  config = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}graph.invoke({"foo": ""}, config) graph. invoke({"foo": ""}, config)\n```\n\n在我们运行图之后，我们期望看到正好 4 个检查点：\n\n* 一个空的检查点，以`START`作为下一个要执行的节点。\n* 一个包含用户输入`{\'foo\': \'\', \'bar\': []}`的检查点，以`node_a`作为下一个要执行的节点。\n* 一个包含`node_a`输出`{\'foo\': \'a\', \'bar\': [\'a\']}`的检查点，以`node_b`作为下一个要执行的节点。\n* 一个包含`node_b`输出`{\'foo\': \'b\', \'bar\': [\'a\', \'b\']}`的检查点，没有下一个要执行的节点。\n\n请注意，`bar`通道的值包含了两个节点的输出，因为我们为`bar`通道定义了一个归约器（reducer）。\n\n### 获取状态[¶](#get-state "Permanent link")\n\n当与保存的图状态进行交互时，您**必须**指定一个[线程标识符](#threads)。您可以通过调用`graph.get_state(config)`来查看图的*最新*状态。这将返回一个`StateSnapshot`对象，它对应于配置中提供的线程 ID 的最新检查点，或者（如果提供了）对应于该线程的某个检查点 ID 的检查点。\n\n```\n # get the latest state snapshot # get the latest state snapshotconfig = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}graph.get_state(config) graph. get_state(config)  # get a state snapshot for a specific checkpoint_id # get a state snapshot for a specific checkpoint_idconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}} config ={"configurable":{"thread_id": "1", "checkpoint_id":"1ef663ba-28fe-6528-8002-5a559208592c"}}graph.get_state(config) graph. get_state(config)\n```\n\n在我们的例子中，`get_state`的输出将如下所示：\n\n```\nStateSnapshot(  values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},  next=(),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},  created_at=\'2024-08-29T19:19:38.821749+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}}, tasks=() ) \n```\n\n### 获取状态历史[¶](#get-state-history "Permanent link")\n\n您可以通过调用`graph.get_state_history(config)`来获取给定线程的完整图执行历史。这将返回一个与配置中提供的线程 ID 相关联的`StateSnapshot`对象列表。重要的是，检查点将按时间顺序排列，最新的检查点 / `StateSnapshot`位于列表的第一个。\n\n```\nconfig = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}list(graph.get_state_history(config)) list(graph. get_state_history(config))\n```\n\n在我们的例子中，`get_state_history`的输出将如下所示：\n\n```\n[  StateSnapshot(  values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},  next=(),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},  created_at=\'2024-08-29T19:19:38.821749+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},  tasks=(),  ),  StateSnapshot(  values={\'foo\': \'a\', \'bar\': [\'a\']}, next=(\'node_b\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_a\': {\'foo\': \'a\', \'bar\': [\'a\']}}, \'step\': 1},  created_at=\'2024-08-29T19:19:38.819946+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},  tasks=(PregelTask(id=\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\', name=\'node_b\', error=None, interrupts=()),),  ),  StateSnapshot(  values={\'foo\': \'\', \'bar\': []},  next=(\'node_a\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},  metadata={\'source\': \'loop\', \'writes\': None, \'step\': 0},  created_at=\'2024-08-29T19:19:38.817813+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},  tasks=(PregelTask(id=\'f1b14528-5ee5-579c-949b-23ef9bfbed58\', name=\'node_a\', error=None, interrupts=()),),  ),  StateSnapshot(  values={\'bar\': []},  next=(\'__start__\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},  metadata={\'source\': \'input\', \'writes\': {\'foo\': \'\'}, \'step\': -1},  created_at=\'2024-08-29T19:19:38.816205+00:00\',  parent_config=None,  tasks=(PregelTask(id=\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\', name=\'__start__\', error=None, interrupts=()),),  ) ] \n```\n\n### 重放[¶](#replay "Permanent link")\n\n也可以回放之前的图执行。如果我们使用一个`thread_id`和一个`checkpoint_id`来`invoke`一个图，那么我们将*重放*与`checkpoint_id`对应的检查点*之前*已执行的步骤，而只执行该检查点*之后*的步骤。\n\n* `thread_id`是线程的 ID。\n* `checkpoint_id`是引用线程内特定检查点的标识符。\n\n您必须在调用图时将这些作为配置的`configurable`部分传递。\n\n```\nconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}} config ={"configurable":{"thread_id": "1", "checkpoint_id":"0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}graph.invoke(None, config=config) graph. invoke(None, config = config)\n```\n\n重要的是，LangGraph 知道某个特定步骤之前是否已执行过。如果是，LangGraph 只是*重放*图中该特定步骤，而不会重新执行它，但这仅限于所提供的`checkpoint_id`*之前*的步骤。所有在`checkpoint_id`*之后*的步骤都将被执行（即一个新的分支），即使它们之前已经执行过。请参阅关于[时间旅行的这份操作指南，以了解更多关于重放的信息](../../how-tos/human_in_the_loop/time-travel/)。\n\n### 更新状态[¶](#update-state "永久链接")\n\n除了从特定的`checkpoints`重放图之外，我们还可以*编辑*图状态。我们使用`graph.update_state()`来做到这一点。该方法接受三个不同的参数：\n\n#### `config`[¶](#config "Permanent link")\n\n配置应包含`thread_id`，指定要更新哪个线程。当只传递`thread_id`时，我们更新（或分叉）当前状态。或者，如果我们包含`checkpoint_id`字段，那么我们将分叉该选定的检查点。\n\n#### `values`[¶](#values "Permanent link")\n\n这些是用于更新状态的值。请注意，此更新的处理方式与来自节点的任何更新完全相同。这意味着这些值将被传递给[归约器](../low_level/#reducers)函数（如果为图状态中的某些通道定义了归约器）。这意味着`update_state`不会自动覆盖每个通道的值，而只会覆盖没有归约器的通道。让我们看一个例子。\n\n假设您使用以下模式定义了图的状态（参见上面的完整示例）：\n\n```\n from typing import Annotated from  typing  import Annotated from typing_extensions import TypedDict from  typing_extensions  import TypedDict from operator import add from  operator  import add  class State(TypedDict): class  State(TypedDict): foo: int foo: int bar: Annotated[list[str], add] bar: Annotated[list[str], add]\n```\n\n现在假设图的当前状态是：\n\n```\n{"foo": 1, "bar": ["a"]} \n```\n\n如果您如下更新状态：\n\n```\ngraph.update_state(config, {"foo": 2, "bar": ["b"]}) graph. update_state(config,{"foo": 2, "bar":["b"]})\n```\n\n那么图的新状态将是：\n\n```\n{"foo": 2, "bar": ["a", "b"]} \n```\n\n`foo`键（通道）被完全更改（因为没有为该通道指定归约器，所以`update_state`会覆盖它）。然而，为`bar`键指定了一个归约器，所以它将`"b"`附加到`bar`的状态中。\n\n#### `as_node`[¶](#as_node "Permanent link")\n\n在调用`update_state`时，您可以选择性地指定最后一个参数是`as_node`。如果您提供了它，更新将像来自节点`as_node`一样被应用。如果未提供`as_node`，它将被设置为最后更新状态的节点（如果不模糊）。这之所以重要，是因为下一步要执行的步骤取决于最后给出更新的节点，因此这可以用来控制接下来执行哪个节点。请参阅关于[分叉状态的这份操作指南，以了解更多信息](../../how-tos/human_in_the_loop/time-travel/)。\n\n## 记忆存储[¶](#memory-store "Permanent link")\n\n一个[状态模式](../low_level/#schema)指定了一组在图执行过程中被填充的键。如上所述，状态可以由检查点在每个图步骤写入线程，从而实现状态持久化。\n\n但是，如果我们想在*多个线程之间*保留一些信息怎么办？考虑一个聊天机器人的案例，我们希望在与该用户的所有聊天对话（即线程）中保留关于该用户的特定信息！\n\n仅使用检查点，我们无法在线程之间共享信息。这就需要[`Store`](../../reference/store/#langgraph.store.base.BaseStore)接口。作为示例，我们可以定义一个`InMemoryStore`来跨线程存储关于用户的信息。我们像以前一样用检查点编译我们的图，并使用我们新的`in_memory_store`变量。\n\nLangGraph API 自动处理存储\n\n当使用 LangGraph API 时，您无需手动实现或配置存储。API 在后台为您处理所有存储基础架构。\n\n### 基本用法[¶](#basic-usage "Permanent link")\n\n首先，让我们在不使用 LangGraph 的情况下单独展示这一点。\n\n```\nfrom langgraph.store.memory import InMemoryStore from  langgraph.store.memory  import InMemoryStorein_memory_store = InMemoryStore() in_memory_store = InMemoryStore()\n```\n\n记忆由一个`tuple`进行命名空间划分，在这个特定示例中是`(, "memories")`。命名空间可以是任何长度，可以代表任何东西，不一定是特定于用户的。\n\n```\nuser_id = "1" user_id = "1"namespace_for_memory = (user_id, "memories") namespace_for_memory =(user_id, "memories")\n```\n\n我们使用`store.put`方法将记忆保存到存储中的命名空间。当我们这样做时，我们指定如上定义的命名空间，以及记忆的键值对：键是记忆的唯一标识符（`memory_id`），值（一个字典）是记忆本身。\n\n```\nmemory_id = str(uuid.uuid4()) memory_id = str(uuid. uuid4())memory = {"food_preference" : "I like pizza"} memory ={"food_preference": "I like pizza"}in_memory_store.put(namespace_for_memory, memory_id, memory) in_memory_store. put(namespace_for_memory, memory_id, memory)\n```\n\n我们可以使用`store.search`方法读出我们命名空间中的记忆，它将返回给定用户的所有记忆作为一个列表。最新的记忆是列表中的最后一个。\n\n```\nmemories = in_memory_store.search(namespace_for_memory) memories = in_memory_store. search(namespace_for_memory)memories[-1].dict() memories[- 1]. dict(){\'value\': {\'food_preference\': \'I like pizza\'}, {\'value\':{\'food_preference\': \'I like pizza\'}, \'key\': \'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'namespace\': [\'1\', \'memories\'], \'namespace\':[\'1\', \'memories\'], \'created_at\': \'2024-10-02T17:22:31.590602+00:00\', \'created_at\':\'2024-10-02T17:22:31.590602+00:00\', \'updated_at\': \'2024-10-02T17:22:31.590605+00:00\'} \'updated_at\':\'2024-10-02T17:22:31.590605+00:00\'}\n```\n\n每种记忆类型都是一个 Python 类 ([`Item`](https://github.langchain.ac.cn/langgraph/reference/store/#langgraph.store.base.Item))，具有某些属性。我们可以通过`.dict`转换将其作为字典访问，如上所示。\n\n它具有的属性是：\n\n* `value`: 该记忆的值（本身是一个字典）\n* `key`: 该记忆在此命名空间中的唯一键\n* `namespace`: 一个字符串列表，该记忆类型的命名空间\n* `created_at`: 该记忆被创建时的时间戳\n* `updated_at`: 该记忆被更新时的时间戳\n\n### 语义搜索[¶](#semantic-search "Permanent link")\n\n除了简单的检索，存储还支持语义搜索，允许您根据含义而不是精确匹配来查找记忆。要启用此功能，请使用嵌入模型配置存储：\n\n*API 参考：[init\\_embeddings](https://python.langchain.ac.cn/api_reference/langchain/embeddings/langchain.embeddings.base.init_embeddings.html)*\n\n```\nfrom langchain.embeddings import init_embeddings from  langchain.embeddings  import init_embeddings  store = InMemoryStore( store = InMemoryStore( index={ index ={ "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "dims": 1536, # Embedding dimensions "dims": 1536, # Embedding dimensions "fields": ["food_preference", "$"] # Fields to embed "fields":["food_preference", "$"] # Fields to embed } }) )\n```\n\n现在搜索时，您可以使用自然语言查询来查找相关的记忆：\n\n```\n # Find memories about food preferences # Find memories about food preferences# (This can be done after putting memories into the store) # (This can be done after putting memories into the store)memories = store.search( memories = store. search( namespace_for_memory, namespace_for_memory, query="What does the user like to eat?", query = "What does the user like to eat?", limit=3 # Return top 3 matches limit = 3 # Return top 3 matches) )\n```\n\n您可以通过配置`fields`参数或在存储记忆时指定`index`参数来控制记忆的哪些部分被嵌入。\n\n```\n # Store with specific fields to embed # Store with specific fields to embedstore.put( store. put( namespace_for_memory, namespace_for_memory, str(uuid.uuid4()), str(uuid. uuid4()), { { "food_preference": "I love Italian cuisine", "food_preference": "I love Italian cuisine", "context": "Discussing dinner plans" "context": "Discussing dinner plans" }, }, index=["food_preference"] # Only embed "food_preferences" field index =["food_preference"] # Only embed "food_preferences" field) )  # Store without embedding (still retrievable, but not searchable) # Store without embedding (still retrievable, but not searchable)store.put( store. put( namespace_for_memory, namespace_for_memory, str(uuid.uuid4()), str(uuid. uuid4()), {"system_info": "Last updated: 2024-01-01"}, {"system_info":"Last updated: 2024-01-01"}, index=False index = False) )\n```\n\n### 在 LangGraph 中使用[¶](#using-in-langgraph "Permanent link")\n\n有了这些，我们就可以在 LangGraph 中使用`in_memory_store`了。`in_memory_store`与检查点协同工作：检查点将状态保存到线程中（如上所述），而`in_memory_store`允许我们存储任意信息以便在*多个*线程之间访问。我们用检查点和`in_memory_store`两者来编译图，如下所示。\n\n*API 参考：[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)*\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaver  # We need this because we want to enable threads (conversations) # We need this because we want to enable threads (conversations)checkpointer = InMemorySaver() checkpointer = InMemorySaver()  # ... Define the graph ... # ... Define the graph ...  # Compile the graph with the checkpointer and store # Compile the graph with the checkpointer and storegraph = graph.compile(checkpointer=checkpointer, store=in_memory_store) graph = graph. compile(checkpointer = checkpointer, store = in_memory_store)\n```\n\n我们像以前一样用`thread_id`调用图，同时也用`user_id`，我们将用它来将我们的记忆命名空间限定到这个特定用户，如上所示。\n\n```\n # Invoke the graph # Invoke the graphuser_id = "1" user_id = "1"config = {"configurable": {"thread_id": "1", "user_id": user_id}} config ={"configurable":{"thread_id": "1", "user_id": user_id}}  # First let\'s just say hi to the AI # First let\'s just say hi to the AIfor update in graph.stream( for update in graph. stream( {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates" {"messages":[{"role": "user", "content": "hi"}]}, config, stream_mode = "updates"): ): print(update) print(update)\n```\n\n我们可以通过将`store: BaseStore`和`config: RunnableConfig`作为节点参数，在*任何节点*中访问`in_memory_store`和`user_id`。以下是我们如何在节点中使用语义搜索来查找相关记忆：\n\n```\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore): def  update_memory(state: MessagesState, config: RunnableConfig,*, store: BaseStore):   # Get the user id from the config # Get the user id from the config user_id = config["configurable"]["user_id"] user_id = config["configurable"]["user_id"]   # Namespace the memory # Namespace the memory namespace = (user_id, "memories") namespace =(user_id, "memories")   # ... Analyze conversation and create a new memory # ... Analyze conversation and create a new memory   # Create a new memory ID # Create a new memory ID memory_id = str(uuid.uuid4()) memory_id = str(uuid. uuid4())   # We create a new memory # We create a new memory store.put(namespace, memory_id, {"memory": memory}) store. put(namespace, memory_id,{"memory": memory})\n```\n\n如上所示，我们也可以在任何节点中访问存储并使用`store.search`方法来获取记忆。回想一下，记忆是作为可以转换为字典的对象列表返回的。\n\n```\nmemories[-1].dict() memories[- 1]. dict(){\'value\': {\'food_preference\': \'I like pizza\'}, {\'value\':{\'food_preference\': \'I like pizza\'}, \'key\': \'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'namespace\': [\'1\', \'memories\'], \'namespace\':[\'1\', \'memories\'], \'created_at\': \'2024-10-02T17:22:31.590602+00:00\', \'created_at\':\'2024-10-02T17:22:31.590602+00:00\', \'updated_at\': \'2024-10-02T17:22:31.590605+00:00\'} \'updated_at\':\'2024-10-02T17:22:31.590605+00:00\'}\n```\n\n我们可以访问记忆并在我们的模型调用中使用它们。\n\n```\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore): def  call_model(state: MessagesState, config: RunnableConfig,*, store: BaseStore):  # Get the user id from the config # Get the user id from the config user_id = config["configurable"]["user_id"] user_id = config["configurable"]["user_id"]   # Namespace the memory # Namespace the memory namespace = (user_id, "memories") namespace =(user_id, "memories")   # Search based on the most recent message # Search based on the most recent message memories = store.search( memories = store. search( namespace, namespace, query=state["messages"][-1].content, query = state["messages"][- 1]. content, limit=3 limit = 3 ) ) info = "\\n".join([d.value["memory"] for d in memories]) info = " \\n ". join([d. value["memory"] for d in memories])   # ... Use memories in the model call # ... Use memories in the model call\n```\n\n如果我们创建一个新线程，只要`user_id`相同，我们仍然可以访问相同的记忆。\n\n```\n # Invoke the graph # Invoke the graphconfig = {"configurable": {"thread_id": "2", "user_id": "1"}} config ={"configurable":{"thread_id": "2", "user_id": "1"}}  # Let\'s say hi again # Let\'s say hi againfor update in graph.stream( for update in graph. stream( {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates" {"messages":[{"role": "user", "content":"hi, tell me about my memories"}]}, config, stream_mode = "updates"): ): print(update) print(update)\n```\n\n当我们使用 LangGraph 平台时，无论是在本地（例如，在 LangGraph Studio 中）还是在 LangGraph 平台上，基础存储默认可用，无需在图编译期间指定。然而，要启用语义搜索，您**确实**需要在您的`langgraph.json`文件中配置索引设置。例如：\n\n```\n{ { ...  ... "store": {  "store":  { "index": {  "index":  { "embed": "openai:text-embeddings-3-small",  "embed":  "openai:text-embeddings-3-small", "dims": 1536,  "dims":  1536, "fields": ["$"]  "fields":  ["$"] }  } }  }} }\n```\n\n更多详情和配置选项，请参见[部署指南](../../cloud/deployment/semantic_search/)。\n\n## 检查点库[¶](#checkpointer-libraries "Permanent link")\n\n在底层，检查点功能由符合[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)接口的检查点对象提供支持。LangGraph 提供了几种检查点实现，都是通过独立、可安装的库实现的：\n\n* `langgraph-checkpoint`: 检查点保存器（[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)）和序列化/反序列化接口（[SerializerProtocol](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)）的基础接口。包括用于实验的内存检查点实现（[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)）。LangGraph 自带`langgraph-checkpoint`。\n* `langgraph-checkpoint-sqlite`: 使用 SQLite 数据库的 LangGraph 检查点实现（[SqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [AsyncSqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)）。非常适合实验和本地工作流。需要单独安装。\n* `langgraph-checkpoint-postgres`: 使用 Postgres 数据库的高级检查点（[PostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [AsyncPostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)），在 LangGraph 平台中使用。非常适合在生产环境中使用。需要单独安装。\n\n### 检查点接口[¶](#checkpointer-interface "Permanent link")\n\n每个检查点都符合[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)接口，并实现以下方法：\n\n* `.put` - 存储一个检查点及其配置和元数据。\n* `.put_writes` - 存储与一个检查点相关联的中间写入（即[待定写入](#pending-writes)）。\n* `.get_tuple` - 使用给定的配置（`thread_id`和`checkpoint_id`）获取一个检查点元组。这用于在`graph.get_state()`中填充`StateSnapshot`。\n* `.list` - 列出匹配给定配置和筛选条件的检查点。这用于在`graph.get_state_history()`中填充状态历史。\n\n如果检查点用于异步图执行（即通过`.ainvoke`、`.astream`、`.abatch`执行图），则将使用上述方法的异步版本（`.aput`、`.aput_writes`、`.aget_tuple`、`.alist`）。\n\n注意\n\n为了异步运行您的图，您可以使用`InMemorySaver`，或 Sqlite/Postgres 检查点的异步版本——`AsyncSqliteSaver` / `AsyncPostgresSaver`检查点。\n\n### 序列化器[¶](#serializer "Permanent link")\n\n当检查点保存图状态时，它们需要序列化状态中的通道值。这是使用序列化器对象完成的。\n\n`langgraph_checkpoint`定义了实现序列化器的[协议](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)，并提供了一个默认实现（[JsonPlusSerializer](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)），该实现处理多种类型，包括 LangChain 和 LangGraph 的原生类型、日期时间、枚举等。\n\n#### 使用`pickle`进行序列化[¶](#serialization-with-pickle "Permanent link")\n\n默认的序列化器[`JsonPlusSerializer`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)在底层使用 ormsgpack 和 JSON，这不适用于所有类型的对象。\n\n如果您想对我们 msgpack 编码器目前不支持的对象（如 Pandas 数据帧）回退到使用 pickle，您可以使用`JsonPlusSerializer`的`pickle_fallback`参数。\n\n*API 参考：[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) | [JsonPlusSerializer](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)*\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaverfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer from  langgraph.checkpoint.serde.jsonplus  import JsonPlusSerializer  # ... Define the graph ... # ... Define the graph ...graph.compile( graph. compile( checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True)) checkpointer = InMemorySaver(serde = JsonPlusSerializer(pickle_fallback = True))) )\n```\n\n#### 加密[¶](#encryption "Permanent link")\n\n检查点可以有选择地加密所有持久化的状态。要启用此功能，请将[`EncryptedSerializer`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer)的实例传递给任何`BaseCheckpointSaver`实现的`serde`参数。创建加密序列化器的最简单方法是通过[`from_pycryptodome_aes`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes)，它从`LANGGRAPH_AES_KEY`环境变量中读取 AES 密钥（或接受一个`key`参数）。\n\n*API 参考：[SqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver)*\n\n```\n import sqlite3 import  sqlite3  from langgraph.checkpoint.serde.encrypted import EncryptedSerializer from  langgraph.checkpoint.serde.encrypted  import EncryptedSerializerfrom langgraph.checkpoint.sqlite import SqliteSaver from  langgraph.checkpoint.sqlite  import SqliteSaver  serde = EncryptedSerializer.from_pycryptodome_aes() # reads LANGGRAPH_AES_KEY serde = EncryptedSerializer. from_pycryptodome_aes() # reads LANGGRAPH_AES_KEYcheckpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde) checkpointer = SqliteSaver(sqlite3. connect("checkpoint.db"), serde = serde)\n```\n\n*API 参考：[PostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver)*\n\n```\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer from  langgraph.checkpoint.serde.encrypted  import EncryptedSerializerfrom langgraph.checkpoint.postgres import PostgresSaver from  langgraph.checkpoint.postgres  import PostgresSaver  serde = EncryptedSerializer.from_pycryptodome_aes() serde = EncryptedSerializer. from_pycryptodome_aes()checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde) checkpointer = PostgresSaver. from_conn_string("postgresql://...", serde = serde)checkpointer.setup() checkpointer. setup()\n```\n\n在 LangGraph 平台上运行时，只要存在`LANGGRAPH_AES_KEY`，加密就会自动启用，因此您只需提供环境变量即可。可以通过实现[`CipherProtocol`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol)并将其提供给`EncryptedSerializer`来使用其他加密方案。\n\n## 功能[¶](#capabilities "Permanent link")\n\n### 人机协作 (Human-in-the-loop)[¶](#human-in-the-loop "Permanent link")\n\n首先，检查点通过允许人类检查、中断和批准图步骤，促进了[人在环路的工作流](../agentic_concepts/#human-in-the-loop)。这些工作流需要检查点，因为人类必须能够随时查看图的状态，并且图必须能够在人类对状态进行任何更新后恢复执行。请参阅[操作指南](../../how-tos/human_in_the_loop/add-human-in-the-loop/)中的示例。\n\n### 内存[¶](#memory "Permanent link")\n\n其次，检查点允许在交互之间建立[“记忆”](../memory/)。在重复的人类交互（如对话）的情况下，任何后续消息都可以发送到该线程，该线程将保留其先前交互的记忆。有关如何使用检查点添加和管理对话记忆的信息，请参见[添加记忆](../../how-tos/memory/add-memory/)。\n\n### 时光穿梭[¶](#time-travel "Permanent link")\n\n第三，检查点允许[“时间旅行”](../time-travel/)，允许用户重放先前的图执行，以审查和/或调试特定的图步骤。此外，检查点还可以在任意检查点分叉图状态，以探索不同的轨迹。\n\n### 容错[¶](#fault-tolerance "Permanent link")\n\n最后，检查点还提供了容错和错误恢复功能：如果一个或多个节点在给定的超级步骤中失败，您可以从最后一个成功的步骤重新启动您的图。此外，当一个图节点在给定的超级步骤中执行失败时，LangGraph 会存储在该超级步骤中成功完成的任何其他节点的待定检查点写入，这样每当我们从该超级步骤恢复图执行时，我们就不必重新运行成功的节点。\n\n#### 待定写入[¶](#pending-writes "Permanent link")\n\n此外，当一个图节点在给定的超级步骤中执行失败时，LangGraph 会存储在该超级步骤中成功完成的任何其他节点的待定检查点写入，这样每当我们从该超级步骤恢复图执行时，我们就不必重新运行成功的节点。', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraph/concepts/persistence/', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.9999943, 'saved_path': None}}
2026-02-22 20:38:00,069 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:38:00,070 - __main__ - INFO - call_tool payload: source_tool=tavily_search, result_type=papers, count=1
2026-02-22 20:38:00,070 - __main__ - INFO - call_tool: name=tavily_search, result_type=papers, count=1
2026-02-22 20:38:00,070 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '人机协作(Human-in-the-Loop) - LangChain 文档', 'authors': [], 'abstract': '* [用例](#use-cases)\n* [中断](#interrupt)\n* [要求](#requirements)\n* [设计模式](#design-patterns) \n\n  + [批准或拒绝](#approve-or-reject)\n  + [审查与编辑状态](#review-edit-state)\n  + [审查工具调用](#review-tool-calls)\n  + [多轮对话](#multi-turn-conversation)\n  + [验证人类输入](#validating-human-input)\n* [Command 原语](#the-command-primitive)\n* [与 invoke 结合使用](#using-with-invoke)\n* [从中断恢复如何工作？](#how-does-resuming-from-an-interrupt-work)\n* [常见陷阱](#common-pitfalls) \n\n  + [副作用](#side-effects)\n  + [作为函数调用的子图](#subgraphs-called-as-functions)\n  + [使用多个中断](#using-multiple-interrupts)\n* [附加资源 📚](#additional-resources)\n\n1. [LangGraph](../..)\n2. [指南](../../how-tos/)\n3. [概念](../)\n4. [LangGraph](../../concepts#langgraph)\n\n# 人机协作 (Human-in-the-loop)[¶](#human-in-the-loop "Permanent link")\n\n本指南使用新的 `interrupt` 函数。\n\n自 LangGraph 0.2.31 起，推荐使用 [`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 来设置断点，因为它简化了**人机协作（human-in-the-loop）**模式。\n\n如果您正在寻找此概念指南的先前版本，该版本依赖于静态断点和 `NodeInterrupt` 异常，请点击[此处](../v0-human-in-the-loop/)。\n\n**人机协作（human-in-the-loop）**（或“在环”）工作流将人类输入集成到自动化流程中，允许在关键阶段进行决策、验证或更正。这在**基于 LLM 的应用程序**中尤其有用，因为底层模型可能会偶尔产生不准确的内容。在合规、决策或内容生成等低容错场景中，人类参与通过允许审查、更正或覆盖模型输出来确保可靠性。\n\n## 用例[¶](#use-cases "Permanent link")\n\n基于 LLM 应用程序中**人机协作**工作流的主要用例包括：\n\n1. [**🛠️ 审查工具调用**](#review-tool-calls)：人类可以在工具执行前审查、编辑或批准 LLM 请求的工具调用。\n2. **✅ 验证 LLM 输出**：人类可以审查、编辑或批准 LLM 生成的内容。\n3. **💡 提供上下文**：使 LLM 能够明确请求人类输入以进行澄清或提供额外细节，或支持多轮对话。\n\n## `interrupt`[¶](#interrupt "Permanent link")\n\nLangGraph 中的 [`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 通过在特定节点暂停图，向人类呈现信息，并用他们的输入恢复图，从而实现人机协作工作流。此函数对于批准、编辑或收集额外输入等任务非常有用。[`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 与 [`Command`](/langgraphjs/reference/classes/langgraph.Command.html) 对象结合使用，以人类提供的值恢复图。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { const value = interrupt(  const  value  =  interrupt( // Any JSON serializable value to surface to the human.  // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state  // For example, a question or a piece of text or a set of keys in the state {  { text_to_revise: state.some_text,  text_to_revise:  state.some_text, }  } );  ); // Update the state with the human\'s input or route the graph based on the input  // Update the state with the human\'s input or route the graph based on the input return {  return  { some_text: value,  some_text:  value, };  };} }  const graph = workflow.compile({ const  graph  =  workflow. compile({ checkpointer, // Required for `interrupt` to work  checkpointer,  // Required for `interrupt` to work}); });  // Run the graph until the interrupt // Run the graph until the interruptconst threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke(someInput, threadConfig); await  graph. invoke(someInput,  threadConfig);  // Below code can run some amount of time later and/or in a different process // Below code can run some amount of time later and/or in a different process  // Human input // Human inputconst valueFromHuman = "..."; const  valueFromHuman  =  "...";  // Resume the graph with the human\'s input // Resume the graph with the human\'s inputawait graph.invoke(new Command({ resume: valueFromHuman }), threadConfig); await  graph. invoke(new  Command({ resume:  valueFromHuman  }),  threadConfig);\n```\n\n```\n{ { some_text: "Edited text";  some_text:  "Edited text";} }\n```\n\n 完整代码\n\n以下是关于如何在图中使用 `interrupt` 的完整示例，如果您想查看代码的实际运行情况。\n\n```\nimport { MemorySaver, Annotation, interrupt, Command, StateGraph } from "@langchain/langgraph"; import  { MemorySaver,  Annotation,  interrupt,  Command,  StateGraph  }  from  "@langchain/langgraph";  // Define the graph state // Define the graph stateconst StateAnnotation = Annotation.Root({ const  StateAnnotation  =  Annotation. Root({ some_text: Annotation<string>()  some_text:  Annotation< string>()}); });  function humanNode(state: typeof StateAnnotation.State) { function  humanNode(state:  typeof  StateAnnotation. State)  { const value = interrupt(  const  value  =  interrupt( // Any JSON serializable value to surface to the human.  // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state  // For example, a question or a piece of text or a set of keys in the state {  { text_to_revise: state.some_text  text_to_revise:  state.some_text }  } );  ); return {  return  { // Update the state with the human\'s input  // Update the state with the human\'s input some_text: value  some_text:  value };  };} }  // Build the graph // Build the graphconst workflow = new StateGraph(StateAnnotation) const  workflow  =  new  StateGraph(StateAnnotation)// Add the human-node to the graph // Add the human-node to the graph .addNode("human_node", humanNode)  . addNode("human_node",  humanNode) .addEdge("__start__", "human_node")  . addEdge("__start__",  "human_node")  // A checkpointer is required for `interrupt` to work. // A checkpointer is required for `interrupt` to work.const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();const graph = workflow.compile({ const  graph  =  workflow. compile({ checkpointer  checkpointer}); });  // Using stream() to directly surface the `__interrupt__` information. // Using stream() to directly surface the `__interrupt__` information.for await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( { some_text: "Original text" },  { some_text:  "Original text"  },  threadConfig  threadConfig)) { ))  { console.log(chunk);  console. log(chunk);} }  // Resume using Command // Resume using Commandfor await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( new Command({ resume: "Edited text" }),  new  Command({ resume:  "Edited text"  }),  threadConfig  threadConfig)) { ))  { console.log(chunk);  console. log(chunk);} }\n```\n\n```\n{ { __interrupt__: [  __interrupt__:  [ {  { value: { question: \'Please revise the text\', some_text: \'Original text\' },  value:  { question:  \'Please revise the text\',  some_text:  \'Original text\'  }, resumable: true,  resumable:  true, ns: [\'human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6\'],  ns:  [\'human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6\'], when: \'during\'  when:  \'during\' }  } ]  ]} }{ human_node: { some_text: \'Edited text\' } } { human_node:  { some_text:  \'Edited text\'  }  }\n```\n\n## 要求[¶](#requirements "永久链接")\n\n要在图中使用 `interrupt`，您需要：\n\n1. [**指定检查点**](../persistence/#checkpoints) 以在每一步之后保存图状态。\n2. 在适当的位置**调用 `interrupt()`**。请参阅[设计模式](#design-patterns)部分以获取示例。\n3. 使用[**线程 ID**](../persistence/#threads) **运行图**，直到触发 `interrupt`。\n4. 使用 `invoke`/`stream` **恢复执行**（请参阅[**`Command` 原语**](#the-command-primitive)）。\n\n## 设计模式[¶](#design-patterns "Permanent link")\n\n通常，您可以通过人机协作工作流执行三种不同的**操作**：\n\n1. **批准或拒绝**：在关键步骤（例如 API 调用）之前暂停图，以审查和批准操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。此模式通常涉及根据人类的输入对图进行**路由**。\n2. **编辑图状态**：暂停图以审查和编辑图状态。这对于纠正错误或使用附加信息更新状态很有用。此模式通常涉及使用人类的输入**更新**状态。\n3. **获取输入**：在图的特定步骤中明确请求人类输入。这对于收集额外信息或上下文以指导代理的决策过程或支持**多轮对话**很有用。\n\n下面我们展示了可以使用这些**操作**实现的不同设计模式。\n\n**注意：** `interrupt` 函数通过抛出特殊的 `GraphInterrupt` 错误来传播。因此，您应该避免在 `interrupt` 函数周围使用 `try/catch` 块——如果确实使用了，请确保在 `catch` 块中再次抛出 `GraphInterrupt` 错误。\n\n### 批准或拒绝[¶](#approve-or-reject "Permanent link")\n\n在关键步骤（例如 API 调用）之前暂停图，以审查和批准操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。\n\n```\nimport { interrupt, Command } from "@langchain/langgraph"; import  { interrupt,  Command  }  from  "@langchain/langgraph";  function humanApproval(state: typeof GraphAnnotation.State): Command { function  humanApproval(state:  typeof  GraphAnnotation. State):  Command  { const isApproved = interrupt({  const  isApproved  =  interrupt({ question: "Is this correct?",  question:  "Is this correct?", // Surface the output that should be  // Surface the output that should be // reviewed and approved by the human.  // reviewed and approved by the human. llm_output: state.llm_output,  llm_output:  state.llm_output, });  });   if (isApproved) {  if  (isApproved)  { return new Command({ goto: "some_node" });  return  new  Command({ goto:  "some_node"  }); } else {  }  else  { return new Command({ goto: "another_node" });  return  new  Command({ goto:  "another_node"  }); }  }} }  // Add the node to the graph in an appropriate location // Add the node to the graph in an appropriate location// and connect it to the relevant nodes. // and connect it to the relevant nodes.const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_approval", humanApproval)  . addNode("human_approval",  humanApproval) .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with either an approval or rejection. // Resume it with either an approval or rejection.const threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke(new Command({ resume: true }), threadConfig); await  graph. invoke(new  Command({ resume:  true  }),  threadConfig);\n```\n\n有关更详细的示例，请参阅[如何审查工具调用](/langgraphjs/how-tos/review-tool-calls)。\n\n### 审查与编辑状态[¶](#review-edit-state "Permanent link")\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanEditing(state: typeof GraphAnnotation.State): Command { function  humanEditing(state:  typeof  GraphAnnotation. State):  Command  { const result = interrupt({  const  result  =  interrupt({ // Interrupt information to surface to the client.  // Interrupt information to surface to the client. // Can be any JSON serializable value.  // Can be any JSON serializable value. task: "Review the output from the LLM and make any necessary edits.",  task:  "Review the output from the LLM and make any necessary edits.", llm_generated_summary: state.llm_generated_summary,  llm_generated_summary:  state.llm_generated_summary, });  });   // Update the state with the edited text  // Update the state with the edited text return {  return  { llm_generated_summary: result.edited_text,  llm_generated_summary:  result.edited_text, };  };} }  // Add the node to the graph in an appropriate location // Add the node to the graph in an appropriate location// and connect it to the relevant nodes. // and connect it to the relevant nodes.const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_editing", humanEditing)  . addNode("human_editing",  humanEditing) .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with the edited text. // Resume it with the edited text.const threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke( await  graph. invoke( new Command({ resume: { edited_text: "The edited text" } }),  new  Command({ resume:  { edited_text:  "The edited text"  }  }),  threadConfig  threadConfig); );\n```\n\n有关更详细的示例，请参阅[如何使用中断等待用户输入](/langgraphjs/how-tos/wait-user-input)。\n\n### 审查工具调用[¶](#review-tool-calls "Permanent link")\n\n```\nimport { interrupt, Command } from "@langchain/langgraph"; import  { interrupt,  Command  }  from  "@langchain/langgraph";  function humanReviewNode(state: typeof GraphAnnotation.State): Command { function  humanReviewNode(state:  typeof  GraphAnnotation. State):  Command  { // This is the value we\'ll be providing via Command.resume()  // This is the value we\'ll be providing via Command.resume() const humanReview = interrupt({  const  humanReview  =  interrupt({ question: "Is this correct?",  question:  "Is this correct?", // Surface tool calls for review  // Surface tool calls for review tool_call: toolCall,  tool_call:  toolCall, });  });   const [reviewAction, reviewData] = humanReview;  const  [reviewAction,  reviewData]  =  humanReview;   // Approve the tool call and continue  // Approve the tool call and continue if (reviewAction === "continue") {  if  (reviewAction  ===  "continue")  { return new Command({ goto: "run_tool" });  return  new  Command({ goto:  "run_tool"  }); }  } // Modify the tool call manually and then continue  // Modify the tool call manually and then continue else if (reviewAction === "update") {  else  if  (reviewAction  ===  "update")  { const updatedMsg = getUpdatedMsg(reviewData);  const  updatedMsg  =  getUpdatedMsg(reviewData); // Remember that to modify an existing message you will need  // Remember that to modify an existing message you will need // to pass the message with a matching ID.  // to pass the message with a matching ID. return new Command({  return  new  Command({ goto: "run_tool",  goto:  "run_tool", update: { messages: [updatedMsg] },  update:  { messages:  [updatedMsg]  }, });  }); }  } // Give natural language feedback, and then pass that back to the agent  // Give natural language feedback, and then pass that back to the agent else if (reviewAction === "feedback") {  else  if  (reviewAction  ===  "feedback")  { const feedbackMsg = getFeedbackMsg(reviewData);  const  feedbackMsg  =  getFeedbackMsg(reviewData); return new Command({  return  new  Command({ goto: "call_llm",  goto:  "call_llm", update: { messages: [feedbackMsg] },  update:  { messages:  [feedbackMsg]  }, });  }); }  }} }\n```\n\n有关更详细的示例，请参阅[如何审查工具调用](/langgraphjs/how-tos/review-tool-calls)。\n\n### 多轮对话[¶](#multi-turn-conversation "Permanent link")\n\n**多轮对话**涉及代理和人类之间的多次来回交互，这可以允许代理以对话方式从人类那里收集额外信息。\n\n这种设计模式在由[多个代理](../multi_agent/)组成的 LLM 应用程序中很有用。一个或多个代理可能需要与人类进行多轮对话，其中人类在对话的不同阶段提供输入或反馈。为简单起见，下面的代理实现被说明为单个节点，但实际上它可能是由多个节点组成的更大图的一部分，并包含条件边。\n\n在此模式中，每个代理都有自己的人类节点用于收集用户输入。\n\n这可以通过为人类节点使用唯一名称（例如，“代理 1 的人类节点”，“代理 2 的人类节点”）或使用子图（其中子图包含人类节点和代理节点）来实现。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanInput(state: typeof GraphAnnotation.State) { function  humanInput(state:  typeof  GraphAnnotation. State)  { const humanMessage = interrupt("human_input");  const  humanMessage  =  interrupt("human_input");   return {  return  { messages: [  messages:  [ {  { role: "human",  role:  "human", content: humanMessage  content:  humanMessage }  } ]  ] };  };} }  function agent(state: typeof GraphAnnotation.State) { function  agent(state:  typeof  GraphAnnotation. State)  { // Agent logic  // Agent logic // ...  // ...} }  const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_input", humanInput)  . addNode("human_input",  humanInput) .addEdge("human_input", "agent")  . addEdge("human_input",  "agent") .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with the human\'s input. // Resume it with the human\'s input.await graph.invoke( await  graph. invoke( new Command({ resume: "hello!" }),  new  Command({ resume:  "hello!"  }),  threadConfig  threadConfig); );\n```\n\n在此模式中，单个人类节点用于收集多个代理的用户输入。活动代理从状态中确定，因此在收集人类输入后，图可以路由到正确的代理。\n\n```\nimport { interrupt, Command, MessagesAnnotation } from "@langchain/langgraph"; import  { interrupt,  Command,  MessagesAnnotation  }  from  "@langchain/langgraph";  function humanNode(state: typeof MessagesAnnotation.State): Command { function  humanNode(state:  typeof  MessagesAnnotation. State):  Command  { /**  /** * A node for collecting user input.  * A node for collecting user input. */  */ const userInput = interrupt("Ready for user input.");  const  userInput  =  interrupt("Ready for user input.");   // Determine the **active agent** from the state, so  // Determine the **active agent** from the state, so // we can route to the correct agent after collecting input.  // we can route to the correct agent after collecting input. // For example, add a field to the state or use the last active agent.  // For example, add a field to the state or use the last active agent. // or fill in `name` attribute of AI messages generated by the agents.  // or fill in `name` attribute of AI messages generated by the agents. const activeAgent = ...;  const  activeAgent  =  ...;   return new Command({  return  new  Command({ goto: activeAgent,  goto:  activeAgent, update: {  update:  { messages: [{  messages:  [{ role: "human",  role:  "human", content: userInput,  content:  userInput, }]  }] }  } });  });} }\n```\n\n有关更详细的示例，请参阅[如何实现多轮对话](/langgraphjs/how-tos/multi-agent-multi-turn-convo)。\n\n### 验证人类输入[¶](#validating-human-input "Permanent link")\n\n如果您需要在图本身中（而不是在客户端）验证人类提供的输入，可以通过在单个节点中使用多个中断调用来实现。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */ let question = "What is your age?";  let  question  =  "What is your age?";   while (true) {  while  (true)  { const answer = interrupt(question);  const  answer  =  interrupt(question);   // Validate answer, if the answer isn\'t valid ask for input again.  // Validate answer, if the answer isn\'t valid ask for input again. if (typeof answer !== "number" || answer < 0) {  if  (typeof  answer  !==  "number"  ||  answer  <  0)  { question = `\'${answer}\' is not a valid age. What is your age?`;  question  =  `\'${answer}\' is not a valid age. What is your age?`; continue;  continue; } else {  }  else  { // If the answer is valid, we can proceed.  // If the answer is valid, we can proceed. break;  break; }  } }  }   console.log(`The human in the loop is ${answer} years old.`);  console. log(`The human in the loop is ${answer} years old.`);   return {  return  { age: answer,  age:  answer, };  };} }\n```\n\n## Command 原语[¶](#the-command-primitive "Permanent link")\n\n当使用 `interrupt` 函数时，图将在中断处暂停并等待用户输入。\n\n图的执行可以使用 [Command](/langgraphjs/reference/classes/langgraph.Command.html) 原语恢复，该原语可以通过 `invoke` 或 `stream` 方法传递。\n\n`Command` 原语提供了几个选项来控制和修改恢复期间图的状态：\n\n1. **将值传递给 `interrupt`**：使用 `new Command({ resume: value })` 向图提供数据，例如用户的响应。执行从使用 `interrupt` 的节点的开头恢复，但是，这次 `interrupt(...)` 调用将返回在 `new Command({ resume: value })` 中传递的值，而不是暂停图。\n\n```\n// Resume graph execution with the user\'s input. // Resume graph execution with the user\'s input.await graph.invoke(new Command({ resume: { age: "25" } }), threadConfig); await  graph. invoke(new  Command({ resume:  { age:  "25"  }  }),  threadConfig);\n```\n\n1. **更新图状态**：使用 `Command({ goto: ..., update: ... })` 修改图状态。请注意，恢复从使用 `interrupt` 的节点的开头开始。执行从使用 `interrupt` 的节点的开头恢复，但带有更新后的状态。\n\n```\n// Update the graph state and resume. // Update the graph state and resume.// You must provide a `resume` value if using an `interrupt`. // You must provide a `resume` value if using an `interrupt`.await graph.invoke( await  graph. invoke( new Command({ resume: "Let\'s go!!!", update: { foo: "bar" } }),  new  Command({ resume:  "Let\'s go!!!",  update:  { foo:  "bar"  }  }),  threadConfig  threadConfig); );\n```\n\n通过利用 `Command`，您可以恢复图的执行，处理用户输入，并动态调整图的状态。\n\n## 与 `invoke` 结合使用[¶](#using-with-invoke "Permanent link")\n\n当您使用 `stream` 运行图时，您将收到一个 `Interrupt` 事件，它会通知您 `interrupt` 已被触发。\n\n`invoke` 不会返回中断信息。要访问此信息，您必须在调用 `invoke` 后使用 [getState](/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#getState) 方法检索图状态。\n\n```\n// Run the graph up to the interrupt // Run the graph up to the interruptconst result = await graph.invoke(inputs, threadConfig); const  result  =  await  graph. invoke(inputs,  threadConfig);  // Get the graph state to get interrupt information. // Get the graph state to get interrupt information.const state = await graph.getState(threadConfig); const  state  =  await  graph. getState(threadConfig);  // Print the state values // Print the state valuesconsole.log(state.values); console. log(state. values);  // Print the pending tasks // Print the pending tasksconsole.log(state.tasks); console. log(state. tasks);  // Resume the graph with the user\'s input. // Resume the graph with the user\'s input.await graph.invoke(new Command({ resume: { age: "25" } }), threadConfig); await  graph. invoke(new  Command({ resume:  { age:  "25"  }  }),  threadConfig);\n```\n\n```\n{ { foo: "bar";  foo:  "bar";} // State values }  // State values  [ [ {  { id: "5d8ffc92-8011-0c9b-8b59-9d3545b7e553",  id:  "5d8ffc92-8011-0c9b-8b59-9d3545b7e553", name: "node_foo",  name:  "node_foo", path: ["__pregel_pull", "node_foo"],  path:  ["__pregel_pull",  "node_foo"], error: null,  error:  null, interrupts: [  interrupts:  [ {  { value: "value_in_interrupt",  value:  "value_in_interrupt", resumable: true,  resumable:  true, ns: ["node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553"],  ns:  ["node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553"], when: "during",  when:  "during", },  }, ],  ], state: null,  state:  null, result: null,  result:  null, },  },]; // Pending tasks. interrupts ];  // Pending tasks. interrupts\n```\n\n## 从中断恢复如何工作？[¶](#how-does-resuming-from-an-interrupt-work "Permanent link")\n\n使用 `interrupt` 的一个关键方面是理解恢复的工作原理。当您在 `interrupt` 后恢复执行时，图的执行从上次触发 `interrupt` 的**图节点**的**开头**开始。\n\n从节点开头到 `interrupt` 的**所有**代码都将重新执行。\n\n```\nlet counter = 0; let  counter  =  0;  function node(state: State) { function  node(state:  State)  { // All the code from the beginning of the node to the interrupt will be re-executed  // All the code from the beginning of the node to the interrupt will be re-executed // when the graph resumes.  // when the graph resumes. counter += 1;  counter  +=  1;   console.log(`> Entered the node: ${counter} # of times`);  console. log(`> Entered the node: ${counter}  # of times`);   // Pause the graph and wait for user input.  // Pause the graph and wait for user input. const answer = interrupt();  const  answer  =  interrupt();   console.log("The value of counter is:", counter);  console. log("The value of counter is:",  counter); // ...  // ...} }\n```\n\n在**恢复**图时，计数器将第二次递增，导致以下输出：\n\n```\n> Entered the node: 2 # of times >  Entered  the  node:  2  #  of  timesThe value of counter is: 2 The  value  of  counter  is:  2\n```\n\n## 常见陷阱[¶](#common-pitfalls "永久链接")\n\n### 副作用[¶](#side-effects "Permanent link")\n\n将带有副作用的代码（例如 API 调用）放在 `interrupt` **之后**，以避免重复，因为这些代码在每次节点恢复时都会重新触发。\n\n当节点从 `interrupt` 恢复时，此代码将再次重新执行 API 调用。如果 API 调用不是幂等的或者成本很高，这可能会导致问题。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */ apiCall(); // This code will be re-executed when the node is resumed.  apiCall();  // This code will be re-executed when the node is resumed.   const answer = interrupt(question);  const  answer  =  interrupt(question);} }\n```\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */   const answer = interrupt(question);  const  answer  =  interrupt(question);   apiCall(answer); // OK as it\'s after the interrupt  apiCall(answer);  // OK as it\'s after the interrupt} }\n```\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */   const answer = interrupt(question);  const  answer  =  interrupt(question);   return {  return  { answer  answer };  };} }  function apiCallNode(state: typeof GraphAnnotation.State) { function  apiCallNode(state:  typeof  GraphAnnotation. State)  { apiCall(); // OK as it\'s in a separate node  apiCall();  // OK as it\'s in a separate node} }\n```\n\n### 作为函数调用的子图[¶](#subgraphs-called-as-functions "Permanent link")\n\n当[作为函数](../low_level/#as-a-function)调用子图时，**父图**将从调用子图的**节点开头**（以及触发 `interrupt` 的地方）恢复执行。同样，**子图**将从调用 `interrupt()` 函数的**节点开头**恢复。\n\n例如：\n\n```\nasync function nodeInParentGraph(state: typeof GraphAnnotation.State) { async  function  nodeInParentGraph(state:  typeof  GraphAnnotation. State)  { someCode(); // <-- This will re-execute when the subgraph is resumed.  someCode();  // <-- This will re-execute when the subgraph is resumed. // Invoke a subgraph as a function.  // Invoke a subgraph as a function. // The subgraph contains an `interrupt` call.  // The subgraph contains an `interrupt` call. const subgraphResult = await subgraph.invoke(someInput);  const  subgraphResult  =  await  subgraph. invoke(someInput); ...  ...} }\n```\n\n **示例：父图和子图的执行流程**\n\n假设我们有一个包含 3 个节点的父图：\n\n**父图**：`node_1` → `node_2`（子图调用） → `node_3`\n\n子图有 3 个节点，其中第二个节点包含 `interrupt`：\n\n**子图**：`sub_node_1` → `sub_node_2`（`interrupt`） → `sub_node_3`\n\n恢复图时，执行将按以下方式进行：\n\n1. **跳过父图中的 `node_1`**（已执行，图状态已保存为快照）。\n2. **从头开始重新执行父图中的 `node_2`**。\n3. **跳过子图中的 `sub_node_1`**（已执行，图状态已保存为快照）。\n4. **从头开始重新执行子图中的 `sub_node_2`**。\n5. 继续执行 `sub_node_3` 和后续节点。\n\n这是一个缩写的示例代码，您可以用来理解子图如何与中断一起工作。它计算每个节点进入的次数并打印计数。\n\n```\nimport { import  { StateGraph,  StateGraph, START,  START, interrupt,  interrupt, Command,  Command, MemorySaver,  MemorySaver,  Annotation  Annotation} from "@langchain/langgraph"; }  from  "@langchain/langgraph";  const GraphAnnotation = Annotation.Root({ const  GraphAnnotation  =  Annotation. Root({ stateCounter: Annotation<number>({  stateCounter:  Annotation< number>({ reducer: (a, b) => a + b,  reducer:  (a,  b)  =>  a  +  b, default: () => 0  default:  ()  =>  0 })  })}) })  let counterNodeInSubgraph = 0; let  counterNodeInSubgraph  =  0;  function nodeInSubgraph(state: typeof GraphAnnotation.State) { function  nodeInSubgraph(state:  typeof  GraphAnnotation. State)  { counterNodeInSubgraph += 1; // This code will **NOT** run again!  counterNodeInSubgraph  +=  1;  // This code will **NOT** run again! console.log(`Entered \'nodeInSubgraph\' a total of ${counterNodeInSubgraph} times`);  console. log(`Entered \'nodeInSubgraph\' a total of ${counterNodeInSubgraph}  times`); return {};  return  {};} }  let counterHumanNode = 0; let  counterHumanNode  =  0;  async function humanNode(state: typeof GraphAnnotation.State) { async  function  humanNode(state:  typeof  GraphAnnotation. State)  { counterHumanNode += 1; // This code will run again!  counterHumanNode  +=  1;  // This code will run again! console.log(`Entered humanNode in sub-graph a total of ${counterHumanNode} times`);  console. log(`Entered humanNode in sub-graph a total of ${counterHumanNode}  times`); const answer = await interrupt("what is your name?");  const  answer  =  await  interrupt("what is your name?"); console.log(`Got an answer of ${answer}`);  console. log(`Got an answer of ${answer} `); return {};  return  {};} }  const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();  const subgraphBuilder = new StateGraph(GraphAnnotation) const  subgraphBuilder  =  new  StateGraph(GraphAnnotation) .addNode("some_node", nodeInSubgraph)  . addNode("some_node",  nodeInSubgraph) .addNode("human_node", humanNode)  . addNode("human_node",  humanNode) .addEdge(START, "some_node")  . addEdge(START,  "some_node") .addEdge("some_node", "human_node")  . addEdge("some_node",  "human_node")const subgraph = subgraphBuilder.compile({ checkpointer }); const  subgraph  =  subgraphBuilder. compile({ checkpointer  });  let counterParentNode = 0; let  counterParentNode  =  0;  async function parentNode(state: typeof GraphAnnotation.State) { async  function  parentNode(state:  typeof  GraphAnnotation. State)  { counterParentNode += 1; // This code will run again on resuming!  counterParentNode  +=  1;  // This code will run again on resuming! console.log(`Entered \'parentNode\' a total of ${counterParentNode} times`);  console. log(`Entered \'parentNode\' a total of ${counterParentNode}  times`);   // Please note that we\'re intentionally incrementing the state counter  // Please note that we\'re intentionally incrementing the state counter // in the graph state as well to demonstrate that the subgraph update  // in the graph state as well to demonstrate that the subgraph update // of the same key will not conflict with the parent graph (until  // of the same key will not conflict with the parent graph (until const subgraphState = await subgraph.invoke(state);  const  subgraphState  =  await  subgraph. invoke(state); return subgraphState;  return  subgraphState;} }  const builder = new StateGraph(GraphAnnotation) const  builder  =  new  StateGraph(GraphAnnotation) .addNode("parent_node", parentNode)  . addNode("parent_node",  parentNode) .addEdge(START, "parent_node")  . addEdge(START,  "parent_node")  // A checkpointer must be enabled for interrupts to work! // A checkpointer must be enabled for interrupts to work!const graph = builder.compile({ checkpointer }); const  graph  =  builder. compile({ checkpointer  });  const config = { const  config  =  { configurable: {  configurable:  { thread_id: crypto.randomUUID(),  thread_id:  crypto.randomUUID(), }  }}; };  for await (const chunk of await graph.stream({ stateCounter: 1 }, config)) { for  await  (const  chunk  of  await  graph. stream({ stateCounter:  1  },  config))  { console.log(chunk);  console. log(chunk);} }  console.log(\'--- Resuming ---\'); console. log(\'--- Resuming ---\');  for await (const chunk of await graph.stream(new Command({ resume: "35" }), config)) { for  await  (const  chunk  of  await  graph. stream(new  Command({ resume:  "35"  }),  config))  { console.log(chunk);  console. log(chunk);} }\n```\n\n这将打印出\n\n```\n --- First invocation --- ---  First  invocation  ---In parent node: { foo: \'bar\' } In  parent  node:  { foo:  \'bar\'  } Entered \'parentNode\' a total of 1 times Entered  \'parentNode\'  a  total  of  1  times Entered \'nodeInSubgraph\' a total of 1 times Entered  \'nodeInSubgraph\'  a  total  of  1  timesEntered humanNode in sub-graph a total of 1 times Entered  humanNode  in  sub - graph  a  total  of  1  times{ __interrupt__: [{ value: \'what is your name?\', resumable: true, ns: [\'parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b\', \'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c\'], when: \'during\' }] } { __interrupt__:  [{ value:  \'what is your name?\',  resumable:  true,  ns:  [\'parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b\',  \'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c\'],  when:  \'during\'  }]  }  --- Resuming --- ---  Resuming  ---In parent node: { foo: \'bar\' } In  parent  node:  { foo:  \'bar\'  } Entered \'parentNode\' a total of 2 times Entered  \'parentNode\'  a  total  of  2  timesEntered humanNode in sub-graph a total of 2 times Entered  humanNode  in  sub - graph  a  total  of  2  times Got an answer of 35 Got  an  answer  of  35{ parent_node: null } { parent_node:  null  }\n```\n\n### 使用多个中断[¶](#using-multiple-interrupts "Permanent link")\n\n在**单个**节点中使用多个中断可能有助于实现诸如[验证人类输入](#validating-human-input)之类的模式。然而，如果在同一节点中使用多个中断且不小心处理，可能会导致意外行为。\n\n当一个节点包含多个中断调用时，LangGraph 会为执行该任务的节点保留一个特定于任务的恢复值列表。每当执行恢复时，它都会从节点的开头开始。对于遇到的每个中断，LangGraph 都会检查任务的恢复列表中是否存在匹配的值。匹配是**严格基于索引**的，因此中断调用在节点中的顺序至关重要。\n\n为避免问题，请避免在执行之间动态更改节点结构。这包括添加、删除或重新排序中断调用，因为此类更改可能导致索引不匹配。这些问题通常源于非常规模式，例如通过 `Command.resume(...).update(SOME_STATE_MUTATION)` 改变状态或依赖全局变量动态修改节点结构。\n\n 不正确的代码示例\n\n```\nimport { v4 as uuidv4 } from "uuid"; import  { v4  as  uuidv4  }  from  "uuid";import { import  { StateGraph,  StateGraph, MemorySaver,  MemorySaver, START,  START, interrupt,  interrupt, Command,  Command,  Annotation  Annotation} from "@langchain/langgraph"; }  from  "@langchain/langgraph";  const GraphAnnotation = Annotation.Root({ const  GraphAnnotation  =  Annotation. Root({ name: Annotation<string>(),  name:  Annotation< string>(), age: Annotation<string>()  age:  Annotation< string>()}); });  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { let name;  let  name; if (!state.name) {  if  (! state. name)  { name = interrupt("what is your name?");  name  =  interrupt("what is your name?"); } else {  }  else  { name = "N/A";  name  =  "N/A"; }  }   let age;  let  age; if (!state.age) {  if  (! state. age)  { age = interrupt("what is your age?");  age  =  interrupt("what is your age?"); } else {  }  else  { age = "N/A";  age  =  "N/A"; }  }   console.log(`Name: ${name}. Age: ${age}`);  console. log(`Name: ${name}. Age: ${age} `);   return {  return  { age,  age, name,  name, };  };} }  const builder = new StateGraph(GraphAnnotation) const  builder  =  new  StateGraph(GraphAnnotation) .addNode("human_node", humanNode);  . addNode("human_node",  humanNode); .addEdge(START, "human_node");  . addEdge(START,  "human_node");  // A checkpointer must be enabled for interrupts to work! // A checkpointer must be enabled for interrupts to work!const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();  const graph = builder.compile({ checkpointer }); const  graph  =  builder. compile({ checkpointer  });  const config = { const  config  =  { configurable: {  configurable:  { thread_id: uuidv4(),  thread_id:  uuidv4(), }  }}; };  for await (const chunk of await graph.stream({ age: undefined, name: undefined }, config)) { for  await  (const  chunk  of  await  graph. stream({ age:  undefined,  name:  undefined  },  config))  { console.log(chunk);  console. log(chunk);} }  for await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( new Command({ resume: "John", update: { name: "foo" } }),  new  Command({ resume:  "John",  update:  { name:  "foo"  }  }),  config  config)) { ))  { console.log(chunk);  console. log(chunk);} }\n```\n\n```\n{ __interrupt__: [{ { __interrupt__:  [{ value: \'what is your name?\',  value:  \'what is your name?\', resumable: true,  resumable:  true, ns: [\'human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba\'],  ns:  [\'human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba\'], when: \'during\'  when:  \'during\'}]} }]}Name: N/A. Age: John Name:  N/ A.  Age:  John{ human_node: { age: \'John\', name: \'N/A\' } } { human_node:  { age:  \'John\',  name:  \'N/A\'  }  }\n```\n\n## 附加资源 📚[¶](#additional-resources "Permanent link")\n\n* [**概念指南：持久化**](../persistence/#replay)：阅读持久化指南以获取有关重放的更多上下文。\n* [**操作指南：人机协作**](/langgraphjs/how-tos/#human-in-the-loop)：了解如何在 LangGraph 中实现人机协作工作流。\n* [**如何实现多轮对话**](/langgraphjs/how-tos/multi-agent-multi-turn-convo)：了解如何在 LangGraph 中实现多轮对话。', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraphjs/concepts/human_in_the_loop/', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.99990237, 'saved_path': None}}
2026-02-22 20:38:00,200 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:38:00,200 - __main__ - INFO - call_tool payload: source_tool=tavily_search, result_type=papers, count=1
2026-02-22 20:38:00,200 - __main__ - INFO - call_tool: name=tavily_search, result_type=papers, count=1
2026-02-22 20:38:00,201 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': 'LangGraph 记忆系统实战：反馈循环+ 动态Prompt 让AI 持续学习', 'authors': [], 'abstract': '## LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\n[*腾讯云*](/?from=20060&from_column=20060)\n[*开发者社区*](/developer)\n\n[文档](/document/product?from=20702&from_column=20702)[建议反馈](/voc/?from=20703&from_column=20703)[控制台](https://console.cloud.tencent.com?from=20063&from_column=20063)\n\n[首页](/developer)\n\n文章/答案/技术大牛\n\ndeephub\n\n[社区首页](/developer) >[专栏](/developer/column) >LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\n# LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\ndeephub\n\n发布于 2025-11-15 11:44:45\n\n发布于 2025-11-15 11:44:45\n\n8610\n\n文章被收录于专栏：[DeepHub IMBA](/developer/column/86944)DeepHub IMBA\n\n**点击上方“Deephub Imba”,关注公众号,好文章不错过 !**\n\n代理系统或者 RAG 方案，基本都需要一个双层记忆架构，这样 LLM 既能保持对当前上下文的专注，又能记住过往交互的内容。\n\n短期记忆负责单个会话内的即时信息管理，长期记忆则跨会话存储知识，让系统能够持续学习和进化。两者配合，代理才能表现出连贯性、上下文感知能力，看起来更加智能。这些记忆组件在现代 AI 架构中的位置如下图所示：\n\n#### 线程级记忆（短期）\n\n可以把它理解为代理的"工作记忆"。它帮助代理理解上下文，自然地延续讨论，不会丢失之前的步骤。LangGraph 通过检查点机制自动管理这部分记忆。对话结束后，短期记忆会被清空，下次会话则会重新开始一个新的记忆。\n\n#### 跨线程记忆（长期）\n\nLangGraph 将这些数据以 JSON 文档形式保存在记忆存储中，通过命名空间（类似文件夹）和键（类似文件名）整齐组织。因为这种记忆在对话后不会消失，所以代理能够随时间积累知识，提供更一致、更个性化的响应。\n\n> 本文会探讨生产级 AI 系统如何使用 LangGraph 管理长期记忆流。LangGraph 是一个构建可扩展、上下文感知 AI 工作流的主流框架。\n\n### LangGraph 数据持久层\n\n处理代理记忆时，LangGraph 是最常用的组件。其中 Store 功能尤为关键，它根据项目的运行位置管理记忆的保存、检索和更新方式。\n\nLangGraph 提供了几种存储实现，在简单性、持久性和可扩展性之间取得平衡。每种选项适合开发或部署的特定阶段。\n\n#### InMemory Store（用于 notebook 和快速测试）\n\n使用 `from langgraph.store.memory import InMemoryStore` 导入，创建一个完全在内存中运行的存储，使用标准 Python 字典。\n\n不写入磁盘，进程结束后所有信息都会丢失。但速度快，易用，非常适合测试工作流或尝试新的图配置。如果需要，也可以添加语义搜索能力。\n\n#### 本地开发存储（langgraph dev）\n\n这个选项的行为与上面的内存版本类似，但是可以在会话之间提供了基本持久性。\n\n用 `langgraph dev` 命令运行应用时，LangGraph 会自动使用 Python 的 pickle 格式将存储保存到本地文件系统，并在重启开发环境后恢复数据。\n\n这个方式轻量且方便，不需要外部数据库。同样支持语义搜索功能，所以它非常适合开发阶段，但不适合生产环境。\n\n#### 生产存储（LangGraph Platform 或自托管）\n\n大规模或生产部署，LangGraph 使用与 pgvector 集成的 PostgreSQL 数据库实现高效的向量存储和语义检索。\n\n这样可以提供完整的数据持久性、内置可靠性，并且能够处理更大的工作负载或多用户系统。语义搜索依靠pgvector ，默认使用余弦相似度作为相似性度量，也可以根据需求自定义。\n\n这种配置确保记忆数据安全存储，跨会话保持可用，即使在高流量或分布式工作负载下也能稳定运行。\n\n> 基础知识介绍完毕，接下来开始逐步编写完整的工作架构代码。\n\n### InMemory 功能实践\n\n本文要实现的是 InMemory 功能，这是基于 AI 系统中最常用的内存管理方式。\n\n> 它按顺序执行，在逐步构建或测试技术流程时非常实用。\n\nInMemory 功能允许在运行代码时临时存储数据，通过了解它可以有助于我们理解 LangGraph 中内存处理的工作原理。\n\n从 LangGraph 导入 `InMemoryStore` 开始。这个类让我们直接在内存中存储记忆，不需要外部数据库或文件系统。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the InMemoryStore class for storing memories in memory (no persistence) from langgraph.store.memory import InMemoryStore # Initialize an in-memory store instance for use in this notebook in_memory_store = InMemoryStore()\n```\n\n这里创建了 InMemoryStore 的实例，用于保存临时数据。因为只在内存中运行，进程停止后所有存储的数据都会被清除。\n\n> LangGraph 中的每个记忆都保存在命名空间（namespace）中。\n\n命名空间像标签或文件夹，帮助组织记忆。它被定义为元组，可以有一个或多个部分。下面这个例子使用包含用户 ID 和 "memories" 标签的元组。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a user ID for memory storage user_id = "1" # Set the namespace for storing and retrieving memories namespace_for_memory = (user_id, "memories")\n```\n\n命名空间可以代表任何东西，不一定基于用户 ID，所以可以根据应用结构自由分组记忆。\n\n下面我们保存一个记忆到存储中，使用 `put` 方法。这个方法需要三样东西：命名空间、唯一键和实际的记忆值。\n\n这里键是用 `uuid` 库生成的唯一标识符，记忆值是存储信息的字典——一个简单的偏好设置。\n\n代码语言：javascript\n\n复制\n\n```\n import uuid # Generate a unique ID for the memory memory_id = str(uuid.uuid4()) # Create a memory dictionary memory = {"food_preference": "I like pizza"} # Save the memory in the defined namespace in_memory_store.put(namespace_for_memory, memory_id, memory)\n```\n\n这会将记忆条目添加到之前定义的命名空间下的内存存储中。\n\n存储记忆后，可以用 `search` 方法取回。这个方法在命名空间内查找并返回属于它的所有记忆列表。\n\n每个记忆都是一个 `Item` 对象，包含命名空间、键、值和时间戳等详细信息。可以转换为字典以便更清晰地查看数据。\n\n代码语言：javascript\n\n复制\n\n```\n # Retrieve all stored memories for the given namespace memories = in_memory_store.search(namespace_for_memory) # View the latest memory memories[-1].dict()\n```\n\n在 notebook 中运行这段代码，得到以下输出：\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ###### { \'namespace\': [\'1\', \'memories\'], \'key\': \'c8619cd4-3d3f-4108-857c-5c8c12f39e87\', \'value\': {\'food_preference\': \'I like pizza\'}, \'created_at\': \'2025-10-08T15:46:16.531625+00:00\', \'updated_at\': \'2025-10-08T15:46:16.531625+00:00\', \'score\': None }\n```\n\n输出显示了存储的记忆详情。最重要的是 **value** 字段，包含实际保存的信息。其他字段帮助识别和管理记忆创建的时间和位置。\n\n存储就绪后，可以将其连接到图中，让记忆和检查点协同工作。这里使用两个主要组件：\n\n* **InMemorySaver** 管理线程间的检查点\n* **InMemoryStore** 存储跨线程的记忆\n\n代码语言：javascript\n\n复制\n\n```\n # To enable threads (conversations) from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() # To enable across-thread memory from langgraph.store.memory import InMemoryStore in_memory_store = InMemoryStore() # Compile the graph with the checkpointer and store # graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n```\n\n这使图能够记住线程内的对话上下文（短期），并使用相同的内存机制在线程间保留重要信息（长期）。\n\n> 这是转向生产级存储之前测试记忆行为的简单有效方式。\n\n### 构建代理架构\n\n在使用记忆系统工作流之前，需要构建使用它的智能代理。因为本文专注于记忆管理，所以只会构建一个中等复杂的电子邮件助手，模拟在真实场景中探索记忆的工作方式。\n\n下面我们从零开始构建这个系统，定义数据结构、"大脑"（提示词）和能力（工具）。最终得到一个不仅能回复邮件，还能从反馈中学习的代理。\n\n#### 定义模式\n\n处理数据前需要定义其形状。模式是代理信息流的蓝图，确保一切结构化、可预测且类型安全。\n\n首先编写 `RouterSchema`。需要它是为了让初始分类步骤可靠。不能冒险让 LLM 在期望明确决定时返回非结构化文本。\n\n这个 Pydantic 模型会强制 LLM 返回一个干净的 JSON 对象，包含推理过程和一个严格为 \'ignore\'、\'respond\' 或 \'notify\' 之一的分类结果。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the necessary libraries from Pydantic and Python\'s typing module from pydantic import BaseModel, Field from typing_extensions import TypedDict, Literal # Define a Pydantic model for our router\'s structured output. class RouterSchema(BaseModel): """Analyze the unread email and route it according to its content.""" # Add a field for the LLM to explain its step-by-step reasoning. reasoning: str = Field(description="分类背后的逐步推理。") # Add a field to hold the final classification. # The `Literal` type restricts the output to one of these three specific strings. classification: Literal["ignore", "respond", "notify"] = Field( description="电子邮件的分类。" )\n```\n\n这是在为分类 LLM 创建契约。后面与 LangChain 的 `.with_structured_output()` 方法配对时，能保证输出是一个可预测的 Python 对象，让图中的逻辑更加健壮。\n\n接下来需要一个地方存储代理单次运行的所有信息，这就是 `State` 的作用。它像一个中央白板，图的每个部分都可以读写。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the base state class from LangGraph from langgraph.graph import MessagesState # Define the central state object for our graph. class State(MessagesState): # This field will hold the initial raw email data. email_input: dict # This field will store the decision made by our triage router. classification_decision: Literal["ignore", "respond", "notify"]\n```\n\n继承自 LangGraph 的 `MessagesState`，自动获得一个 `messages` 列表来跟踪对话历史。然后添加自定义字段。随着流程从一个节点移动到另一个节点，这个 `State` 对象会被传递并累积信息。\n\n最后定义一个小但重要的 `StateInput` 模式，规定图的初始输入应该是什么样子。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a TypedDict for the initial input to our entire workflow. class StateInput(TypedDict): # The workflow must be started with a dictionary containing an \'email_input\' key. email_input: dict\n```\n\n这个简单的模式从应用入口点就提供了清晰性和类型安全性，确保对图的任何调用都以正确的数据结构开始。\n\n#### 创建提示词\n\n使用提示词方法来指导和引导 LLM 行为。对于代理，会定义几个提示词，每个都有特定的任务。\n\n代理从我们这里学到任何东西之前，需要一套基准指令。这些默认字符串会在第一次运行时加载到记忆存储中，为代理行为提供起点。\n\n首先定义 `default_background` 给代理一个角色。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a default persona for the agent. default_background=""" I\'m Lance, a software engineer at LangChain. """\n```\n\n接下来是 `default_triage_instructions`，分类路由器遵循的初始规则。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the initial rules for the triage LLM. default_triage_instructions = """ Emails that are not worth responding to: - Marketing newsletters and promotional emails - Spam or suspicious emails - CC\'d on FYI threads with no direct questions Emails that require notification but no response: - Team member out sick or on vacation - Build system notifications or deployments Emails that require a response: - Direct questions from team members - Meeting requests requiring confirmation """\n```\n\n然后是 `default_response_preferences`，定义代理的初始写作风格。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the default preferences for how the agent should compose emails. default_response_preferences = """ Use professional and concise language. If the e-mail mentions a deadline, make sure to explicitly acknowledge and reference the deadline in your response. When responding to meeting scheduling requests: - If times are proposed, verify calendar availability and commit to one. - If no times are proposed, check your calendar and propose multiple options. """\n```\n\n最后是 `default_cal_preferences`，指导日程安排行为。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the default preferences for scheduling meetings. default_cal_preferences = """ 30 minute meetings are preferred, but 15 minute meetings are also acceptable. """\n```\n\n现在创建使用这些默认值的提示词，首先是 `triage_system_prompt`。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the system prompt for the initial triage step. triage_system_prompt = """ < Role > Your role is to triage incoming emails based on background and instructions.  Role > < Background > {background}  Background > < Instructions > Categorize each email into IGNORE, NOTIFY, or RESPOND.  Instructions > < Rules > {triage_instructions}  Rules > """\n```\n\n这个提示词模板给分类路由器提供角色和指令。`{background}` 和 `{triage_instructions}` 占位符会被刚才定义的默认字符串填充。\n\n接着是 `triage_user_prompt`，一个简单的模板，用于将原始邮件内容构造成 LLM 易于解析的清晰格式。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the user prompt for triage, which will format the raw email. triage_user_prompt = """ Please determine how to handle the following email: From: {author} To: {to} Subject: {subject} {email_thread}"""\n```\n\n现在是主要组件，必须创建 `agent_system_prompt_hitl_memory`，它包含到目前为止编码的角色和其他指令。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the datetime library to include the current date in the prompt. from datetime import datetime # Define the main system prompt for the response agent. agent_system_prompt_hitl_memory = """ < Role > You are a top-notch executive assistant.  Role > < Tools > You have access to the following tools: {tools_prompt}  Tools > < Instructions > 1. Analyze the email content carefully. 2. Always call one tool at a time until the task is complete. 3. Use Question to ask the user for clarification. 4. Draft emails using write_email. 5. For meetings, check availability and schedule accordingly. - Today\'s date is """ + datetime.now().strftime("%Y-%m-%d") + """ 6. After sending emails, use the Done tool.  Instructions > < Background > {background}  Background > < Response Preferences > {response_preferences}  Response Preferences > < Calendar Preferences > {cal_preferences}  Calendar Preferences > """\n```\n\n这是主响应代理的主指令集。像 `{response_preferences}` 和 `{cal_preferences}` 这样的占位符是记忆系统的关键。\n\n> 它们允许动态地从记忆存储中注入代理学到的知识，使其能够随时间调整行为。\n\n为了让代理改进，为专门的"记忆管理器"LLM 定义特殊提示词。它唯一的工作就是安全、智能地更新记忆存储。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the system prompt for our specialized memory update manager LLM. MEMORY_UPDATE_INSTRUCTIONS = """ # Role You are a memory profile manager for an email assistant. # Rules - NEVER overwrite the entire profile - ONLY add new information - ONLY update facts contradicted by feedback - PRESERVE all other information # Reasoning Steps 1. Analyze the current memory profile. 2. Review feedback messages. 3. Extract relevant preferences. 4. Compare to existing profile. 5. Identify facts to update. 6. Preserve everything else. 7. Output updated profile. # Process current profile for {namespace}  {current_profile}  """\n```\n\n`MEMORY_UPDATE_INSTRUCTIONS` 提示词高度结构化，规则严格：永不覆盖、只做针对性添加、保留现有信息。这种方法对防止代理记忆被破坏至关重要。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a reinforcement prompt to remind the LLM of the most critical rules. MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT = """ Remember: - NEVER overwrite the entire profile - ONLY make targeted additions - ONLY update specific facts contradicted by feedback - PRESERVE all other information """\n```\n\n`MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT` 是现代提示词工程技术。它是要求 LLM 更新记忆时会附加到消息中的最关键规则的简明摘要。重复关键指令有助于确保 LLM 遵守它们。\n\n#### 定义工具和实用函数\n\n代理有了指令，现在需要赋予它采取行动的能力。会定义作为工具的 Python 函数，以及一些辅助实用函数，保持主代码整洁有序。\n\n编写实际工具函数之前，需要一个关于它们的简单文本描述。这是代理在主提示词中会"看到"的内容，让它理解有哪些工具可用以及如何使用。\n\n代码语言：javascript\n\n复制\n\n```\n # A simple string describing the available tools for the LLM. HITL_MEMORY_TOOLS_PROMPT = """ 1. write_email(to, subject, content) - Send emails to specified recipients 2. schedule_meeting(attendees, subject, duration_minutes, preferred_day, start_time) - Schedule calendar meetings 3. check_calendar_availability(day) - Check available time slots 4. Question(content) - Ask follow-up questions 5. Done - Mark the email as sent """\n```\n\n这个字符串本身不是可执行代码，而是作为 LLM 的文档。它会被插入到主 `agent_system_prompt_hitl_memory` 中的 `{tools_prompt}` 占位符。这样代理就知道 `write_email` 函数存在，需要 `to`、`subject` 和 `content` 参数。\n\n好的项目都有一个 `utils.py` 文件来存放执行常见、重复性任务的辅助函数，保持主图逻辑整洁，专注于工作流本身。\n\n首先需要一个函数来解析初始邮件输入。\n\n代码语言：javascript\n\n复制\n\n```\n # This utility unpacks the email input dictionary for easier access. def parse_email(email_input: dict) -> tuple[str, str, str, str]: """Parse an email input dictionary into its constituent parts.""" # Return a tuple containing the author, recipient, subject, and body of the email. return ( email_input["author"], email_input["to"], email_input["subject"], email_input["email_thread"], )\n```\n\n`parse_email` 函数是输入字典的简单解包器。虽然可以在图节点中直接访问 `email_input["author"]`，但这个辅助函数让代码更可读，并集中了解析逻辑。\n\n接下来，一个将邮件内容格式化为 Markdown 供 LLM 使用的函数。\n\n代码语言：javascript\n\n复制\n\n```\n # This function formats the raw email data into clean markdown for the LLM. def format_email_markdown(subject, author, to, email_thread): """Format email details into a nicely formatted markdown string.""" # Use f-string formatting to create a structured string with clear labels. return f""" **Subject**: {subject} **From**: {author} **To**: {to} {email_thread} --- """\n```\n\n`format_email_markdown` 函数接收解析后的邮件部分，将它们排列成干净的 Markdown 格式块。这种结构化格式比原始的非结构化字符串更容易被 LLM 解析，帮助它更好地理解邮件的不同组成部分（发件人、主题、正文）。\n\n最后需要一个函数为人工审阅者格式化代理建议的操作。\n\n代码语言：javascript\n\n复制\n\n```\n # This function creates a human-friendly view of a tool call for the HITL interface. def format_for_display(tool_call: dict) -> str: """Format a tool call into a readable string for the user.""" # Initialize an empty string to build our display. display = "" # Use conditional logic to create custom, readable formats for our main tools. if tool_call["name"] == "write_email": display += f\'# Email Draft\\n\\n**To**: {tool_call["args"].get("to")}\\n**Subject**: {tool_call["args"].get("subject")}\\n\\n{tool_call["args"].get("content")}\' elif tool_call["name"] == "schedule_meeting": display += f\'# Calendar Invite\\n\\n**Meeting**: {tool_call["args"].get("subject")}\\n**Attendees**: {", ".join(tool_call["args"].get("attendees"))}\' elif tool_call["name"] == "Question": display += f\'# Question for User\\n\\n{tool_call["args"].get("content")}\' # Provide a generic fallback for any other tools. else: display += f\'# Tool Call: {tool_call["name"]}\\n\\nArguments:\\n{tool_call["args"]}\' # Return the final formatted string. return display\n```\n\n这个 `format_for_display` 函数对人机协作（HITL）步骤很重要。当代理提出工具调用如 `write_email` 时，不想向人工审阅者展示原始 JSON 对象。\n\n这个函数将技术表示转换为看起来像实际邮件草稿或日历邀请的内容，让用户更容易审阅、编辑或批准。\n\n模式、提示词和实用函数都定义好了，现在可以将它们组装成完整的图，让学习代理运转起来。\n\n### 记忆功能与图节点\n\n从这里开始我们就要实现记忆逻辑了，先看看整个系统如何运作。\n\n代理从静态指令集转变为能够学习的动态系统，关键就在这里。\n\n构建使用记忆的图节点之前，需要实际与 `InMemoryStore` 交互的函数。我们需要创建两个关键函数：一个获取现有偏好，另一个根据反馈更新它们。\n\n首先需要可靠的方式从存储中获取偏好。编写一个叫 `get_memory` 的函数，它会在存储中查找特定偏好（如 `"triage_preferences"`）。找到就返回存储的值。\n\n如果找不到——这会在用户第一次运行时发生——它会使用之前定义的默认内容创建条目。这确保代理始终有一套规则可以遵循。\n\n代码语言：javascript\n\n复制\n\n```\n # A function to retrieve memory from the store or initialize with defaults. def get_memory(store, namespace, default_content=None): """Get memory from the store or initialize with default if it doesn\'t exist.""" # Use the store\'s .get() method to search for an item with a specific key. user_preferences = store.get(namespace, "user_preferences") # If the item exists, return its value (the stored string). if user_preferences: return user_preferences.value # If the item does not exist, this is the first time we\'re accessing this memory. else: # Use the store\'s .put() method to create the memory item with default content. store.put(namespace, "user_preferences", default_content) # Return the default content to be used in this run. return default_content\n```\n\n这个简单的函数功能强大。它抽象了检查和初始化记忆的逻辑。图中的任何节点现在都可以调用 `get_memory` 获取最新的用户偏好，无需知道这是第一次运行还是第一百次。\n\n这里是代理学习被触发的地方。`update_memory` 函数设计用来接收用户反馈——比如编辑过的邮件或自然语言指令——并用它来精炼代理存储的知识。它使用之前精心制作的 `MEMORY_UPDATE_INSTRUCTIONS` 提示词来协调一个特殊用途的 LLM 调用。\n\n为确保 LLM 输出可预测，先定义一个 `UserPreferences` Pydantic 模式。这会强制记忆管理器 LLM 返回一个 JSON 对象，包含推理过程和最终更新的偏好字符串。\n\n代码语言：javascript\n\n复制\n\n```\n # A Pydantic model to structure the output of our memory update LLM call. class UserPreferences(BaseModel): """Updated user preferences based on user\'s feedback.""" # A field for the LLM to explain its reasoning, useful for debugging. chain_of_thought: str = Field(description="Reasoning about which user preferences need to add / update if required") # The final, updated string of user preferences. user_preferences: str = Field(description="Updated user preferences")\n```\n\n现在可以编写 `update_memory` 函数本身。它会检索当前偏好，将它们与用户反馈和特殊提示词结合，然后将 LLM 的精炼输出保存回存储。\n\n代码语言：javascript\n\n复制\n\n```\n # Import AIMessage to help filter messages before sending them to the memory updater. from langchain_core.messages import AIMessage # This function intelligently updates the memory store based on user feedback. def update_memory(store, namespace, messages): """Update memory profile in the store.""" # First, get the current memory from the store so we can provide it as context. user_preferences = store.get(namespace, "user_preferences") # Initialize a new LLM instance specifically for this task, configured for structured output. memory_updater_llm = llm.with_structured_output(UserPreferences) # This is a small but important fix: filter out any previous AI messages with tool calls. # Passing these complex objects can sometimes cause errors in the downstream LLM call. messages_to_send = [ msg for msg in messages if not (isinstance(msg, AIMessage) and hasattr(msg, \'tool_calls\') and msg.tool_calls) ] # Invoke the LLM with the memory prompt, current preferences, and the user\'s feedback. result = memory_updater_llm.invoke( [ # The system prompt that instructs the LLM on how to update memory. {"role": "system", "content": MEMORY_UPDATE_INSTRUCTIONS.format(current_profile=user_preferences.value, namespace=namespace)}, ] # Append the filtered conversation messages containing the feedback. + messages_to_send ) # Save the newly generated preference string back into the store, overwriting the old one. store.put(namespace, "user_preferences", result.user_preferences)\n```\n\n这个函数是代理学习能力的主要组成部分。通过使用带有严格指令的专用 LLM 调用，确保记忆以受控和增量方式更新，让代理随时间逐渐与用户偏好对齐。\n\n现在可以定义代理的核心逻辑。在 LangGraph 中，这个逻辑封装在节点中。每个节点都是一个 Python 函数，接收图的当前 `State`，执行操作，返回对该状态的更新。\n\n> 电子邮件助手会有几个关键节点，处理从初始分类到生成最终响应的所有事情。\n\n工作流中的第一个节点是 `triage_router`。这个函数的任务是对收到的邮件做初步决定……\n\n> 应该回复、只通知用户，还是完全忽略？长期记忆首次发挥作用就在这里。\n\n路由器会使用 `get_memory` 函数获取用户最新的 `triage_preferences` 并注入到提示词中，确保决策能力随时间提高。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the Command class for routing and BaseStore for type hinting from langgraph.types import Command from langgraph.store.base import BaseStore # Define the first node in our graph, the triage router. def triage_router(state: State, store: BaseStore) -> Command: """Analyze email content to decide the next step.""" # Unpack the raw email data using our utility function. author, to, subject, email_thread = parse_email(state["email_input"]) # Format the email content into a clean string for the LLM. email_markdown = format_email_markdown(subject, author, to, email_thread) # Here is the memory integration: fetch the latest triage instructions. # If they don\'t exist, it will use the `default_triage_instructions`. triage_instructions = get_memory(store, ("email_assistant", "triage_preferences"), default_triage_instructions) # Format the system prompt, injecting the retrieved triage instructions. system_prompt = triage_system_prompt.format( background=default_background, triage_instructions=triage_instructions, ) # Format the user prompt with the specific details of the current email. user_prompt = triage_user_prompt.format( author=author, to=to, subject=subject, email_thread=email_thread ) # Invoke the LLM router, which is configured to return our `RouterSchema`. result = llm_router.invoke( [ {"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}, ] ) # Based on the LLM\'s classification, decide which node to go to next. if result.classification == "respond": print("📧 Classification: RESPOND - This email requires a response") # Set the next node to be the \'response_agent\'. goto = "response_agent" # Update the state with the decision and the formatted email for the agent. update = { "classification_decision": result.classification, "messages": [{"role": "user", "content": f"Respond to the email: {email_markdown}"}], } elif result.classification == "ignore": print("🚫 Classification: IGNORE - This email can be safely ignored") # End the workflow immediately. goto = END # Update the state with the classification decision. update = {"classification_decision": result.classification} elif result.classification == "notify": print("🔔 Classification: NOTIFY - This email contains important information") # Go to the human-in-the-loop handler for notification. goto = "triage_interrupt_handler" # Update the state with the classification decision. update = {"classification_decision": result.classification} else: # Raise an error if the classification is invalid. raise ValueError(f"Invalid classification: {result.classification}") # Return a Command object to tell LangGraph where to go next and what to update. return Command(goto=goto, update=update)\n```\n\n这个节点是整个系统的入口。通过添加一行 `triage_instructions = get_memory(...)`，就把它从静态路由器变成了能学习的路由器。当用户对分类决策提供反馈时，存储中的 `triage_preferences` 会被更新，这个节点就会自动开始在未来的邮件上做出更好、更个性化的分类。\n\n邮件被分类为 "respond" 时，会被传递给主响应代理。该代理的核心是 `llm_call` 节点。这个函数的目的是获取当前对话历史并采取下一步行动，通常是决定调用哪个工具。\n\n跟分类路由器一样，这个节点集成了记忆来指导决策。它获取 `response_preferences` 和 `cal_preferences`，确保行为与用户学到的风格保持一致。\n\n代码语言：javascript\n\n复制\n\n```\n # This is the primary reasoning node for the response agent. def llm_call(state: State, store: BaseStore): """LLM decides whether to call a tool or not, using stored preferences.""" # Fetch the user\'s latest calendar preferences from the memory store. cal_preferences = get_memory(store, ("email_assistant", "cal_preferences"), default_cal_preferences) # Fetch the user\'s latest response (writing style) preferences. response_preferences = get_memory(store, ("email_assistant", "response_preferences"), default_response_preferences) # Filter out previous AI messages with tool calls to prevent API errors. messages_to_send = [ msg for msg in state["messages"] if not (isinstance(msg, AIMessage) and hasattr(msg, \'tool_calls\') and msg.tool_calls) ] # Invoke the main LLM, which is bound to our set of tools. # The prompt is formatted with the preferences retrieved from memory. response = llm_with_tools.invoke( [ {"role": "system", "content": agent_system_prompt_hitl_memory.format( tools_prompt=HITL_MEMORY_TOOLS_PROMPT, background=default_background, response_preferences=response_preferences, cal_preferences=cal_preferences )} ] + messages_to_send ) # Return the LLM\'s response to be added to the state. return {"messages": [response]}\n```\n\n这个节点展示了长期记忆的重要性。每次执行时都会提取用户最新的写作风格和日历偏好。\n\n当用户提供反馈说更喜欢较短的邮件或 30 分钟的会议时，`update_memory` 函数会修改存储。下次这个 `llm_call` 节点运行时，会自动获取那些新偏好并注入到提示词中，立即改变代理行为，无需任何代码更改。\n\n这就创建了一个反馈循环，代理在其中不断适应用户。\n\n### 通过人机协作捕获反馈\n\n代理不仅仅是在行动，还得知道何时请求帮助或确认。接下来要构建的节点集是中断处理器。\n\n这些特殊节点会暂停图的执行并等待人类输入。神奇的地方就发生在这里：在这些步骤中提供的反馈会被捕获并用于更新代理的长期记忆。\n\n会有两个中断点：\n\n一个在初始分类之后（用于 `notify` 分类），另一个更复杂，用于审查代理建议的工具调用。\n\n首先构建 `triage_interrupt_handler`。当 `triage_router` 将邮件分类为 `notify` 时，这个节点被触发。代理不会对邮件采取行动，而是呈现给用户并请求决策：\n\n> 应该忽略，还是实际回复？用户在这里的选择是关于分类偏好的宝贵反馈。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the `interrupt` function from LangGraph. from langgraph.types import interrupt # Define the interrupt handler for the triage step. def triage_interrupt_handler(state: State, store: BaseStore) -> Command: """Handles interrupts from the triage step, pausing for user input.""" # Parse the email input to format it for display. author, to, subject, email_thread = parse_email(state["email_input"]) email_markdown = format_email_markdown(subject, author, to, email_thread) # This is the data structure that defines the interrupt. # It specifies the action, the allowed user responses, and the content to display. request = { "action_request": { "action": f"Email Assistant: {state[\'classification_decision\']}", "args": {} }, "config": { "allow_ignore": True, "allow_respond": True }, "description": email_markdown, } # The `interrupt()` function pauses the graph and sends the request to the user. # It waits here until it receives a response. response = interrupt([request])[0] # Now, we process the user\'s response. if response["type"] == "response": # The user decided to respond, overriding the \'notify\' classification. user_input = response["args"] # We create a message to pass to the memory updater. messages = [{"role": "user", "content": f"The user decided to respond to the email, so update the triage preferences to capture this."}] # This is a key step: we call `update_memory` to teach the agent. update_memory(store, ("email_assistant", "triage_preferences"), messages) # Prepare to route to the main response agent. goto = "response_agent" # Update the state with the user\'s feedback. update = {"messages": [{"role": "user", "content": f"User wants to reply. Use this feedback: {user_input}"}]} elif response["type"] == "ignore": # The user confirmed the email should be ignored. messages = [{"role": "user", "content": f"The user decided to ignore the email even though it was classified as notify. Update triage preferences to capture this."}] # We still update memory to reinforce this preference. update_memory(store, ("email_assistant", "triage_preferences"), messages) # End the workflow. goto = END update = {} # No message update needed. else: raise ValueError(f"Invalid response: {response}") # Return a Command to direct the graph\'s next step. return Command(goto=goto, update=update)\n```\n\n如果代理认为一封邮件只是通知，但用户决定回复，`update_memory` 就会被调用。记忆管理器 LLM 会看到消息"用户决定回复..."并分析邮件内容。\n\n然后它会精确地更新 `triage_preferences` 字符串，也许通过将"构建系统通知"从 `NOTIFY` 类别移动到 `RESPOND` 类别。下次类似邮件到达时，`triage_router` 会做出更好、更个性化的决定。\n\n但还需要一个主中断处理器，这是图中最复杂的节点。`llm_call` 节点提出要使用的工具（如 `write_email` 或 `schedule_meeting`）后，这个 `interrupt_handler` 就会介入。它将代理建议的操作呈现给用户审查。\n\n然后用户可以接受、忽略、提供自然语言反馈或直接编辑。每种选择都为记忆系统提供了不同且有价值的信号。\n\n代码语言：javascript\n\n复制\n\n```\n # The main interrupt handler for reviewing tool calls. def interrupt_handler(state: State, store: BaseStore) -> Command: """Creates an interrupt for human review of tool calls and updates memory.""" # We\'ll build up a list of new messages to add to the state. result = [] # By default, we\'ll loop back to the LLM after this. goto = "llm_call" # The agent can propose multiple tool calls, so we loop through them. for tool_call in state["messages"][-1].tool_calls: # We only want to interrupt for certain "high-stakes" tools. hitl_tools = ["write_email", "schedule_meeting", "Question"] if tool_call["name"] not in hitl_tools: # For other tools (like check_calendar), execute them without interruption. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(tool_call["args"]) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) continue # Format the proposed action for display to the human reviewer. tool_display = format_for_display(tool_call) # Define the interrupt request payload. request = { "action_request": {"action": tool_call["name"], "args": tool_call["args"]}, "config": { "allow_ignore": True, "allow_respond": True, "allow_edit": True, "allow_accept": True }, "description": tool_display, } # Pause the graph and wait for the user\'s response. response = interrupt([request])[0] # --- MEMORY UPDATE LOGIC BASED ON USER RESPONSE --- if response["type"] == "edit": # The user directly edited the agent\'s proposed action. initial_tool_call = tool_call["args"] edited_args = response["args"]["args"] # This is the most direct form of feedback. We call `update_memory`. if tool_call["name"] == "write_email": update_memory(store, ("email_assistant", "response_preferences"), [{"role": "user", "content": f"User edited the email. Initial draft: {initial_tool_call}. Edited draft: {edited_args}."}]) elif tool_call["name"] == "schedule_meeting": update_memory(store, ("email_assistant", "cal_preferences"), [{"role": "user", "content": f"User edited the meeting. Initial invite: {initial_tool_call}. Edited invite: {edited_args}."}]) # Execute the tool with the user\'s edited arguments. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(edited_args) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) elif response["type"] == "response": # The user gave natural language feedback. user_feedback = response["args"] # We capture this feedback and use it to update memory. if tool_call["name"] == "write_email": update_memory(store, ("email_assistant", "response_preferences"), [{"role": "user", "content": f"User gave feedback on the email draft: {user_feedback}"}]) elif tool_call["name"] == "schedule_meeting": update_memory(store, ("email_assistant", "cal_preferences"), [{"role": "user", "content": f"User gave feedback on the meeting invite: {user_feedback}"}]) # We don\'t execute the tool. Instead, we pass the feedback back to the agent. result.append({"role": "tool", "content": f"User gave feedback: {user_feedback}", "tool_call_id": tool_call["id"]}) elif response["type"] == "ignore": # The user decided this action should not be taken. This is triage feedback. update_memory(store, ("email_assistant", "triage_preferences"), [{"role": "user", "content": f"User ignored the proposal to {tool_call[\'name\']}. This email should not have been classified as \'respond\'."}]) result.append({"role": "tool", "content": "User ignored this. End the workflow.", "tool_call_id": tool_call["id"]}) goto = END elif response["type"] == "accept": # The user approved the action. No memory update is needed. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(tool_call["args"]) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) # Return a command with the next node and the messages to add to the state. return Command(goto=goto, update={"messages": result})\n```\n\n这个节点是学习系统的核心。注意到没有，每种类型的用户反馈——编辑、响应和忽略——都会触发对 `update_memory` 的调用，并带有特定的、上下文相关的消息。\n\n当用户将会议时长从 45 分钟编辑为 30 分钟时，记忆管理器 LLM 会看到这个明确信号，并更新 `cal_preferences` 以在未来偏好 30 分钟的会议。当用户说"让它不那么正式"时，LLM 会对此进行概括，并在 `response_preferences` 中添加新规则。这种持续、细粒度的反馈循环让代理能够随时间成为高度个性化的助手。\n\n### 组装成工作流\n\n代理的所有单独组件都构建好了：模式、提示词、工具、实用函数和图节点。现在用 LangGraph 将它们组装成功能性的状态机。这涉及定义图结构、添加节点以及指定连接它们的边。\n\n主 `llm_call` 节点运行后，代理会提出一个或多个工具调用。需要一种方法来决定接下来会发生什么。代理应该停止，还是应该进入人工审查步骤？这由条件边来处理。它是一个简单的函数，检查状态中的最后一条消息并指导图的流程。\n\n代码语言：javascript\n\n复制\n\n```\n # This function determines the next step after the LLM has made its decision. def should_continue(state: State) -> Literal["interrupt_handler", END]: """Route to the interrupt handler or end the workflow if the \'Done\' tool is called.""" # Get the list of messages from the current state. messages = state["messages"] # Get the most recent message, which contains the agent\'s proposed action. last_message = messages[-1] # Check if the last message contains any tool calls. if last_message.tool_calls: # Loop through each proposed tool call. for tool_call in last_message.tool_calls: # If the agent has decided it\'s finished, we end the workflow. if tool_call["name"] == "Done": return END # For any other tool, we proceed to the human review step. else: return "interrupt_handler"\n```\n\n这个函数是响应代理的主路由器。它检查代理的决定并充当交通警察。如果调用了 `Done` 工具，通过返回 `END` 表示过程已完成。\n\n对于任何其他工具调用，它将图路由到 `interrupt_handler` 节点进行人工审查，确保在未经批准的情况下不采取任何行动。\n\n现在可以组装图，直观地看到它的样子。用 `StateGraph` 定义结构。该过程涉及两个主要阶段：\n\n构建 `response_agent` 子图——包含 `llm_call` -> `interrupt_handler` 的核心循环。构建 `overall_workflow`——这个主图从 `triage_router` 开始，并将 `response_agent` 子图作为其节点之一。\n\n这种方式保持架构清晰易懂。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the main graph-building class from LangGraph. from langgraph.graph import StateGraph, START, END # --- Part 1: Build the Response Agent Subgraph --- # Initialize a new state graph with our defined `State` schema. agent_builder = StateGraph(State) # Add the \'llm_call\' node to the graph. agent_builder.add_node("llm_call", llm_call) # Add the \'interrupt_handler\' node to the graph. agent_builder.add_node("interrupt_handler", interrupt_handler) # Set the entry point of this subgraph to be the \'llm_call\' node. agent_builder.add_edge(START, "llm_call") # Add the conditional edge that routes from \'llm_call\' to either \'interrupt_handler\' or END. agent_builder.add_conditional_edges( "llm_call", should_continue, { "interrupt_handler": "interrupt_handler", END: END, }, ) # After the interrupt handler, the graph always loops back to the LLM to continue the task. agent_builder.add_edge("interrupt_handler", "llm_call") # Compile the subgraph into a runnable object. response_agent = agent_builder.compile() # --- Part 2: Build the Overall Workflow --- # Initialize the main graph, defining its input schema as `StateInput`. overall_workflow = ( StateGraph(State, input=StateInput) # Add the triage router as the first node. .add_node("triage_router", triage_router) # Add the triage interrupt handler node. .add_node("triage_interrupt_handler", triage_interrupt_handler) # Add our entire compiled `response_agent` subgraph as a single node. .add_node("response_agent", response_agent) # Set the entry point for the entire workflow. .add_edge(START, "triage_router") # Define the edges from the triage router to the appropriate next steps. .add_edge("triage_router", "response_agent") .add_edge("triage_router", "triage_interrupt_handler") .add_edge("triage_interrupt_handler", "response_agent") ) # Compile the final, complete graph. email_assistant = overall_workflow.compile()\n```\n\n`triage_router` 做初步决定，然后分支到结束流程、通过 `triage_interrupt_handler` 询问用户输入，或者将控制权交给 `response_agent`。\n\n`response_agent` 进入自己的思考（`llm_call`）和请求审查（`interrupt_handler`）循环，在此过程中更新记忆，直到任务完成。\n\n这种有状态架构使得 LangGraph 非常适合构建复杂的、能够学习的代理。现在可以拿着这个编译好的 `email_assistant` 开始测试它从反馈中学习的能力。\n\n### 使用记忆测试代理\n\n记忆功能已经实现到电子邮件助手中，接下来测试系统如何从用户反馈中学习并随时间适应。这部分测试会探讨不同类型的用户交互如何创建不同的记忆更新，从而提高助手未来的性能。\n\n主要解决一下的问题：\n\n* 系统如何捕获和持久化用户偏好？\n* 存储的偏好以何种方式影响后续决策过程？\n* 哪些用户交互模式会触发特定类型的记忆更新？\n\n首先构建一个辅助函数来显示记忆内容，以便在整个测试过程中跟踪其演变。\n\n代码语言：javascript\n\n复制\n\n```\n # Import necessary libraries for testing. import uuid from langgraph.checkpoint.memory import MemorySaver from langgraph.types import Command from langgraph.store.memory import InMemoryStore # Define a helper function to display the content of our memory store. def display_memory_content(store, namespace=None): """A utility to print the current state of the memory store.""" # Print a header for clarity. print("\\n======= CURRENT MEMORY CONTENT =======") # If a specific namespace is requested, show only that one. if namespace: # Retrieve the memory item for the specified namespace. memory = store.get(namespace, "user_preferences") print(f"\\n--- {namespace[1]} ---") if memory: print(memory.value) else: print("No memory found") # If no specific namespace is given, show all of them. else: # Define the list of all possible namespaces we are using. for ns in [ ("email_assistant", "triage_preferences"), ("email_assistant", "response_preferences"), ("email_assistant", "cal_preferences"), ("email_assistant", "background") ]: # Retrieve and print the memory content for each namespace. memory = store.get(ns, "user_preferences") print(f"\\n--- {ns[1]} ---") if memory: print(memory.value) else: print("No memory found") print("=======================================\\n")\n```\n\n这个实用程序提供了一个实时窗口，可以观察代理不断演变的知识库，能轻松看到每次交互后到底学到了什么。\n\n开始执行不同的测试用例。\n\n#### 测试用例 1：基线——接受提议\n\n第一个测试检查当用户未经修改地接受代理操作时会发生什么。这个基线案例帮助理解在没有提供反馈时系统的行为。期望代理使用记忆来做决定，但不会更新它。\n\n先设置一个新的测试运行。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the input email for our test case. email_input_respond = { "to": "Lance Martin ", "author": "Project Manager ", "subject": "Tax season let\'s schedule call", "email_thread": "Lance,\\n\\nIt\'s tax season again... Are you available sometime next week? ... for about 45 minutes." } # --- Setup for a new test run --- # Initialize a new checkpointer and a fresh, empty memory store. checkpointer = MemorySaver() store = InMemoryStore() # Compile our graph, connecting it to our new checkpointer and store. graph = overall_workflow.compile(checkpointer=checkpointer, store=store) # Create a unique ID and configuration for this conversation. thread_id_1 = uuid.uuid4() thread_config_1 = {"configurable": {"thread_id": thread_id_1}} # Run the graph until its first interrupt. print("Running the graph until the first interrupt...") for chunk in graph.stream({"email_input": email_input_respond}, config=thread_config_1): if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the memory state after the first interrupt. display_memory_content(store)\n```\n\n图会一直运行直到代理提出第一个动作并暂停供审查。\n\n代码语言：javascript\n\n复制\n\n```\n ####### OUTPUT ######### Running the graph until the first interrupt... 📧 Classification: RESPOND - This email requires a response INTERRUPT OBJECT: Action Request: {\'action\': \'schedule_meeting\', \'args\': {\'attendees\': [\'lance@company.com\', \'pm@client.com\'], \'subject\': \'Tax Planning Strategies\', \'duration_minutes\': 45, ...}} ======= CURRENT MEMORY CONTENT ======= --- triage_preferences --- Emails that are not worth responding to: ... --- response_preferences --- Use professional and concise language. ... --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. --- background --- No memory found =======================================\n```\n\n输出显示了两件关键的事。首先，代理正确提出了一个 45 分钟的 `schedule_meeting` 工具调用，尊重了发件人的请求，即使默认偏好是 30 分钟。\n\n其次，`display_memory_content` 函数确认所有记忆命名空间都已使用默认值初始化。尚未发生学习。\n\n现在接受代理的提议。\n\n代码语言：javascript\n\n复制\n\n```\n # Resume the graph by sending an \'accept\' command. print(f"\\nSimulating user accepting the {Interrupt_Object.value[0][\'action_request\'][\'action\']} tool call...") for chunk in graph.stream(Command(resume=[{"type": "accept"}]), config=thread_config_1): # Let the graph run until its next natural pause point. if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}")\n```\n\n代理执行会议工具并进入下一个逻辑步骤：起草一封确认邮件。然后再次中断供审查。\n\n代码语言：javascript\n\n复制\n\n```\n Simulating user accepting the schedule_meeting tool call... INTERRUPT OBJECT: Action Request: {\'action\': \'write_email\', \'args\': {\'to\': \'pm@client.com\', \'subject\': "Re: Tax season let\'s schedule call", \'content\': \'Dear Project Manager, I have scheduled a meeting...for 45 minutes...\'}}\n```\n\n代理已经起草了适当的确认邮件，等待最终批准。现在接受这第二个提议并检查记忆的最终状态。\n\n代码语言：javascript\n\n复制\n\n```\n # Resume the graph one last time with another \'accept\' command. print(f"\\nSimulating user accepting the {Interrupt_Object.value[0][\'action_request\'][\'action\']} tool call...") for chunk in graph.stream(Command(resume=[{"type": "accept"}]), config=thread_config_1): pass # Let the graph finish. # Check the final state of all memory namespaces. display_memory_content(store)\n```\n\n工作流完成。用户批准了代理的所有操作。\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ####### Simulating user accepting the write_email tool call... ======= CURRENT MEMORY CONTENT ======= --- triage_preferences --- Emails that are not worth responding to: ... --- response_preferences --- Use professional and concise language. ... --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. --- background --- No memory found =======================================\n```\n\n最终的记忆检查证实了假设。即使在一次完整、成功的运行之后，记忆内容也与其初始默认状态相同。这是正确的行为。简单的接受不能提供强烈的学习信号，所以代理明智地没有改变其长期知识。它使用记忆，但没有明确反馈就不会改变它。\n\n#### 测试用例 2：从直接编辑中学习\n\n下面我们测试一个有趣的例子，看看当通过直接编辑代理提议来提供明确反馈时会发生什么。这创建了一个清晰的"之前"和"之后"场景，记忆管理器 LLM 可以从中学习。\n\n用相同的邮件开始一次新的运行。\n\n代码语言：javascript\n\n复制\n\n```\n # --- Setup for a new edit test run --- checkpointer = MemorySaver() store = InMemoryStore() graph = overall_workflow.compile(checkpointer=checkpointer, store=store) thread_id_2 = uuid.uuid4() thread_config_2 = {"configurable": {"thread_id": thread_id_2}} # Run the graph until the first interrupt. print("Running the graph until the first interrupt...") for chunk in graph.stream({"email_input": email_input_respond}, config=thread_config_2): if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the initial memory state. display_memory_content(store,("email_assistant", "cal_preferences"))\n```\n\n代理暂停，再次提议一个 45 分钟的会议。现在不接受，而是将提议编辑成真正的偏好：一个 30 分钟的会议，主题更简洁。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the user\'s edits to the proposed `schedule_meeting` tool call. edited_schedule_args = { "attendees": ["pm@client.com", "lance@company.com"], "subject": "Tax Planning Discussion", # Changed from "Tax Planning Strategies" "duration_minutes": 30, # Changed from 45 to 30 "preferred_day": "2025-04-22", "start_time": 14 } # Resume the graph by sending an \'edit\' command with our new arguments. print("\\nSimulating user editing the schedule_meeting tool call...") for chunk in graph.stream(Command(resume=[{"type": "edit", "args": {"args": edited_schedule_args}}]), config=thread_config_2): if \'__interrupt__\' in chunk: # Capture the next interrupt Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT (Second Interrupt):") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the memory AGAIN, after the edit has been processed. print("\\nChecking memory after editing schedule_meeting:") display_memory_content(store,("email_assistant", "cal_preferences"))\n```\n\n运行这个看看效果。\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ####### Simulating user editing the schedule_meeting tool call... INTERRUPT OBJECT (Second Interrupt): Action Request: {\'action\': \'write_email\', \'args\': {\'to\': \'pm@client.com\', ...}} Checking memory after editing schedule_meeting: ======= CURRENT MEMORY CONTENT ======= --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. The subject of the meeting should be \'Tax Planning Discussion\' instead of \'Tax Planning Strategies\'. The meeting duration should be 30 minutes instead of 45 minutes. ...\n```\n\n这个输出证明了系统有效。`cal_preferences` 记忆不再是简单的默认值。记忆管理器 LLM 分析了代理提议和编辑之间的差异，将更改概括为更广泛的规则。\n\n它已经学会了对较短会议和更简洁主题的偏好，这个新知识现在成为了代理记忆的永久组成部分。\n\n现在通过编辑邮件草稿来完成工作流。\n\n代码语言：javascript\n\n复制\n\n```\n # The graph is paused. Let\'s define our edits for the email draft. edited_email_args = { "to": "pm@client.com", "subject": "Re: Tax Planning Discussion", "content": "Thanks for reaching out. Sounds good. I\'ve scheduled a 30-minute call for us next Tuesday. Looking forward to it!\\n\\nBest,\\nLance" } # Resume the graph with the \'edit\' command for the write_email tool. print("\\nSimulating user editing the write_email tool call...") for chunk in graph.stream(Command(resume=[{"type": "edit", "args": {"args": edited_email_args}}]), config=thread_config_2): pass # Check the \'response_preferences\' memory to see what was learned. print("\\nChecking memory after editing write_email:") display_memory_content(store, ("email_assistant", "response_preferences")) print("\\n--- Workflow Complete ---")\n```\n\n看看终端中显示了什么。\n\n代码语言：javascript\n\n复制\n\n```\n ######## OUTPUT ######### Simulating user editing the write_email tool call... Checking memory after editing write_email: ======= CURRENT MEMORY CONTENT ======= --- response_preferences --- When responding to meeting scheduling requests, the assistant should schedule a meeting for 30 minutes instead of 45 minutes. The assistant should also use the subject line "Re: Tax Planning Discussion" instead of "Re: Tax season let\'s schedule call". The rest of the user preferences remain the same. --- Workflow Complete ---\n```\n\n再一次学习显而易见。`response_preferences` 已被更新。记忆管理器 LLM 正确识别了语气和结构上的关键差异，提取了关于主题行和会议时长的可概括规则。\n\n通过在单次运行中提供两次编辑，已经在两个不同领域个性化了代理行为，展示了这种反馈循环的力量。\n\n### 长期记忆系统的工作原理\n\n我们通过本文已经看到代理从反馈中学习，但是背后发生了什么呢？这是一个简单而强大的四步循环，将更正转化为代理的新规则。\n\n整个过程的分解：\n\n**反馈是触发器**。学习过程只在提供反馈时才会开始。简单接受一个提议不会改变记忆。只有当编辑一个动作或给出对话式响应时，学习才会被触发。\n\n**调用专用的记忆管理器**。不只是保存原始反馈。而是进行一次特殊用途的 LLM 调用。这个"记忆管理器"使用严格的 `MEMORY_UPDATE_INSTRUCTIONS` 提示词来分析反馈。\n\n**对记忆进行精确更新**。记忆管理器的任务是进行有针对性的更新。它将反馈与现有偏好进行比较，在不覆盖或删除旧规则的情况下集成新规则。这确保代理永远不会忘记过去的教训。\n\n**新知识在下次运行时注入**。更新后的偏好字符串被保存到 `Store` 中。下次代理开始新任务时会获取这个新字符串，将学到的行为注入其提示词中，改变其未来的行为方式。\n\n这个"触发 -> 管理 -> 更新 -> 注入"循环使代理能够从一个通用工具演变为个性化的助手。\n\n作者：Fareed Khan\n\n本文参与\xa0[腾讯云自媒体同步曝光计划](/developer/support-plan)，分享自微信公众号。\n\n原始发表：2025-10-20，如有侵权请联系\xa0[cloudcommunity@tencent.com](mailto:cloudcommunity@tencent.com) 删除\n\n[存储](/developer/tag/10665)\n\n[prompt](/developer/tag/15406)\n\n[代理](/developer/tag/17225)\n\n[工具](/developer/tag/17276)\n\n[函数](/developer/tag/17290)\n\n本文分享自 DeepHub IMBA 微信公众号，前往查看\n\n如有侵权，请联系 [cloudcommunity@tencent.com](mailto:cloudcommunity@tencent.com) 删除。\n\n本文参与\xa0[腾讯云自媒体同步曝光计划](/developer/support-plan)\xa0 ，欢迎热爱写作的你一起参与！\n\n[存储](/developer/tag/10665)\n\n[prompt](/developer/tag/15406)\n\n[代理](/developer/tag/17225)\n\n[工具](/developer/tag/17276)\n\n[函数](/developer/tag/17290)\n\n登录后参与评论\n\n登录 后参与评论\n\n* 线程级记忆（短期）\n\n* 跨线程记忆（长期）\n\n* LangGraph 数据持久层\n  + InMemory Store（用于 notebook 和快速测试）\n  + 本地开发存储（langgraph dev）\n  + 生产存储（LangGraph Platform 或自托管）\n\n* InMemory 功能实践\n\n* 构建代理架构\n  + 定义模式\n  + 创建提示词\n  + 定义工具和实用函数\n\n* 记忆功能与图节点\n\n* 通过人机协作捕获反馈\n\n* 组装成工作流\n\n* 使用记忆测试代理\n  + 测试用例 1：基线——接受提议\n  + 测试用例 2：从直接编辑中学习\n\n* 长期记忆系统的工作原理\n\n对象存储（Cloud Object Storage，COS）是由腾讯云推出的无目录层次结构、无数据格式限制，可容纳海量数据且支持 HTTP/HTTPS 协议访问的分布式存储服务。腾讯云 COS 的存储桶空间无容量上限，无需分区管理，适用于 CDN 数据分发、数据万象处理或大数据计算与分析的数据湖等多种场景。\n\n[产品介绍](https://cloud.tencent.com/product/cos?from=21341&from_column=21341)[产品文档](https://cloud.tencent.com/document/product/436?from=21342&from_column=21342)\n\n* ### 社区\n\n  + [技术文章](/developer/column)\n  + [技术问答](/developer/ask)\n  + [技术沙龙](/developer/salon)\n  + [技术视频](/developer/video)\n  + [学习中心](/developer/learning)\n  + [技术百科](/developer/techpedia)\n  + [技术专区](/developer/zone/list)\n* ### 活动\n\n  + [自媒体同步曝光计划](/developer/support-plan)\n  + [邀请作者入驻](/developer/support-plan-invitation)\n  + [自荐上首页](/developer/article/1535830)\n  + [技术竞赛](/developer/competition)\n* ### 圈层\n\n  + [腾讯云最具价值专家](/tvp)\n  + [腾讯云架构师技术同盟](/developer/program/tm)\n  + [腾讯云创作之星](/developer/program/tci)\n  + [腾讯云TDP](/developer/program/tdp)\n* ### 关于\n\n  + [社区规范](/developer/article/1006434)\n  + [免责声明](/developer/article/1006435)\n  + [联系我们](mailto:cloudcommunity@tencent.com)\n  + [友情链接](/developer/friendlink)\n  + [MCP广场开源版权声明](/developer/article/2537547)\n\n### 腾讯云开发者\n\n### 热门产品\n\n* [域名注册](/product/domain?from=20064&from_column=20064)\n* [云服务器](/product/cvm?from=20064&from_column=20064)\n* [区块链服务](/product/tbaas?from=20064&from_column=20064)\n* [消息队列](/product/message-queue-catalog?from=20064&from_column=20064)\n* [网络加速](/product/ecdn?from=20064&from_column=20064)\n* [云数据库](/product/tencentdb-catalog?from=20064&from_column=20064)\n* [域名解析](/product/dns?from=20064&from_column=20064)\n* [云存储](/product/cos?from=20064&from_column=20064)\n* [视频直播](/product/css?from=20064&from_column=20064)\n\n### 热门推荐\n\n* [人脸识别](/product/facerecognition?from=20064&from_column=20064)\n* [腾讯会议](/product/tm?from=20064&from_column=20064)\n* [企业云](/act/pro/enterprise2022?from=20064&from_column=20064)\n* [CDN加速](/product/cdn?from=20064&from_column=20064)\n* [视频通话](/product/trtc?from=20064&from_column=20064)\n* [图像分析](/product/imagerecognition?from=20064&from_column=20064)\n* [MySQL 数据库](/product/cdb?from=20064&from_column=20064)\n* [SSL 证书](/product/ssl?from=20064&from_column=20064)\n* [语音识别](/product/asr?from=20064&from_column=20064)\n\n### 更多推荐\n\n* [数据安全](/solution/data_protection?from=20064&from_column=20064)\n* [负载均衡](/product/clb?from=20064&from_column=20064)\n* [短信](/product/sms?from=20064&from_column=20064)\n* [文字识别](/product/ocr?from=20064&from_column=20064)\n* [云点播](/product/vod?from=20064&from_column=20064)\n* [大数据](/product/bigdata-class?from=20064&from_column=20064)\n* [小程序开发](/solution/la?from=20064&from_column=20064)\n* [网站监控](/product/tcop?from=20064&from_column=20064)\n* [数据迁移](/product/cdm?from=20064&from_column=20064)\n\nCopyright © 2013 - 2026Tencent Cloud. All Rights Reserved. 腾讯云 版权所有\n\n[深圳市腾讯计算机系统有限公司](https://qcloudimg.tencent-cloud.cn/raw/986376a919726e0c35e96b311f54184d.jpg)\xa0ICP备案/许可证号：[粤B2-20090059](https://beian.miit.gov.cn/#/Integrated/index)[粤公网安备44030502008569号](https://beian.mps.gov.cn/#/query/webSearch?code=44030502008569)\n\n[腾讯云计算（北京）有限责任公司](https://qcloudimg.tencent-cloud.cn/raw/a2390663ee4a95ceeead8fdc34d4b207.jpg) 京ICP证150476号 | \xa0[京ICP备11018762号](https://beian.miit.gov.cn/#/Integrated/index)\n\n[问题归档](/developer/ask/archives.html)[专栏文章](/developer/column/archives.html)[快讯文章归档](/developer/news/archives.html)[关键词归档](/developer/information/all.html)[开发者手册归档](/developer/devdocs/archives.html)[开发者手册 Section 归档](/developer/devdocs/sections_p1.html)\n\nCopyright © 2013 - 2026Tencent Cloud.\n\nAll Rights Reserved. 腾讯云 版权所有\n\n登录 后参与评论', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://cloud.tencent.com/developer/article/2588385', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.8591094, 'saved_path': None}}
2026-02-22 20:38:05,161 - __main__ - INFO - call_tool: name=tavily_download, args={'papers': [{'paper_id': '', 'title': 'LangGraph 记忆系统实战：反馈循环+ 动态Prompt 让AI 持续学习', 'authors': [], 'abstract': '## LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\n[*腾讯云*](/?from=20060&from_column=20060)\n[*开发者社区*](/developer)\n\n[文档](/document/product?from=20702&from_column=20702)[建议反馈](/voc/?from=20703&from_column=20703)[控制台](https://console.cloud.tencent.com?from=20063&from_column=20063)\n\n[首页](/developer)\n\n文章/答案/技术大牛\n\ndeephub\n\n[社区首页](/developer) >[专栏](/developer/column) >LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\n# LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\ndeephub\n\n发布于 2025-11-15 11:44:45\n\n发布于 2025-11-15 11:44:45\n\n8610\n\n文章被收录于专栏：[DeepHub IMBA](/developer/column/86944)DeepHub IMBA\n\n**点击上方“Deephub Imba”,关注公众号,好文章不错过 !**\n\n代理系统或者 RAG 方案，基本都需要一个双层记忆架构，这样 LLM 既能保持对当前上下文的专注，又能记住过往交互的内容。\n\n短期记忆负责单个会话内的即时信息管理，长期记忆则跨会话存储知识，让系统能够持续学习和进化。两者配合，代理才能表现出连贯性、上下文感知能力，看起来更加智能。这些记忆组件在现代 AI 架构中的位置如下图所示：\n\n#### 线程级记忆（短期）\n\n可以把它理解为代理的"工作记忆"。它帮助代理理解上下文，自然地延续讨论，不会丢失之前的步骤。LangGraph 通过检查点机制自动管理这部分记忆。对话结束后，短期记忆会被清空，下次会话则会重新开始一个新的记忆。\n\n#### 跨线程记忆（长期）\n\nLangGraph 将这些数据以 JSON 文档形式保存在记忆存储中，通过命名空间（类似文件夹）和键（类似文件名）整齐组织。因为这种记忆在对话后不会消失，所以代理能够随时间积累知识，提供更一致、更个性化的响应。\n\n> 本文会探讨生产级 AI 系统如何使用 LangGraph 管理长期记忆流。LangGraph 是一个构建可扩展、上下文感知 AI 工作流的主流框架。\n\n### LangGraph 数据持久层\n\n处理代理记忆时，LangGraph 是最常用的组件。其中 Store 功能尤为关键，它根据项目的运行位置管理记忆的保存、检索和更新方式。\n\nLangGraph 提供了几种存储实现，在简单性、持久性和可扩展性之间取得平衡。每种选项适合开发或部署的特定阶段。\n\n#### InMemory Store（用于 notebook 和快速测试）\n\n使用 `from langgraph.store.memory import InMemoryStore` 导入，创建一个完全在内存中运行的存储，使用标准 Python 字典。\n\n不写入磁盘，进程结束后所有信息都会丢失。但速度快，易用，非常适合测试工作流或尝试新的图配置。如果需要，也可以添加语义搜索能力。\n\n#### 本地开发存储（langgraph dev）\n\n这个选项的行为与上面的内存版本类似，但是可以在会话之间提供了基本持久性。\n\n用 `langgraph dev` 命令运行应用时，LangGraph 会自动使用 Python 的 pickle 格式将存储保存到本地文件系统，并在重启开发环境后恢复数据。\n\n这个方式轻量且方便，不需要外部数据库。同样支持语义搜索功能，所以它非常适合开发阶段，但不适合生产环境。\n\n#### 生产存储（LangGraph Platform 或自托管）\n\n大规模或生产部署，LangGraph 使用与 pgvector 集成的 PostgreSQL 数据库实现高效的向量存储和语义检索。\n\n这样可以提供完整的数据持久性、内置可靠性，并且能够处理更大的工作负载或多用户系统。语义搜索依靠pgvector ，默认使用余弦相似度作为相似性度量，也可以根据需求自定义。\n\n这种配置确保记忆数据安全存储，跨会话保持可用，即使在高流量或分布式工作负载下也能稳定运行。\n\n> 基础知识介绍完毕，接下来开始逐步编写完整的工作架构代码。\n\n### InMemory 功能实践\n\n本文要实现的是 InMemory 功能，这是基于 AI 系统中最常用的内存管理方式。\n\n> 它按顺序执行，在逐步构建或测试技术流程时非常实用。\n\nInMemory 功能允许在运行代码时临时存储数据，通过了解它可以有助于我们理解 LangGraph 中内存处理的工作原理。\n\n从 LangGraph 导入 `InMemoryStore` 开始。这个类让我们直接在内存中存储记忆，不需要外部数据库或文件系统。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the InMemoryStore class for storing memories in memory (no persistence) from langgraph.store.memory import InMemoryStore # Initialize an in-memory store instance for use in this notebook in_memory_store = InMemoryStore()\n```\n\n这里创建了 InMemoryStore 的实例，用于保存临时数据。因为只在内存中运行，进程停止后所有存储的数据都会被清除。\n\n> LangGraph 中的每个记忆都保存在命名空间（namespace）中。\n\n命名空间像标签或文件夹，帮助组织记忆。它被定义为元组，可以有一个或多个部分。下面这个例子使用包含用户 ID 和 "memories" 标签的元组。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a user ID for memory storage user_id = "1" # Set the namespace for storing and retrieving memories namespace_for_memory = (user_id, "memories")\n```\n\n命名空间可以代表任何东西，不一定基于用户 ID，所以可以根据应用结构自由分组记忆。\n\n下面我们保存一个记忆到存储中，使用 `put` 方法。这个方法需要三样东西：命名空间、唯一键和实际的记忆值。\n\n这里键是用 `uuid` 库生成的唯一标识符，记忆值是存储信息的字典——一个简单的偏好设置。\n\n代码语言：javascript\n\n复制\n\n```\n import uuid # Generate a unique ID for the memory memory_id = str(uuid.uuid4()) # Create a memory dictionary memory = {"food_preference": "I like pizza"} # Save the memory in the defined namespace in_memory_store.put(namespace_for_memory, memory_id, memory)\n```\n\n这会将记忆条目添加到之前定义的命名空间下的内存存储中。\n\n存储记忆后，可以用 `search` 方法取回。这个方法在命名空间内查找并返回属于它的所有记忆列表。\n\n每个记忆都是一个 `Item` 对象，包含命名空间、键、值和时间戳等详细信息。可以转换为字典以便更清晰地查看数据。\n\n代码语言：javascript\n\n复制\n\n```\n # Retrieve all stored memories for the given namespace memories = in_memory_store.search(namespace_for_memory) # View the latest memory memories[-1].dict()\n```\n\n在 notebook 中运行这段代码，得到以下输出：\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ###### { \'namespace\': [\'1\', \'memories\'], \'key\': \'c8619cd4-3d3f-4108-857c-5c8c12f39e87\', \'value\': {\'food_preference\': \'I like pizza\'}, \'created_at\': \'2025-10-08T15:46:16.531625+00:00\', \'updated_at\': \'2025-10-08T15:46:16.531625+00:00\', \'score\': None }\n```\n\n输出显示了存储的记忆详情。最重要的是 **value** 字段，包含实际保存的信息。其他字段帮助识别和管理记忆创建的时间和位置。\n\n存储就绪后，可以将其连接到图中，让记忆和检查点协同工作。这里使用两个主要组件：\n\n* **InMemorySaver** 管理线程间的检查点\n* **InMemoryStore** 存储跨线程的记忆\n\n代码语言：javascript\n\n复制\n\n```\n # To enable threads (conversations) from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() # To enable across-thread memory from langgraph.store.memory import InMemoryStore in_memory_store = InMemoryStore() # Compile the graph with the checkpointer and store # graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n```\n\n这使图能够记住线程内的对话上下文（短期），并使用相同的内存机制在线程间保留重要信息（长期）。\n\n> 这是转向生产级存储之前测试记忆行为的简单有效方式。\n\n### 构建代理架构\n\n在使用记忆系统工作流之前，需要构建使用它的智能代理。因为本文专注于记忆管理，所以只会构建一个中等复杂的电子邮件助手，模拟在真实场景中探索记忆的工作方式。\n\n下面我们从零开始构建这个系统，定义数据结构、"大脑"（提示词）和能力（工具）。最终得到一个不仅能回复邮件，还能从反馈中学习的代理。\n\n#### 定义模式\n\n处理数据前需要定义其形状。模式是代理信息流的蓝图，确保一切结构化、可预测且类型安全。\n\n首先编写 `RouterSchema`。需要它是为了让初始分类步骤可靠。不能冒险让 LLM 在期望明确决定时返回非结构化文本。\n\n这个 Pydantic 模型会强制 LLM 返回一个干净的 JSON 对象，包含推理过程和一个严格为 \'ignore\'、\'respond\' 或 \'notify\' 之一的分类结果。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the necessary libraries from Pydantic and Python\'s typing module from pydantic import BaseModel, Field from typing_extensions import TypedDict, Literal # Define a Pydantic model for our router\'s structured output. class RouterSchema(BaseModel): """Analyze the unread email and route it according to its content.""" # Add a field for the LLM to explain its step-by-step reasoning. reasoning: str = Field(description="分类背后的逐步推理。") # Add a field to hold the final classification. # The `Literal` type restricts the output to one of these three specific strings. classification: Literal["ignore", "respond", "notify"] = Field( description="电子邮件的分类。" )\n```\n\n这是在为分类 LLM 创建契约。后面与 LangChain 的 `.with_structured_output()` 方法配对时，能保证输出是一个可预测的 Python 对象，让图中的逻辑更加健壮。\n\n接下来需要一个地方存储代理单次运行的所有信息，这就是 `State` 的作用。它像一个中央白板，图的每个部分都可以读写。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the base state class from LangGraph from langgraph.graph import MessagesState # Define the central state object for our graph. class State(MessagesState): # This field will hold the initial raw email data. email_input: dict # This field will store the decision made by our triage router. classification_decision: Literal["ignore", "respond", "notify"]\n```\n\n继承自 LangGraph 的 `MessagesState`，自动获得一个 `messages` 列表来跟踪对话历史。然后添加自定义字段。随着流程从一个节点移动到另一个节点，这个 `State` 对象会被传递并累积信息。\n\n最后定义一个小但重要的 `StateInput` 模式，规定图的初始输入应该是什么样子。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a TypedDict for the initial input to our entire workflow. class StateInput(TypedDict): # The workflow must be started with a dictionary containing an \'email_input\' key. email_input: dict\n```\n\n这个简单的模式从应用入口点就提供了清晰性和类型安全性，确保对图的任何调用都以正确的数据结构开始。\n\n#### 创建提示词\n\n使用提示词方法来指导和引导 LLM 行为。对于代理，会定义几个提示词，每个都有特定的任务。\n\n代理从我们这里学到任何东西之前，需要一套基准指令。这些默认字符串会在第一次运行时加载到记忆存储中，为代理行为提供起点。\n\n首先定义 `default_background` 给代理一个角色。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a default persona for the agent. default_background=""" I\'m Lance, a software engineer at LangChain. """\n```\n\n接下来是 `default_triage_instructions`，分类路由器遵循的初始规则。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the initial rules for the triage LLM. default_triage_instructions = """ Emails that are not worth responding to: - Marketing newsletters and promotional emails - Spam or suspicious emails - CC\'d on FYI threads with no direct questions Emails that require notification but no response: - Team member out sick or on vacation - Build system notifications or deployments Emails that require a response: - Direct questions from team members - Meeting requests requiring confirmation """\n```\n\n然后是 `default_response_preferences`，定义代理的初始写作风格。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the default preferences for how the agent should compose emails. default_response_preferences = """ Use professional and concise language. If the e-mail mentions a deadline, make sure to explicitly acknowledge and reference the deadline in your response. When responding to meeting scheduling requests: - If times are proposed, verify calendar availability and commit to one. - If no times are proposed, check your calendar and propose multiple options. """\n```\n\n最后是 `default_cal_preferences`，指导日程安排行为。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the default preferences for scheduling meetings. default_cal_preferences = """ 30 minute meetings are preferred, but 15 minute meetings are also acceptable. """\n```\n\n现在创建使用这些默认值的提示词，首先是 `triage_system_prompt`。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the system prompt for the initial triage step. triage_system_prompt = """ < Role > Your role is to triage incoming emails based on background and instructions.  Role > < Background > {background}  Background > < Instructions > Categorize each email into IGNORE, NOTIFY, or RESPOND.  Instructions > < Rules > {triage_instructions}  Rules > """\n```\n\n这个提示词模板给分类路由器提供角色和指令。`{background}` 和 `{triage_instructions}` 占位符会被刚才定义的默认字符串填充。\n\n接着是 `triage_user_prompt`，一个简单的模板，用于将原始邮件内容构造成 LLM 易于解析的清晰格式。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the user prompt for triage, which will format the raw email. triage_user_prompt = """ Please determine how to handle the following email: From: {author} To: {to} Subject: {subject} {email_thread}"""\n```\n\n现在是主要组件，必须创建 `agent_system_prompt_hitl_memory`，它包含到目前为止编码的角色和其他指令。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the datetime library to include the current date in the prompt. from datetime import datetime # Define the main system prompt for the response agent. agent_system_prompt_hitl_memory = """ < Role > You are a top-notch executive assistant.  Role > < Tools > You have access to the following tools: {tools_prompt}  Tools > < Instructions > 1. Analyze the email content carefully. 2. Always call one tool at a time until the task is complete. 3. Use Question to ask the user for clarification. 4. Draft emails using write_email. 5. For meetings, check availability and schedule accordingly. - Today\'s date is """ + datetime.now().strftime("%Y-%m-%d") + """ 6. After sending emails, use the Done tool.  Instructions > < Background > {background}  Background > < Response Preferences > {response_preferences}  Response Preferences > < Calendar Preferences > {cal_preferences}  Calendar Preferences > """\n```\n\n这是主响应代理的主指令集。像 `{response_preferences}` 和 `{cal_preferences}` 这样的占位符是记忆系统的关键。\n\n> 它们允许动态地从记忆存储中注入代理学到的知识，使其能够随时间调整行为。\n\n为了让代理改进，为专门的"记忆管理器"LLM 定义特殊提示词。它唯一的工作就是安全、智能地更新记忆存储。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the system prompt for our specialized memory update manager LLM. MEMORY_UPDATE_INSTRUCTIONS = """ # Role You are a memory profile manager for an email assistant. # Rules - NEVER overwrite the entire profile - ONLY add new information - ONLY update facts contradicted by feedback - PRESERVE all other information # Reasoning Steps 1. Analyze the current memory profile. 2. Review feedback messages. 3. Extract relevant preferences. 4. Compare to existing profile. 5. Identify facts to update. 6. Preserve everything else. 7. Output updated profile. # Process current profile for {namespace}  {current_profile}  """\n```\n\n`MEMORY_UPDATE_INSTRUCTIONS` 提示词高度结构化，规则严格：永不覆盖、只做针对性添加、保留现有信息。这种方法对防止代理记忆被破坏至关重要。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a reinforcement prompt to remind the LLM of the most critical rules. MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT = """ Remember: - NEVER overwrite the entire profile - ONLY make targeted additions - ONLY update specific facts contradicted by feedback - PRESERVE all other information """\n```\n\n`MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT` 是现代提示词工程技术。它是要求 LLM 更新记忆时会附加到消息中的最关键规则的简明摘要。重复关键指令有助于确保 LLM 遵守它们。\n\n#### 定义工具和实用函数\n\n代理有了指令，现在需要赋予它采取行动的能力。会定义作为工具的 Python 函数，以及一些辅助实用函数，保持主代码整洁有序。\n\n编写实际工具函数之前，需要一个关于它们的简单文本描述。这是代理在主提示词中会"看到"的内容，让它理解有哪些工具可用以及如何使用。\n\n代码语言：javascript\n\n复制\n\n```\n # A simple string describing the available tools for the LLM. HITL_MEMORY_TOOLS_PROMPT = """ 1. write_email(to, subject, content) - Send emails to specified recipients 2. schedule_meeting(attendees, subject, duration_minutes, preferred_day, start_time) - Schedule calendar meetings 3. check_calendar_availability(day) - Check available time slots 4. Question(content) - Ask follow-up questions 5. Done - Mark the email as sent """\n```\n\n这个字符串本身不是可执行代码，而是作为 LLM 的文档。它会被插入到主 `agent_system_prompt_hitl_memory` 中的 `{tools_prompt}` 占位符。这样代理就知道 `write_email` 函数存在，需要 `to`、`subject` 和 `content` 参数。\n\n好的项目都有一个 `utils.py` 文件来存放执行常见、重复性任务的辅助函数，保持主图逻辑整洁，专注于工作流本身。\n\n首先需要一个函数来解析初始邮件输入。\n\n代码语言：javascript\n\n复制\n\n```\n # This utility unpacks the email input dictionary for easier access. def parse_email(email_input: dict) -> tuple[str, str, str, str]: """Parse an email input dictionary into its constituent parts.""" # Return a tuple containing the author, recipient, subject, and body of the email. return ( email_input["author"], email_input["to"], email_input["subject"], email_input["email_thread"], )\n```\n\n`parse_email` 函数是输入字典的简单解包器。虽然可以在图节点中直接访问 `email_input["author"]`，但这个辅助函数让代码更可读，并集中了解析逻辑。\n\n接下来，一个将邮件内容格式化为 Markdown 供 LLM 使用的函数。\n\n代码语言：javascript\n\n复制\n\n```\n # This function formats the raw email data into clean markdown for the LLM. def format_email_markdown(subject, author, to, email_thread): """Format email details into a nicely formatted markdown string.""" # Use f-string formatting to create a structured string with clear labels. return f""" **Subject**: {subject} **From**: {author} **To**: {to} {email_thread} --- """\n```\n\n`format_email_markdown` 函数接收解析后的邮件部分，将它们排列成干净的 Markdown 格式块。这种结构化格式比原始的非结构化字符串更容易被 LLM 解析，帮助它更好地理解邮件的不同组成部分（发件人、主题、正文）。\n\n最后需要一个函数为人工审阅者格式化代理建议的操作。\n\n代码语言：javascript\n\n复制\n\n```\n # This function creates a human-friendly view of a tool call for the HITL interface. def format_for_display(tool_call: dict) -> str: """Format a tool call into a readable string for the user.""" # Initialize an empty string to build our display. display = "" # Use conditional logic to create custom, readable formats for our main tools. if tool_call["name"] == "write_email": display += f\'# Email Draft\\n\\n**To**: {tool_call["args"].get("to")}\\n**Subject**: {tool_call["args"].get("subject")}\\n\\n{tool_call["args"].get("content")}\' elif tool_call["name"] == "schedule_meeting": display += f\'# Calendar Invite\\n\\n**Meeting**: {tool_call["args"].get("subject")}\\n**Attendees**: {", ".join(tool_call["args"].get("attendees"))}\' elif tool_call["name"] == "Question": display += f\'# Question for User\\n\\n{tool_call["args"].get("content")}\' # Provide a generic fallback for any other tools. else: display += f\'# Tool Call: {tool_call["name"]}\\n\\nArguments:\\n{tool_call["args"]}\' # Return the final formatted string. return display\n```\n\n这个 `format_for_display` 函数对人机协作（HITL）步骤很重要。当代理提出工具调用如 `write_email` 时，不想向人工审阅者展示原始 JSON 对象。\n\n这个函数将技术表示转换为看起来像实际邮件草稿或日历邀请的内容，让用户更容易审阅、编辑或批准。\n\n模式、提示词和实用函数都定义好了，现在可以将它们组装成完整的图，让学习代理运转起来。\n\n### 记忆功能与图节点\n\n从这里开始我们就要实现记忆逻辑了，先看看整个系统如何运作。\n\n代理从静态指令集转变为能够学习的动态系统，关键就在这里。\n\n构建使用记忆的图节点之前，需要实际与 `InMemoryStore` 交互的函数。我们需要创建两个关键函数：一个获取现有偏好，另一个根据反馈更新它们。\n\n首先需要可靠的方式从存储中获取偏好。编写一个叫 `get_memory` 的函数，它会在存储中查找特定偏好（如 `"triage_preferences"`）。找到就返回存储的值。\n\n如果找不到——这会在用户第一次运行时发生——它会使用之前定义的默认内容创建条目。这确保代理始终有一套规则可以遵循。\n\n代码语言：javascript\n\n复制\n\n```\n # A function to retrieve memory from the store or initialize with defaults. def get_memory(store, namespace, default_content=None): """Get memory from the store or initialize with default if it doesn\'t exist.""" # Use the store\'s .get() method to search for an item with a specific key. user_preferences = store.get(namespace, "user_preferences") # If the item exists, return its value (the stored string). if user_preferences: return user_preferences.value # If the item does not exist, this is the first time we\'re accessing this memory. else: # Use the store\'s .put() method to create the memory item with default content. store.put(namespace, "user_preferences", default_content) # Return the default content to be used in this run. return default_content\n```\n\n这个简单的函数功能强大。它抽象了检查和初始化记忆的逻辑。图中的任何节点现在都可以调用 `get_memory` 获取最新的用户偏好，无需知道这是第一次运行还是第一百次。\n\n这里是代理学习被触发的地方。`update_memory` 函数设计用来接收用户反馈——比如编辑过的邮件或自然语言指令——并用它来精炼代理存储的知识。它使用之前精心制作的 `MEMORY_UPDATE_INSTRUCTIONS` 提示词来协调一个特殊用途的 LLM 调用。\n\n为确保 LLM 输出可预测，先定义一个 `UserPreferences` Pydantic 模式。这会强制记忆管理器 LLM 返回一个 JSON 对象，包含推理过程和最终更新的偏好字符串。\n\n代码语言：javascript\n\n复制\n\n```\n # A Pydantic model to structure the output of our memory update LLM call. class UserPreferences(BaseModel): """Updated user preferences based on user\'s feedback.""" # A field for the LLM to explain its reasoning, useful for debugging. chain_of_thought: str = Field(description="Reasoning about which user preferences need to add / update if required") # The final, updated string of user preferences. user_preferences: str = Field(description="Updated user preferences")\n```\n\n现在可以编写 `update_memory` 函数本身。它会检索当前偏好，将它们与用户反馈和特殊提示词结合，然后将 LLM 的精炼输出保存回存储。\n\n代码语言：javascript\n\n复制\n\n```\n # Import AIMessage to help filter messages before sending them to the memory updater. from langchain_core.messages import AIMessage # This function intelligently updates the memory store based on user feedback. def update_memory(store, namespace, messages): """Update memory profile in the store.""" # First, get the current memory from the store so we can provide it as context. user_preferences = store.get(namespace, "user_preferences") # Initialize a new LLM instance specifically for this task, configured for structured output. memory_updater_llm = llm.with_structured_output(UserPreferences) # This is a small but important fix: filter out any previous AI messages with tool calls. # Passing these complex objects can sometimes cause errors in the downstream LLM call. messages_to_send = [ msg for msg in messages if not (isinstance(msg, AIMessage) and hasattr(msg, \'tool_calls\') and msg.tool_calls) ] # Invoke the LLM with the memory prompt, current preferences, and the user\'s feedback. result = memory_updater_llm.invoke( [ # The system prompt that instructs the LLM on how to update memory. {"role": "system", "content": MEMORY_UPDATE_INSTRUCTIONS.format(current_profile=user_preferences.value, namespace=namespace)}, ] # Append the filtered conversation messages containing the feedback. + messages_to_send ) # Save the newly generated preference string back into the store, overwriting the old one. store.put(namespace, "user_preferences", result.user_preferences)\n```\n\n这个函数是代理学习能力的主要组成部分。通过使用带有严格指令的专用 LLM 调用，确保记忆以受控和增量方式更新，让代理随时间逐渐与用户偏好对齐。\n\n现在可以定义代理的核心逻辑。在 LangGraph 中，这个逻辑封装在节点中。每个节点都是一个 Python 函数，接收图的当前 `State`，执行操作，返回对该状态的更新。\n\n> 电子邮件助手会有几个关键节点，处理从初始分类到生成最终响应的所有事情。\n\n工作流中的第一个节点是 `triage_router`。这个函数的任务是对收到的邮件做初步决定……\n\n> 应该回复、只通知用户，还是完全忽略？长期记忆首次发挥作用就在这里。\n\n路由器会使用 `get_memory` 函数获取用户最新的 `triage_preferences` 并注入到提示词中，确保决策能力随时间提高。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the Command class for routing and BaseStore for type hinting from langgraph.types import Command from langgraph.store.base import BaseStore # Define the first node in our graph, the triage router. def triage_router(state: State, store: BaseStore) -> Command: """Analyze email content to decide the next step.""" # Unpack the raw email data using our utility function. author, to, subject, email_thread = parse_email(state["email_input"]) # Format the email content into a clean string for the LLM. email_markdown = format_email_markdown(subject, author, to, email_thread) # Here is the memory integration: fetch the latest triage instructions. # If they don\'t exist, it will use the `default_triage_instructions`. triage_instructions = get_memory(store, ("email_assistant", "triage_preferences"), default_triage_instructions) # Format the system prompt, injecting the retrieved triage instructions. system_prompt = triage_system_prompt.format( background=default_background, triage_instructions=triage_instructions, ) # Format the user prompt with the specific details of the current email. user_prompt = triage_user_prompt.format( author=author, to=to, subject=subject, email_thread=email_thread ) # Invoke the LLM router, which is configured to return our `RouterSchema`. result = llm_router.invoke( [ {"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}, ] ) # Based on the LLM\'s classification, decide which node to go to next. if result.classification == "respond": print("📧 Classification: RESPOND - This email requires a response") # Set the next node to be the \'response_agent\'. goto = "response_agent" # Update the state with the decision and the formatted email for the agent. update = { "classification_decision": result.classification, "messages": [{"role": "user", "content": f"Respond to the email: {email_markdown}"}], } elif result.classification == "ignore": print("🚫 Classification: IGNORE - This email can be safely ignored") # End the workflow immediately. goto = END # Update the state with the classification decision. update = {"classification_decision": result.classification} elif result.classification == "notify": print("🔔 Classification: NOTIFY - This email contains important information") # Go to the human-in-the-loop handler for notification. goto = "triage_interrupt_handler" # Update the state with the classification decision. update = {"classification_decision": result.classification} else: # Raise an error if the classification is invalid. raise ValueError(f"Invalid classification: {result.classification}") # Return a Command object to tell LangGraph where to go next and what to update. return Command(goto=goto, update=update)\n```\n\n这个节点是整个系统的入口。通过添加一行 `triage_instructions = get_memory(...)`，就把它从静态路由器变成了能学习的路由器。当用户对分类决策提供反馈时，存储中的 `triage_preferences` 会被更新，这个节点就会自动开始在未来的邮件上做出更好、更个性化的分类。\n\n邮件被分类为 "respond" 时，会被传递给主响应代理。该代理的核心是 `llm_call` 节点。这个函数的目的是获取当前对话历史并采取下一步行动，通常是决定调用哪个工具。\n\n跟分类路由器一样，这个节点集成了记忆来指导决策。它获取 `response_preferences` 和 `cal_preferences`，确保行为与用户学到的风格保持一致。\n\n代码语言：javascript\n\n复制\n\n```\n # This is the primary reasoning node for the response agent. def llm_call(state: State, store: BaseStore): """LLM decides whether to call a tool or not, using stored preferences.""" # Fetch the user\'s latest calendar preferences from the memory store. cal_preferences = get_memory(store, ("email_assistant", "cal_preferences"), default_cal_preferences) # Fetch the user\'s latest response (writing style) preferences. response_preferences = get_memory(store, ("email_assistant", "response_preferences"), default_response_preferences) # Filter out previous AI messages with tool calls to prevent API errors. messages_to_send = [ msg for msg in state["messages"] if not (isinstance(msg, AIMessage) and hasattr(msg, \'tool_calls\') and msg.tool_calls) ] # Invoke the main LLM, which is bound to our set of tools. # The prompt is formatted with the preferences retrieved from memory. response = llm_with_tools.invoke( [ {"role": "system", "content": agent_system_prompt_hitl_memory.format( tools_prompt=HITL_MEMORY_TOOLS_PROMPT, background=default_background, response_preferences=response_preferences, cal_preferences=cal_preferences )} ] + messages_to_send ) # Return the LLM\'s response to be added to the state. return {"messages": [response]}\n```\n\n这个节点展示了长期记忆的重要性。每次执行时都会提取用户最新的写作风格和日历偏好。\n\n当用户提供反馈说更喜欢较短的邮件或 30 分钟的会议时，`update_memory` 函数会修改存储。下次这个 `llm_call` 节点运行时，会自动获取那些新偏好并注入到提示词中，立即改变代理行为，无需任何代码更改。\n\n这就创建了一个反馈循环，代理在其中不断适应用户。\n\n### 通过人机协作捕获反馈\n\n代理不仅仅是在行动，还得知道何时请求帮助或确认。接下来要构建的节点集是中断处理器。\n\n这些特殊节点会暂停图的执行并等待人类输入。神奇的地方就发生在这里：在这些步骤中提供的反馈会被捕获并用于更新代理的长期记忆。\n\n会有两个中断点：\n\n一个在初始分类之后（用于 `notify` 分类），另一个更复杂，用于审查代理建议的工具调用。\n\n首先构建 `triage_interrupt_handler`。当 `triage_router` 将邮件分类为 `notify` 时，这个节点被触发。代理不会对邮件采取行动，而是呈现给用户并请求决策：\n\n> 应该忽略，还是实际回复？用户在这里的选择是关于分类偏好的宝贵反馈。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the `interrupt` function from LangGraph. from langgraph.types import interrupt # Define the interrupt handler for the triage step. def triage_interrupt_handler(state: State, store: BaseStore) -> Command: """Handles interrupts from the triage step, pausing for user input.""" # Parse the email input to format it for display. author, to, subject, email_thread = parse_email(state["email_input"]) email_markdown = format_email_markdown(subject, author, to, email_thread) # This is the data structure that defines the interrupt. # It specifies the action, the allowed user responses, and the content to display. request = { "action_request": { "action": f"Email Assistant: {state[\'classification_decision\']}", "args": {} }, "config": { "allow_ignore": True, "allow_respond": True }, "description": email_markdown, } # The `interrupt()` function pauses the graph and sends the request to the user. # It waits here until it receives a response. response = interrupt([request])[0] # Now, we process the user\'s response. if response["type"] == "response": # The user decided to respond, overriding the \'notify\' classification. user_input = response["args"] # We create a message to pass to the memory updater. messages = [{"role": "user", "content": f"The user decided to respond to the email, so update the triage preferences to capture this."}] # This is a key step: we call `update_memory` to teach the agent. update_memory(store, ("email_assistant", "triage_preferences"), messages) # Prepare to route to the main response agent. goto = "response_agent" # Update the state with the user\'s feedback. update = {"messages": [{"role": "user", "content": f"User wants to reply. Use this feedback: {user_input}"}]} elif response["type"] == "ignore": # The user confirmed the email should be ignored. messages = [{"role": "user", "content": f"The user decided to ignore the email even though it was classified as notify. Update triage preferences to capture this."}] # We still update memory to reinforce this preference. update_memory(store, ("email_assistant", "triage_preferences"), messages) # End the workflow. goto = END update = {} # No message update needed. else: raise ValueError(f"Invalid response: {response}") # Return a Command to direct the graph\'s next step. return Command(goto=goto, update=update)\n```\n\n如果代理认为一封邮件只是通知，但用户决定回复，`update_memory` 就会被调用。记忆管理器 LLM 会看到消息"用户决定回复..."并分析邮件内容。\n\n然后它会精确地更新 `triage_preferences` 字符串，也许通过将"构建系统通知"从 `NOTIFY` 类别移动到 `RESPOND` 类别。下次类似邮件到达时，`triage_router` 会做出更好、更个性化的决定。\n\n但还需要一个主中断处理器，这是图中最复杂的节点。`llm_call` 节点提出要使用的工具（如 `write_email` 或 `schedule_meeting`）后，这个 `interrupt_handler` 就会介入。它将代理建议的操作呈现给用户审查。\n\n然后用户可以接受、忽略、提供自然语言反馈或直接编辑。每种选择都为记忆系统提供了不同且有价值的信号。\n\n代码语言：javascript\n\n复制\n\n```\n # The main interrupt handler for reviewing tool calls. def interrupt_handler(state: State, store: BaseStore) -> Command: """Creates an interrupt for human review of tool calls and updates memory.""" # We\'ll build up a list of new messages to add to the state. result = [] # By default, we\'ll loop back to the LLM after this. goto = "llm_call" # The agent can propose multiple tool calls, so we loop through them. for tool_call in state["messages"][-1].tool_calls: # We only want to interrupt for certain "high-stakes" tools. hitl_tools = ["write_email", "schedule_meeting", "Question"] if tool_call["name"] not in hitl_tools: # For other tools (like check_calendar), execute them without interruption. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(tool_call["args"]) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) continue # Format the proposed action for display to the human reviewer. tool_display = format_for_display(tool_call) # Define the interrupt request payload. request = { "action_request": {"action": tool_call["name"], "args": tool_call["args"]}, "config": { "allow_ignore": True, "allow_respond": True, "allow_edit": True, "allow_accept": True }, "description": tool_display, } # Pause the graph and wait for the user\'s response. response = interrupt([request])[0] # --- MEMORY UPDATE LOGIC BASED ON USER RESPONSE --- if response["type"] == "edit": # The user directly edited the agent\'s proposed action. initial_tool_call = tool_call["args"] edited_args = response["args"]["args"] # This is the most direct form of feedback. We call `update_memory`. if tool_call["name"] == "write_email": update_memory(store, ("email_assistant", "response_preferences"), [{"role": "user", "content": f"User edited the email. Initial draft: {initial_tool_call}. Edited draft: {edited_args}."}]) elif tool_call["name"] == "schedule_meeting": update_memory(store, ("email_assistant", "cal_preferences"), [{"role": "user", "content": f"User edited the meeting. Initial invite: {initial_tool_call}. Edited invite: {edited_args}."}]) # Execute the tool with the user\'s edited arguments. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(edited_args) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) elif response["type"] == "response": # The user gave natural language feedback. user_feedback = response["args"] # We capture this feedback and use it to update memory. if tool_call["name"] == "write_email": update_memory(store, ("email_assistant", "response_preferences"), [{"role": "user", "content": f"User gave feedback on the email draft: {user_feedback}"}]) elif tool_call["name"] == "schedule_meeting": update_memory(store, ("email_assistant", "cal_preferences"), [{"role": "user", "content": f"User gave feedback on the meeting invite: {user_feedback}"}]) # We don\'t execute the tool. Instead, we pass the feedback back to the agent. result.append({"role": "tool", "content": f"User gave feedback: {user_feedback}", "tool_call_id": tool_call["id"]}) elif response["type"] == "ignore": # The user decided this action should not be taken. This is triage feedback. update_memory(store, ("email_assistant", "triage_preferences"), [{"role": "user", "content": f"User ignored the proposal to {tool_call[\'name\']}. This email should not have been classified as \'respond\'."}]) result.append({"role": "tool", "content": "User ignored this. End the workflow.", "tool_call_id": tool_call["id"]}) goto = END elif response["type"] == "accept": # The user approved the action. No memory update is needed. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(tool_call["args"]) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) # Return a command with the next node and the messages to add to the state. return Command(goto=goto, update={"messages": result})\n```\n\n这个节点是学习系统的核心。注意到没有，每种类型的用户反馈——编辑、响应和忽略——都会触发对 `update_memory` 的调用，并带有特定的、上下文相关的消息。\n\n当用户将会议时长从 45 分钟编辑为 30 分钟时，记忆管理器 LLM 会看到这个明确信号，并更新 `cal_preferences` 以在未来偏好 30 分钟的会议。当用户说"让它不那么正式"时，LLM 会对此进行概括，并在 `response_preferences` 中添加新规则。这种持续、细粒度的反馈循环让代理能够随时间成为高度个性化的助手。\n\n### 组装成工作流\n\n代理的所有单独组件都构建好了：模式、提示词、工具、实用函数和图节点。现在用 LangGraph 将它们组装成功能性的状态机。这涉及定义图结构、添加节点以及指定连接它们的边。\n\n主 `llm_call` 节点运行后，代理会提出一个或多个工具调用。需要一种方法来决定接下来会发生什么。代理应该停止，还是应该进入人工审查步骤？这由条件边来处理。它是一个简单的函数，检查状态中的最后一条消息并指导图的流程。\n\n代码语言：javascript\n\n复制\n\n```\n # This function determines the next step after the LLM has made its decision. def should_continue(state: State) -> Literal["interrupt_handler", END]: """Route to the interrupt handler or end the workflow if the \'Done\' tool is called.""" # Get the list of messages from the current state. messages = state["messages"] # Get the most recent message, which contains the agent\'s proposed action. last_message = messages[-1] # Check if the last message contains any tool calls. if last_message.tool_calls: # Loop through each proposed tool call. for tool_call in last_message.tool_calls: # If the agent has decided it\'s finished, we end the workflow. if tool_call["name"] == "Done": return END # For any other tool, we proceed to the human review step. else: return "interrupt_handler"\n```\n\n这个函数是响应代理的主路由器。它检查代理的决定并充当交通警察。如果调用了 `Done` 工具，通过返回 `END` 表示过程已完成。\n\n对于任何其他工具调用，它将图路由到 `interrupt_handler` 节点进行人工审查，确保在未经批准的情况下不采取任何行动。\n\n现在可以组装图，直观地看到它的样子。用 `StateGraph` 定义结构。该过程涉及两个主要阶段：\n\n构建 `response_agent` 子图——包含 `llm_call` -> `interrupt_handler` 的核心循环。构建 `overall_workflow`——这个主图从 `triage_router` 开始，并将 `response_agent` 子图作为其节点之一。\n\n这种方式保持架构清晰易懂。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the main graph-building class from LangGraph. from langgraph.graph import StateGraph, START, END # --- Part 1: Build the Response Agent Subgraph --- # Initialize a new state graph with our defined `State` schema. agent_builder = StateGraph(State) # Add the \'llm_call\' node to the graph. agent_builder.add_node("llm_call", llm_call) # Add the \'interrupt_handler\' node to the graph. agent_builder.add_node("interrupt_handler", interrupt_handler) # Set the entry point of this subgraph to be the \'llm_call\' node. agent_builder.add_edge(START, "llm_call") # Add the conditional edge that routes from \'llm_call\' to either \'interrupt_handler\' or END. agent_builder.add_conditional_edges( "llm_call", should_continue, { "interrupt_handler": "interrupt_handler", END: END, }, ) # After the interrupt handler, the graph always loops back to the LLM to continue the task. agent_builder.add_edge("interrupt_handler", "llm_call") # Compile the subgraph into a runnable object. response_agent = agent_builder.compile() # --- Part 2: Build the Overall Workflow --- # Initialize the main graph, defining its input schema as `StateInput`. overall_workflow = ( StateGraph(State, input=StateInput) # Add the triage router as the first node. .add_node("triage_router", triage_router) # Add the triage interrupt handler node. .add_node("triage_interrupt_handler", triage_interrupt_handler) # Add our entire compiled `response_agent` subgraph as a single node. .add_node("response_agent", response_agent) # Set the entry point for the entire workflow. .add_edge(START, "triage_router") # Define the edges from the triage router to the appropriate next steps. .add_edge("triage_router", "response_agent") .add_edge("triage_router", "triage_interrupt_handler") .add_edge("triage_interrupt_handler", "response_agent") ) # Compile the final, complete graph. email_assistant = overall_workflow.compile()\n```\n\n`triage_router` 做初步决定，然后分支到结束流程、通过 `triage_interrupt_handler` 询问用户输入，或者将控制权交给 `response_agent`。\n\n`response_agent` 进入自己的思考（`llm_call`）和请求审查（`interrupt_handler`）循环，在此过程中更新记忆，直到任务完成。\n\n这种有状态架构使得 LangGraph 非常适合构建复杂的、能够学习的代理。现在可以拿着这个编译好的 `email_assistant` 开始测试它从反馈中学习的能力。\n\n### 使用记忆测试代理\n\n记忆功能已经实现到电子邮件助手中，接下来测试系统如何从用户反馈中学习并随时间适应。这部分测试会探讨不同类型的用户交互如何创建不同的记忆更新，从而提高助手未来的性能。\n\n主要解决一下的问题：\n\n* 系统如何捕获和持久化用户偏好？\n* 存储的偏好以何种方式影响后续决策过程？\n* 哪些用户交互模式会触发特定类型的记忆更新？\n\n首先构建一个辅助函数来显示记忆内容，以便在整个测试过程中跟踪其演变。\n\n代码语言：javascript\n\n复制\n\n```\n # Import necessary libraries for testing. import uuid from langgraph.checkpoint.memory import MemorySaver from langgraph.types import Command from langgraph.store.memory import InMemoryStore # Define a helper function to display the content of our memory store. def display_memory_content(store, namespace=None): """A utility to print the current state of the memory store.""" # Print a header for clarity. print("\\n======= CURRENT MEMORY CONTENT =======") # If a specific namespace is requested, show only that one. if namespace: # Retrieve the memory item for the specified namespace. memory = store.get(namespace, "user_preferences") print(f"\\n--- {namespace[1]} ---") if memory: print(memory.value) else: print("No memory found") # If no specific namespace is given, show all of them. else: # Define the list of all possible namespaces we are using. for ns in [ ("email_assistant", "triage_preferences"), ("email_assistant", "response_preferences"), ("email_assistant", "cal_preferences"), ("email_assistant", "background") ]: # Retrieve and print the memory content for each namespace. memory = store.get(ns, "user_preferences") print(f"\\n--- {ns[1]} ---") if memory: print(memory.value) else: print("No memory found") print("=======================================\\n")\n```\n\n这个实用程序提供了一个实时窗口，可以观察代理不断演变的知识库，能轻松看到每次交互后到底学到了什么。\n\n开始执行不同的测试用例。\n\n#### 测试用例 1：基线——接受提议\n\n第一个测试检查当用户未经修改地接受代理操作时会发生什么。这个基线案例帮助理解在没有提供反馈时系统的行为。期望代理使用记忆来做决定，但不会更新它。\n\n先设置一个新的测试运行。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the input email for our test case. email_input_respond = { "to": "Lance Martin ", "author": "Project Manager ", "subject": "Tax season let\'s schedule call", "email_thread": "Lance,\\n\\nIt\'s tax season again... Are you available sometime next week? ... for about 45 minutes." } # --- Setup for a new test run --- # Initialize a new checkpointer and a fresh, empty memory store. checkpointer = MemorySaver() store = InMemoryStore() # Compile our graph, connecting it to our new checkpointer and store. graph = overall_workflow.compile(checkpointer=checkpointer, store=store) # Create a unique ID and configuration for this conversation. thread_id_1 = uuid.uuid4() thread_config_1 = {"configurable": {"thread_id": thread_id_1}} # Run the graph until its first interrupt. print("Running the graph until the first interrupt...") for chunk in graph.stream({"email_input": email_input_respond}, config=thread_config_1): if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the memory state after the first interrupt. display_memory_content(store)\n```\n\n图会一直运行直到代理提出第一个动作并暂停供审查。\n\n代码语言：javascript\n\n复制\n\n```\n ####### OUTPUT ######### Running the graph until the first interrupt... 📧 Classification: RESPOND - This email requires a response INTERRUPT OBJECT: Action Request: {\'action\': \'schedule_meeting\', \'args\': {\'attendees\': [\'lance@company.com\', \'pm@client.com\'], \'subject\': \'Tax Planning Strategies\', \'duration_minutes\': 45, ...}} ======= CURRENT MEMORY CONTENT ======= --- triage_preferences --- Emails that are not worth responding to: ... --- response_preferences --- Use professional and concise language. ... --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. --- background --- No memory found =======================================\n```\n\n输出显示了两件关键的事。首先，代理正确提出了一个 45 分钟的 `schedule_meeting` 工具调用，尊重了发件人的请求，即使默认偏好是 30 分钟。\n\n其次，`display_memory_content` 函数确认所有记忆命名空间都已使用默认值初始化。尚未发生学习。\n\n现在接受代理的提议。\n\n代码语言：javascript\n\n复制\n\n```\n # Resume the graph by sending an \'accept\' command. print(f"\\nSimulating user accepting the {Interrupt_Object.value[0][\'action_request\'][\'action\']} tool call...") for chunk in graph.stream(Command(resume=[{"type": "accept"}]), config=thread_config_1): # Let the graph run until its next natural pause point. if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}")\n```\n\n代理执行会议工具并进入下一个逻辑步骤：起草一封确认邮件。然后再次中断供审查。\n\n代码语言：javascript\n\n复制\n\n```\n Simulating user accepting the schedule_meeting tool call... INTERRUPT OBJECT: Action Request: {\'action\': \'write_email\', \'args\': {\'to\': \'pm@client.com\', \'subject\': "Re: Tax season let\'s schedule call", \'content\': \'Dear Project Manager, I have scheduled a meeting...for 45 minutes...\'}}\n```\n\n代理已经起草了适当的确认邮件，等待最终批准。现在接受这第二个提议并检查记忆的最终状态。\n\n代码语言：javascript\n\n复制\n\n```\n # Resume the graph one last time with another \'accept\' command. print(f"\\nSimulating user accepting the {Interrupt_Object.value[0][\'action_request\'][\'action\']} tool call...") for chunk in graph.stream(Command(resume=[{"type": "accept"}]), config=thread_config_1): pass # Let the graph finish. # Check the final state of all memory namespaces. display_memory_content(store)\n```\n\n工作流完成。用户批准了代理的所有操作。\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ####### Simulating user accepting the write_email tool call... ======= CURRENT MEMORY CONTENT ======= --- triage_preferences --- Emails that are not worth responding to: ... --- response_preferences --- Use professional and concise language. ... --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. --- background --- No memory found =======================================\n```\n\n最终的记忆检查证实了假设。即使在一次完整、成功的运行之后，记忆内容也与其初始默认状态相同。这是正确的行为。简单的接受不能提供强烈的学习信号，所以代理明智地没有改变其长期知识。它使用记忆，但没有明确反馈就不会改变它。\n\n#### 测试用例 2：从直接编辑中学习\n\n下面我们测试一个有趣的例子，看看当通过直接编辑代理提议来提供明确反馈时会发生什么。这创建了一个清晰的"之前"和"之后"场景，记忆管理器 LLM 可以从中学习。\n\n用相同的邮件开始一次新的运行。\n\n代码语言：javascript\n\n复制\n\n```\n # --- Setup for a new edit test run --- checkpointer = MemorySaver() store = InMemoryStore() graph = overall_workflow.compile(checkpointer=checkpointer, store=store) thread_id_2 = uuid.uuid4() thread_config_2 = {"configurable": {"thread_id": thread_id_2}} # Run the graph until the first interrupt. print("Running the graph until the first interrupt...") for chunk in graph.stream({"email_input": email_input_respond}, config=thread_config_2): if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the initial memory state. display_memory_content(store,("email_assistant", "cal_preferences"))\n```\n\n代理暂停，再次提议一个 45 分钟的会议。现在不接受，而是将提议编辑成真正的偏好：一个 30 分钟的会议，主题更简洁。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the user\'s edits to the proposed `schedule_meeting` tool call. edited_schedule_args = { "attendees": ["pm@client.com", "lance@company.com"], "subject": "Tax Planning Discussion", # Changed from "Tax Planning Strategies" "duration_minutes": 30, # Changed from 45 to 30 "preferred_day": "2025-04-22", "start_time": 14 } # Resume the graph by sending an \'edit\' command with our new arguments. print("\\nSimulating user editing the schedule_meeting tool call...") for chunk in graph.stream(Command(resume=[{"type": "edit", "args": {"args": edited_schedule_args}}]), config=thread_config_2): if \'__interrupt__\' in chunk: # Capture the next interrupt Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT (Second Interrupt):") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the memory AGAIN, after the edit has been processed. print("\\nChecking memory after editing schedule_meeting:") display_memory_content(store,("email_assistant", "cal_preferences"))\n```\n\n运行这个看看效果。\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ####### Simulating user editing the schedule_meeting tool call... INTERRUPT OBJECT (Second Interrupt): Action Request: {\'action\': \'write_email\', \'args\': {\'to\': \'pm@client.com\', ...}} Checking memory after editing schedule_meeting: ======= CURRENT MEMORY CONTENT ======= --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. The subject of the meeting should be \'Tax Planning Discussion\' instead of \'Tax Planning Strategies\'. The meeting duration should be 30 minutes instead of 45 minutes. ...\n```\n\n这个输出证明了系统有效。`cal_preferences` 记忆不再是简单的默认值。记忆管理器 LLM 分析了代理提议和编辑之间的差异，将更改概括为更广泛的规则。\n\n它已经学会了对较短会议和更简洁主题的偏好，这个新知识现在成为了代理记忆的永久组成部分。\n\n现在通过编辑邮件草稿来完成工作流。\n\n代码语言：javascript\n\n复制\n\n```\n # The graph is paused. Let\'s define our edits for the email draft. edited_email_args = { "to": "pm@client.com", "subject": "Re: Tax Planning Discussion", "content": "Thanks for reaching out. Sounds good. I\'ve scheduled a 30-minute call for us next Tuesday. Looking forward to it!\\n\\nBest,\\nLance" } # Resume the graph with the \'edit\' command for the write_email tool. print("\\nSimulating user editing the write_email tool call...") for chunk in graph.stream(Command(resume=[{"type": "edit", "args": {"args": edited_email_args}}]), config=thread_config_2): pass # Check the \'response_preferences\' memory to see what was learned. print("\\nChecking memory after editing write_email:") display_memory_content(store, ("email_assistant", "response_preferences")) print("\\n--- Workflow Complete ---")\n```\n\n看看终端中显示了什么。\n\n代码语言：javascript\n\n复制\n\n```\n ######## OUTPUT ######### Simulating user editing the write_email tool call... Checking memory after editing write_email: ======= CURRENT MEMORY CONTENT ======= --- response_preferences --- When responding to meeting scheduling requests, the assistant should schedule a meeting for 30 minutes instead of 45 minutes. The assistant should also use the subject line "Re: Tax Planning Discussion" instead of "Re: Tax season let\'s schedule call". The rest of the user preferences remain the same. --- Workflow Complete ---\n```\n\n再一次学习显而易见。`response_preferences` 已被更新。记忆管理器 LLM 正确识别了语气和结构上的关键差异，提取了关于主题行和会议时长的可概括规则。\n\n通过在单次运行中提供两次编辑，已经在两个不同领域个性化了代理行为，展示了这种反馈循环的力量。\n\n### 长期记忆系统的工作原理\n\n我们通过本文已经看到代理从反馈中学习，但是背后发生了什么呢？这是一个简单而强大的四步循环，将更正转化为代理的新规则。\n\n整个过程的分解：\n\n**反馈是触发器**。学习过程只在提供反馈时才会开始。简单接受一个提议不会改变记忆。只有当编辑一个动作或给出对话式响应时，学习才会被触发。\n\n**调用专用的记忆管理器**。不只是保存原始反馈。而是进行一次特殊用途的 LLM 调用。这个"记忆管理器"使用严格的 `MEMORY_UPDATE_INSTRUCTIONS` 提示词来分析反馈。\n\n**对记忆进行精确更新**。记忆管理器的任务是进行有针对性的更新。它将反馈与现有偏好进行比较，在不覆盖或删除旧规则的情况下集成新规则。这确保代理永远不会忘记过去的教训。\n\n**新知识在下次运行时注入**。更新后的偏好字符串被保存到 `Store` 中。下次代理开始新任务时会获取这个新字符串，将学到的行为注入其提示词中，改变其未来的行为方式。\n\n这个"触发 -> 管理 -> 更新 -> 注入"循环使代理能够从一个通用工具演变为个性化的助手。\n\n作者：Fareed Khan\n\n本文参与\xa0[腾讯云自媒体同步曝光计划](/developer/support-plan)，分享自微信公众号。\n\n原始发表：2025-10-20，如有侵权请联系\xa0[cloudcommunity@tencent.com](mailto:cloudcommunity@tencent.com) 删除\n\n[存储](/developer/tag/10665)\n\n[prompt](/developer/tag/15406)\n\n[代理](/developer/tag/17225)\n\n[工具](/developer/tag/17276)\n\n[函数](/developer/tag/17290)\n\n本文分享自 DeepHub IMBA 微信公众号，前往查看\n\n如有侵权，请联系 [cloudcommunity@tencent.com](mailto:cloudcommunity@tencent.com) 删除。\n\n本文参与\xa0[腾讯云自媒体同步曝光计划](/developer/support-plan)\xa0 ，欢迎热爱写作的你一起参与！\n\n[存储](/developer/tag/10665)\n\n[prompt](/developer/tag/15406)\n\n[代理](/developer/tag/17225)\n\n[工具](/developer/tag/17276)\n\n[函数](/developer/tag/17290)\n\n登录后参与评论\n\n登录 后参与评论\n\n* 线程级记忆（短期）\n\n* 跨线程记忆（长期）\n\n* LangGraph 数据持久层\n  + InMemory Store（用于 notebook 和快速测试）\n  + 本地开发存储（langgraph dev）\n  + 生产存储（LangGraph Platform 或自托管）\n\n* InMemory 功能实践\n\n* 构建代理架构\n  + 定义模式\n  + 创建提示词\n  + 定义工具和实用函数\n\n* 记忆功能与图节点\n\n* 通过人机协作捕获反馈\n\n* 组装成工作流\n\n* 使用记忆测试代理\n  + 测试用例 1：基线——接受提议\n  + 测试用例 2：从直接编辑中学习\n\n* 长期记忆系统的工作原理\n\n对象存储（Cloud Object Storage，COS）是由腾讯云推出的无目录层次结构、无数据格式限制，可容纳海量数据且支持 HTTP/HTTPS 协议访问的分布式存储服务。腾讯云 COS 的存储桶空间无容量上限，无需分区管理，适用于 CDN 数据分发、数据万象处理或大数据计算与分析的数据湖等多种场景。\n\n[产品介绍](https://cloud.tencent.com/product/cos?from=21341&from_column=21341)[产品文档](https://cloud.tencent.com/document/product/436?from=21342&from_column=21342)\n\n* ### 社区\n\n  + [技术文章](/developer/column)\n  + [技术问答](/developer/ask)\n  + [技术沙龙](/developer/salon)\n  + [技术视频](/developer/video)\n  + [学习中心](/developer/learning)\n  + [技术百科](/developer/techpedia)\n  + [技术专区](/developer/zone/list)\n* ### 活动\n\n  + [自媒体同步曝光计划](/developer/support-plan)\n  + [邀请作者入驻](/developer/support-plan-invitation)\n  + [自荐上首页](/developer/article/1535830)\n  + [技术竞赛](/developer/competition)\n* ### 圈层\n\n  + [腾讯云最具价值专家](/tvp)\n  + [腾讯云架构师技术同盟](/developer/program/tm)\n  + [腾讯云创作之星](/developer/program/tci)\n  + [腾讯云TDP](/developer/program/tdp)\n* ### 关于\n\n  + [社区规范](/developer/article/1006434)\n  + [免责声明](/developer/article/1006435)\n  + [联系我们](mailto:cloudcommunity@tencent.com)\n  + [友情链接](/developer/friendlink)\n  + [MCP广场开源版权声明](/developer/article/2537547)\n\n### 腾讯云开发者\n\n### 热门产品\n\n* [域名注册](/product/domain?from=20064&from_column=20064)\n* [云服务器](/product/cvm?from=20064&from_column=20064)\n* [区块链服务](/product/tbaas?from=20064&from_column=20064)\n* [消息队列](/product/message-queue-catalog?from=20064&from_column=20064)\n* [网络加速](/product/ecdn?from=20064&from_column=20064)\n* [云数据库](/product/tencentdb-catalog?from=20064&from_column=20064)\n* [域名解析](/product/dns?from=20064&from_column=20064)\n* [云存储](/product/cos?from=20064&from_column=20064)\n* [视频直播](/product/css?from=20064&from_column=20064)\n\n### 热门推荐\n\n* [人脸识别](/product/facerecognition?from=20064&from_column=20064)\n* [腾讯会议](/product/tm?from=20064&from_column=20064)\n* [企业云](/act/pro/enterprise2022?from=20064&from_column=20064)\n* [CDN加速](/product/cdn?from=20064&from_column=20064)\n* [视频通话](/product/trtc?from=20064&from_column=20064)\n* [图像分析](/product/imagerecognition?from=20064&from_column=20064)\n* [MySQL 数据库](/product/cdb?from=20064&from_column=20064)\n* [SSL 证书](/product/ssl?from=20064&from_column=20064)\n* [语音识别](/product/asr?from=20064&from_column=20064)\n\n### 更多推荐\n\n* [数据安全](/solution/data_protection?from=20064&from_column=20064)\n* [负载均衡](/product/clb?from=20064&from_column=20064)\n* [短信](/product/sms?from=20064&from_column=20064)\n* [文字识别](/product/ocr?from=20064&from_column=20064)\n* [云点播](/product/vod?from=20064&from_column=20064)\n* [大数据](/product/bigdata-class?from=20064&from_column=20064)\n* [小程序开发](/solution/la?from=20064&from_column=20064)\n* [网站监控](/product/tcop?from=20064&from_column=20064)\n* [数据迁移](/product/cdm?from=20064&from_column=20064)\n\nCopyright © 2013 - 2026Tencent Cloud. All Rights Reserved. 腾讯云 版权所有\n\n[深圳市腾讯计算机系统有限公司](https://qcloudimg.tencent-cloud.cn/raw/986376a919726e0c35e96b311f54184d.jpg)\xa0ICP备案/许可证号：[粤B2-20090059](https://beian.miit.gov.cn/#/Integrated/index)[粤公网安备44030502008569号](https://beian.mps.gov.cn/#/query/webSearch?code=44030502008569)\n\n[腾讯云计算（北京）有限责任公司](https://qcloudimg.tencent-cloud.cn/raw/a2390663ee4a95ceeead8fdc34d4b207.jpg) 京ICP证150476号 | \xa0[京ICP备11018762号](https://beian.miit.gov.cn/#/Integrated/index)\n\n[问题归档](/developer/ask/archives.html)[专栏文章](/developer/column/archives.html)[快讯文章归档](/developer/news/archives.html)[关键词归档](/developer/information/all.html)[开发者手册归档](/developer/devdocs/archives.html)[开发者手册 Section 归档](/developer/devdocs/sections_p1.html)\n\nCopyright © 2013 - 2026Tencent Cloud.\n\nAll Rights Reserved. 腾讯云 版权所有\n\n登录 后参与评论', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://cloud.tencent.com/developer/article/2588385', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.8591094, 'saved_path': None}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:05,162 - __main__ - INFO - handle_download: searcher=TavilySearch, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:05,162 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:05,162 - __main__ - INFO - call_tool payload: source_tool=tavily_download, result_type=papers, count=1
2026-02-22 20:38:05,163 - __main__ - INFO - call_tool: name=tavily_download, result_type=papers, count=1
2026-02-22 20:38:05,163 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': 'LangGraph 记忆系统实战：反馈循环+ 动态Prompt 让AI 持续学习', 'authors': [], 'abstract': '## LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\n[*腾讯云*](/?from=20060&from_column=20060)\n[*开发者社区*](/developer)\n\n[文档](/document/product?from=20702&from_column=20702)[建议反馈](/voc/?from=20703&from_column=20703)[控制台](https://console.cloud.tencent.com?from=20063&from_column=20063)\n\n[首页](/developer)\n\n文章/答案/技术大牛\n\ndeephub\n\n[社区首页](/developer) >[专栏](/developer/column) >LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\n# LangGraph 记忆系统实战：反馈循环 + 动态 Prompt 让 AI 持续学习\n\ndeephub\n\n发布于 2025-11-15 11:44:45\n\n发布于 2025-11-15 11:44:45\n\n8610\n\n文章被收录于专栏：[DeepHub IMBA](/developer/column/86944)DeepHub IMBA\n\n**点击上方“Deephub Imba”,关注公众号,好文章不错过 !**\n\n代理系统或者 RAG 方案，基本都需要一个双层记忆架构，这样 LLM 既能保持对当前上下文的专注，又能记住过往交互的内容。\n\n短期记忆负责单个会话内的即时信息管理，长期记忆则跨会话存储知识，让系统能够持续学习和进化。两者配合，代理才能表现出连贯性、上下文感知能力，看起来更加智能。这些记忆组件在现代 AI 架构中的位置如下图所示：\n\n#### 线程级记忆（短期）\n\n可以把它理解为代理的"工作记忆"。它帮助代理理解上下文，自然地延续讨论，不会丢失之前的步骤。LangGraph 通过检查点机制自动管理这部分记忆。对话结束后，短期记忆会被清空，下次会话则会重新开始一个新的记忆。\n\n#### 跨线程记忆（长期）\n\nLangGraph 将这些数据以 JSON 文档形式保存在记忆存储中，通过命名空间（类似文件夹）和键（类似文件名）整齐组织。因为这种记忆在对话后不会消失，所以代理能够随时间积累知识，提供更一致、更个性化的响应。\n\n> 本文会探讨生产级 AI 系统如何使用 LangGraph 管理长期记忆流。LangGraph 是一个构建可扩展、上下文感知 AI 工作流的主流框架。\n\n### LangGraph 数据持久层\n\n处理代理记忆时，LangGraph 是最常用的组件。其中 Store 功能尤为关键，它根据项目的运行位置管理记忆的保存、检索和更新方式。\n\nLangGraph 提供了几种存储实现，在简单性、持久性和可扩展性之间取得平衡。每种选项适合开发或部署的特定阶段。\n\n#### InMemory Store（用于 notebook 和快速测试）\n\n使用 `from langgraph.store.memory import InMemoryStore` 导入，创建一个完全在内存中运行的存储，使用标准 Python 字典。\n\n不写入磁盘，进程结束后所有信息都会丢失。但速度快，易用，非常适合测试工作流或尝试新的图配置。如果需要，也可以添加语义搜索能力。\n\n#### 本地开发存储（langgraph dev）\n\n这个选项的行为与上面的内存版本类似，但是可以在会话之间提供了基本持久性。\n\n用 `langgraph dev` 命令运行应用时，LangGraph 会自动使用 Python 的 pickle 格式将存储保存到本地文件系统，并在重启开发环境后恢复数据。\n\n这个方式轻量且方便，不需要外部数据库。同样支持语义搜索功能，所以它非常适合开发阶段，但不适合生产环境。\n\n#### 生产存储（LangGraph Platform 或自托管）\n\n大规模或生产部署，LangGraph 使用与 pgvector 集成的 PostgreSQL 数据库实现高效的向量存储和语义检索。\n\n这样可以提供完整的数据持久性、内置可靠性，并且能够处理更大的工作负载或多用户系统。语义搜索依靠pgvector ，默认使用余弦相似度作为相似性度量，也可以根据需求自定义。\n\n这种配置确保记忆数据安全存储，跨会话保持可用，即使在高流量或分布式工作负载下也能稳定运行。\n\n> 基础知识介绍完毕，接下来开始逐步编写完整的工作架构代码。\n\n### InMemory 功能实践\n\n本文要实现的是 InMemory 功能，这是基于 AI 系统中最常用的内存管理方式。\n\n> 它按顺序执行，在逐步构建或测试技术流程时非常实用。\n\nInMemory 功能允许在运行代码时临时存储数据，通过了解它可以有助于我们理解 LangGraph 中内存处理的工作原理。\n\n从 LangGraph 导入 `InMemoryStore` 开始。这个类让我们直接在内存中存储记忆，不需要外部数据库或文件系统。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the InMemoryStore class for storing memories in memory (no persistence) from langgraph.store.memory import InMemoryStore # Initialize an in-memory store instance for use in this notebook in_memory_store = InMemoryStore()\n```\n\n这里创建了 InMemoryStore 的实例，用于保存临时数据。因为只在内存中运行，进程停止后所有存储的数据都会被清除。\n\n> LangGraph 中的每个记忆都保存在命名空间（namespace）中。\n\n命名空间像标签或文件夹，帮助组织记忆。它被定义为元组，可以有一个或多个部分。下面这个例子使用包含用户 ID 和 "memories" 标签的元组。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a user ID for memory storage user_id = "1" # Set the namespace for storing and retrieving memories namespace_for_memory = (user_id, "memories")\n```\n\n命名空间可以代表任何东西，不一定基于用户 ID，所以可以根据应用结构自由分组记忆。\n\n下面我们保存一个记忆到存储中，使用 `put` 方法。这个方法需要三样东西：命名空间、唯一键和实际的记忆值。\n\n这里键是用 `uuid` 库生成的唯一标识符，记忆值是存储信息的字典——一个简单的偏好设置。\n\n代码语言：javascript\n\n复制\n\n```\n import uuid # Generate a unique ID for the memory memory_id = str(uuid.uuid4()) # Create a memory dictionary memory = {"food_preference": "I like pizza"} # Save the memory in the defined namespace in_memory_store.put(namespace_for_memory, memory_id, memory)\n```\n\n这会将记忆条目添加到之前定义的命名空间下的内存存储中。\n\n存储记忆后，可以用 `search` 方法取回。这个方法在命名空间内查找并返回属于它的所有记忆列表。\n\n每个记忆都是一个 `Item` 对象，包含命名空间、键、值和时间戳等详细信息。可以转换为字典以便更清晰地查看数据。\n\n代码语言：javascript\n\n复制\n\n```\n # Retrieve all stored memories for the given namespace memories = in_memory_store.search(namespace_for_memory) # View the latest memory memories[-1].dict()\n```\n\n在 notebook 中运行这段代码，得到以下输出：\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ###### { \'namespace\': [\'1\', \'memories\'], \'key\': \'c8619cd4-3d3f-4108-857c-5c8c12f39e87\', \'value\': {\'food_preference\': \'I like pizza\'}, \'created_at\': \'2025-10-08T15:46:16.531625+00:00\', \'updated_at\': \'2025-10-08T15:46:16.531625+00:00\', \'score\': None }\n```\n\n输出显示了存储的记忆详情。最重要的是 **value** 字段，包含实际保存的信息。其他字段帮助识别和管理记忆创建的时间和位置。\n\n存储就绪后，可以将其连接到图中，让记忆和检查点协同工作。这里使用两个主要组件：\n\n* **InMemorySaver** 管理线程间的检查点\n* **InMemoryStore** 存储跨线程的记忆\n\n代码语言：javascript\n\n复制\n\n```\n # To enable threads (conversations) from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() # To enable across-thread memory from langgraph.store.memory import InMemoryStore in_memory_store = InMemoryStore() # Compile the graph with the checkpointer and store # graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n```\n\n这使图能够记住线程内的对话上下文（短期），并使用相同的内存机制在线程间保留重要信息（长期）。\n\n> 这是转向生产级存储之前测试记忆行为的简单有效方式。\n\n### 构建代理架构\n\n在使用记忆系统工作流之前，需要构建使用它的智能代理。因为本文专注于记忆管理，所以只会构建一个中等复杂的电子邮件助手，模拟在真实场景中探索记忆的工作方式。\n\n下面我们从零开始构建这个系统，定义数据结构、"大脑"（提示词）和能力（工具）。最终得到一个不仅能回复邮件，还能从反馈中学习的代理。\n\n#### 定义模式\n\n处理数据前需要定义其形状。模式是代理信息流的蓝图，确保一切结构化、可预测且类型安全。\n\n首先编写 `RouterSchema`。需要它是为了让初始分类步骤可靠。不能冒险让 LLM 在期望明确决定时返回非结构化文本。\n\n这个 Pydantic 模型会强制 LLM 返回一个干净的 JSON 对象，包含推理过程和一个严格为 \'ignore\'、\'respond\' 或 \'notify\' 之一的分类结果。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the necessary libraries from Pydantic and Python\'s typing module from pydantic import BaseModel, Field from typing_extensions import TypedDict, Literal # Define a Pydantic model for our router\'s structured output. class RouterSchema(BaseModel): """Analyze the unread email and route it according to its content.""" # Add a field for the LLM to explain its step-by-step reasoning. reasoning: str = Field(description="分类背后的逐步推理。") # Add a field to hold the final classification. # The `Literal` type restricts the output to one of these three specific strings. classification: Literal["ignore", "respond", "notify"] = Field( description="电子邮件的分类。" )\n```\n\n这是在为分类 LLM 创建契约。后面与 LangChain 的 `.with_structured_output()` 方法配对时，能保证输出是一个可预测的 Python 对象，让图中的逻辑更加健壮。\n\n接下来需要一个地方存储代理单次运行的所有信息，这就是 `State` 的作用。它像一个中央白板，图的每个部分都可以读写。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the base state class from LangGraph from langgraph.graph import MessagesState # Define the central state object for our graph. class State(MessagesState): # This field will hold the initial raw email data. email_input: dict # This field will store the decision made by our triage router. classification_decision: Literal["ignore", "respond", "notify"]\n```\n\n继承自 LangGraph 的 `MessagesState`，自动获得一个 `messages` 列表来跟踪对话历史。然后添加自定义字段。随着流程从一个节点移动到另一个节点，这个 `State` 对象会被传递并累积信息。\n\n最后定义一个小但重要的 `StateInput` 模式，规定图的初始输入应该是什么样子。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a TypedDict for the initial input to our entire workflow. class StateInput(TypedDict): # The workflow must be started with a dictionary containing an \'email_input\' key. email_input: dict\n```\n\n这个简单的模式从应用入口点就提供了清晰性和类型安全性，确保对图的任何调用都以正确的数据结构开始。\n\n#### 创建提示词\n\n使用提示词方法来指导和引导 LLM 行为。对于代理，会定义几个提示词，每个都有特定的任务。\n\n代理从我们这里学到任何东西之前，需要一套基准指令。这些默认字符串会在第一次运行时加载到记忆存储中，为代理行为提供起点。\n\n首先定义 `default_background` 给代理一个角色。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a default persona for the agent. default_background=""" I\'m Lance, a software engineer at LangChain. """\n```\n\n接下来是 `default_triage_instructions`，分类路由器遵循的初始规则。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the initial rules for the triage LLM. default_triage_instructions = """ Emails that are not worth responding to: - Marketing newsletters and promotional emails - Spam or suspicious emails - CC\'d on FYI threads with no direct questions Emails that require notification but no response: - Team member out sick or on vacation - Build system notifications or deployments Emails that require a response: - Direct questions from team members - Meeting requests requiring confirmation """\n```\n\n然后是 `default_response_preferences`，定义代理的初始写作风格。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the default preferences for how the agent should compose emails. default_response_preferences = """ Use professional and concise language. If the e-mail mentions a deadline, make sure to explicitly acknowledge and reference the deadline in your response. When responding to meeting scheduling requests: - If times are proposed, verify calendar availability and commit to one. - If no times are proposed, check your calendar and propose multiple options. """\n```\n\n最后是 `default_cal_preferences`，指导日程安排行为。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the default preferences for scheduling meetings. default_cal_preferences = """ 30 minute meetings are preferred, but 15 minute meetings are also acceptable. """\n```\n\n现在创建使用这些默认值的提示词，首先是 `triage_system_prompt`。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the system prompt for the initial triage step. triage_system_prompt = """ < Role > Your role is to triage incoming emails based on background and instructions.  Role > < Background > {background}  Background > < Instructions > Categorize each email into IGNORE, NOTIFY, or RESPOND.  Instructions > < Rules > {triage_instructions}  Rules > """\n```\n\n这个提示词模板给分类路由器提供角色和指令。`{background}` 和 `{triage_instructions}` 占位符会被刚才定义的默认字符串填充。\n\n接着是 `triage_user_prompt`，一个简单的模板，用于将原始邮件内容构造成 LLM 易于解析的清晰格式。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the user prompt for triage, which will format the raw email. triage_user_prompt = """ Please determine how to handle the following email: From: {author} To: {to} Subject: {subject} {email_thread}"""\n```\n\n现在是主要组件，必须创建 `agent_system_prompt_hitl_memory`，它包含到目前为止编码的角色和其他指令。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the datetime library to include the current date in the prompt. from datetime import datetime # Define the main system prompt for the response agent. agent_system_prompt_hitl_memory = """ < Role > You are a top-notch executive assistant.  Role > < Tools > You have access to the following tools: {tools_prompt}  Tools > < Instructions > 1. Analyze the email content carefully. 2. Always call one tool at a time until the task is complete. 3. Use Question to ask the user for clarification. 4. Draft emails using write_email. 5. For meetings, check availability and schedule accordingly. - Today\'s date is """ + datetime.now().strftime("%Y-%m-%d") + """ 6. After sending emails, use the Done tool.  Instructions > < Background > {background}  Background > < Response Preferences > {response_preferences}  Response Preferences > < Calendar Preferences > {cal_preferences}  Calendar Preferences > """\n```\n\n这是主响应代理的主指令集。像 `{response_preferences}` 和 `{cal_preferences}` 这样的占位符是记忆系统的关键。\n\n> 它们允许动态地从记忆存储中注入代理学到的知识，使其能够随时间调整行为。\n\n为了让代理改进，为专门的"记忆管理器"LLM 定义特殊提示词。它唯一的工作就是安全、智能地更新记忆存储。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the system prompt for our specialized memory update manager LLM. MEMORY_UPDATE_INSTRUCTIONS = """ # Role You are a memory profile manager for an email assistant. # Rules - NEVER overwrite the entire profile - ONLY add new information - ONLY update facts contradicted by feedback - PRESERVE all other information # Reasoning Steps 1. Analyze the current memory profile. 2. Review feedback messages. 3. Extract relevant preferences. 4. Compare to existing profile. 5. Identify facts to update. 6. Preserve everything else. 7. Output updated profile. # Process current profile for {namespace}  {current_profile}  """\n```\n\n`MEMORY_UPDATE_INSTRUCTIONS` 提示词高度结构化，规则严格：永不覆盖、只做针对性添加、保留现有信息。这种方法对防止代理记忆被破坏至关重要。\n\n代码语言：javascript\n\n复制\n\n```\n # Define a reinforcement prompt to remind the LLM of the most critical rules. MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT = """ Remember: - NEVER overwrite the entire profile - ONLY make targeted additions - ONLY update specific facts contradicted by feedback - PRESERVE all other information """\n```\n\n`MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT` 是现代提示词工程技术。它是要求 LLM 更新记忆时会附加到消息中的最关键规则的简明摘要。重复关键指令有助于确保 LLM 遵守它们。\n\n#### 定义工具和实用函数\n\n代理有了指令，现在需要赋予它采取行动的能力。会定义作为工具的 Python 函数，以及一些辅助实用函数，保持主代码整洁有序。\n\n编写实际工具函数之前，需要一个关于它们的简单文本描述。这是代理在主提示词中会"看到"的内容，让它理解有哪些工具可用以及如何使用。\n\n代码语言：javascript\n\n复制\n\n```\n # A simple string describing the available tools for the LLM. HITL_MEMORY_TOOLS_PROMPT = """ 1. write_email(to, subject, content) - Send emails to specified recipients 2. schedule_meeting(attendees, subject, duration_minutes, preferred_day, start_time) - Schedule calendar meetings 3. check_calendar_availability(day) - Check available time slots 4. Question(content) - Ask follow-up questions 5. Done - Mark the email as sent """\n```\n\n这个字符串本身不是可执行代码，而是作为 LLM 的文档。它会被插入到主 `agent_system_prompt_hitl_memory` 中的 `{tools_prompt}` 占位符。这样代理就知道 `write_email` 函数存在，需要 `to`、`subject` 和 `content` 参数。\n\n好的项目都有一个 `utils.py` 文件来存放执行常见、重复性任务的辅助函数，保持主图逻辑整洁，专注于工作流本身。\n\n首先需要一个函数来解析初始邮件输入。\n\n代码语言：javascript\n\n复制\n\n```\n # This utility unpacks the email input dictionary for easier access. def parse_email(email_input: dict) -> tuple[str, str, str, str]: """Parse an email input dictionary into its constituent parts.""" # Return a tuple containing the author, recipient, subject, and body of the email. return ( email_input["author"], email_input["to"], email_input["subject"], email_input["email_thread"], )\n```\n\n`parse_email` 函数是输入字典的简单解包器。虽然可以在图节点中直接访问 `email_input["author"]`，但这个辅助函数让代码更可读，并集中了解析逻辑。\n\n接下来，一个将邮件内容格式化为 Markdown 供 LLM 使用的函数。\n\n代码语言：javascript\n\n复制\n\n```\n # This function formats the raw email data into clean markdown for the LLM. def format_email_markdown(subject, author, to, email_thread): """Format email details into a nicely formatted markdown string.""" # Use f-string formatting to create a structured string with clear labels. return f""" **Subject**: {subject} **From**: {author} **To**: {to} {email_thread} --- """\n```\n\n`format_email_markdown` 函数接收解析后的邮件部分，将它们排列成干净的 Markdown 格式块。这种结构化格式比原始的非结构化字符串更容易被 LLM 解析，帮助它更好地理解邮件的不同组成部分（发件人、主题、正文）。\n\n最后需要一个函数为人工审阅者格式化代理建议的操作。\n\n代码语言：javascript\n\n复制\n\n```\n # This function creates a human-friendly view of a tool call for the HITL interface. def format_for_display(tool_call: dict) -> str: """Format a tool call into a readable string for the user.""" # Initialize an empty string to build our display. display = "" # Use conditional logic to create custom, readable formats for our main tools. if tool_call["name"] == "write_email": display += f\'# Email Draft\\n\\n**To**: {tool_call["args"].get("to")}\\n**Subject**: {tool_call["args"].get("subject")}\\n\\n{tool_call["args"].get("content")}\' elif tool_call["name"] == "schedule_meeting": display += f\'# Calendar Invite\\n\\n**Meeting**: {tool_call["args"].get("subject")}\\n**Attendees**: {", ".join(tool_call["args"].get("attendees"))}\' elif tool_call["name"] == "Question": display += f\'# Question for User\\n\\n{tool_call["args"].get("content")}\' # Provide a generic fallback for any other tools. else: display += f\'# Tool Call: {tool_call["name"]}\\n\\nArguments:\\n{tool_call["args"]}\' # Return the final formatted string. return display\n```\n\n这个 `format_for_display` 函数对人机协作（HITL）步骤很重要。当代理提出工具调用如 `write_email` 时，不想向人工审阅者展示原始 JSON 对象。\n\n这个函数将技术表示转换为看起来像实际邮件草稿或日历邀请的内容，让用户更容易审阅、编辑或批准。\n\n模式、提示词和实用函数都定义好了，现在可以将它们组装成完整的图，让学习代理运转起来。\n\n### 记忆功能与图节点\n\n从这里开始我们就要实现记忆逻辑了，先看看整个系统如何运作。\n\n代理从静态指令集转变为能够学习的动态系统，关键就在这里。\n\n构建使用记忆的图节点之前，需要实际与 `InMemoryStore` 交互的函数。我们需要创建两个关键函数：一个获取现有偏好，另一个根据反馈更新它们。\n\n首先需要可靠的方式从存储中获取偏好。编写一个叫 `get_memory` 的函数，它会在存储中查找特定偏好（如 `"triage_preferences"`）。找到就返回存储的值。\n\n如果找不到——这会在用户第一次运行时发生——它会使用之前定义的默认内容创建条目。这确保代理始终有一套规则可以遵循。\n\n代码语言：javascript\n\n复制\n\n```\n # A function to retrieve memory from the store or initialize with defaults. def get_memory(store, namespace, default_content=None): """Get memory from the store or initialize with default if it doesn\'t exist.""" # Use the store\'s .get() method to search for an item with a specific key. user_preferences = store.get(namespace, "user_preferences") # If the item exists, return its value (the stored string). if user_preferences: return user_preferences.value # If the item does not exist, this is the first time we\'re accessing this memory. else: # Use the store\'s .put() method to create the memory item with default content. store.put(namespace, "user_preferences", default_content) # Return the default content to be used in this run. return default_content\n```\n\n这个简单的函数功能强大。它抽象了检查和初始化记忆的逻辑。图中的任何节点现在都可以调用 `get_memory` 获取最新的用户偏好，无需知道这是第一次运行还是第一百次。\n\n这里是代理学习被触发的地方。`update_memory` 函数设计用来接收用户反馈——比如编辑过的邮件或自然语言指令——并用它来精炼代理存储的知识。它使用之前精心制作的 `MEMORY_UPDATE_INSTRUCTIONS` 提示词来协调一个特殊用途的 LLM 调用。\n\n为确保 LLM 输出可预测，先定义一个 `UserPreferences` Pydantic 模式。这会强制记忆管理器 LLM 返回一个 JSON 对象，包含推理过程和最终更新的偏好字符串。\n\n代码语言：javascript\n\n复制\n\n```\n # A Pydantic model to structure the output of our memory update LLM call. class UserPreferences(BaseModel): """Updated user preferences based on user\'s feedback.""" # A field for the LLM to explain its reasoning, useful for debugging. chain_of_thought: str = Field(description="Reasoning about which user preferences need to add / update if required") # The final, updated string of user preferences. user_preferences: str = Field(description="Updated user preferences")\n```\n\n现在可以编写 `update_memory` 函数本身。它会检索当前偏好，将它们与用户反馈和特殊提示词结合，然后将 LLM 的精炼输出保存回存储。\n\n代码语言：javascript\n\n复制\n\n```\n # Import AIMessage to help filter messages before sending them to the memory updater. from langchain_core.messages import AIMessage # This function intelligently updates the memory store based on user feedback. def update_memory(store, namespace, messages): """Update memory profile in the store.""" # First, get the current memory from the store so we can provide it as context. user_preferences = store.get(namespace, "user_preferences") # Initialize a new LLM instance specifically for this task, configured for structured output. memory_updater_llm = llm.with_structured_output(UserPreferences) # This is a small but important fix: filter out any previous AI messages with tool calls. # Passing these complex objects can sometimes cause errors in the downstream LLM call. messages_to_send = [ msg for msg in messages if not (isinstance(msg, AIMessage) and hasattr(msg, \'tool_calls\') and msg.tool_calls) ] # Invoke the LLM with the memory prompt, current preferences, and the user\'s feedback. result = memory_updater_llm.invoke( [ # The system prompt that instructs the LLM on how to update memory. {"role": "system", "content": MEMORY_UPDATE_INSTRUCTIONS.format(current_profile=user_preferences.value, namespace=namespace)}, ] # Append the filtered conversation messages containing the feedback. + messages_to_send ) # Save the newly generated preference string back into the store, overwriting the old one. store.put(namespace, "user_preferences", result.user_preferences)\n```\n\n这个函数是代理学习能力的主要组成部分。通过使用带有严格指令的专用 LLM 调用，确保记忆以受控和增量方式更新，让代理随时间逐渐与用户偏好对齐。\n\n现在可以定义代理的核心逻辑。在 LangGraph 中，这个逻辑封装在节点中。每个节点都是一个 Python 函数，接收图的当前 `State`，执行操作，返回对该状态的更新。\n\n> 电子邮件助手会有几个关键节点，处理从初始分类到生成最终响应的所有事情。\n\n工作流中的第一个节点是 `triage_router`。这个函数的任务是对收到的邮件做初步决定……\n\n> 应该回复、只通知用户，还是完全忽略？长期记忆首次发挥作用就在这里。\n\n路由器会使用 `get_memory` 函数获取用户最新的 `triage_preferences` 并注入到提示词中，确保决策能力随时间提高。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the Command class for routing and BaseStore for type hinting from langgraph.types import Command from langgraph.store.base import BaseStore # Define the first node in our graph, the triage router. def triage_router(state: State, store: BaseStore) -> Command: """Analyze email content to decide the next step.""" # Unpack the raw email data using our utility function. author, to, subject, email_thread = parse_email(state["email_input"]) # Format the email content into a clean string for the LLM. email_markdown = format_email_markdown(subject, author, to, email_thread) # Here is the memory integration: fetch the latest triage instructions. # If they don\'t exist, it will use the `default_triage_instructions`. triage_instructions = get_memory(store, ("email_assistant", "triage_preferences"), default_triage_instructions) # Format the system prompt, injecting the retrieved triage instructions. system_prompt = triage_system_prompt.format( background=default_background, triage_instructions=triage_instructions, ) # Format the user prompt with the specific details of the current email. user_prompt = triage_user_prompt.format( author=author, to=to, subject=subject, email_thread=email_thread ) # Invoke the LLM router, which is configured to return our `RouterSchema`. result = llm_router.invoke( [ {"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}, ] ) # Based on the LLM\'s classification, decide which node to go to next. if result.classification == "respond": print("📧 Classification: RESPOND - This email requires a response") # Set the next node to be the \'response_agent\'. goto = "response_agent" # Update the state with the decision and the formatted email for the agent. update = { "classification_decision": result.classification, "messages": [{"role": "user", "content": f"Respond to the email: {email_markdown}"}], } elif result.classification == "ignore": print("🚫 Classification: IGNORE - This email can be safely ignored") # End the workflow immediately. goto = END # Update the state with the classification decision. update = {"classification_decision": result.classification} elif result.classification == "notify": print("🔔 Classification: NOTIFY - This email contains important information") # Go to the human-in-the-loop handler for notification. goto = "triage_interrupt_handler" # Update the state with the classification decision. update = {"classification_decision": result.classification} else: # Raise an error if the classification is invalid. raise ValueError(f"Invalid classification: {result.classification}") # Return a Command object to tell LangGraph where to go next and what to update. return Command(goto=goto, update=update)\n```\n\n这个节点是整个系统的入口。通过添加一行 `triage_instructions = get_memory(...)`，就把它从静态路由器变成了能学习的路由器。当用户对分类决策提供反馈时，存储中的 `triage_preferences` 会被更新，这个节点就会自动开始在未来的邮件上做出更好、更个性化的分类。\n\n邮件被分类为 "respond" 时，会被传递给主响应代理。该代理的核心是 `llm_call` 节点。这个函数的目的是获取当前对话历史并采取下一步行动，通常是决定调用哪个工具。\n\n跟分类路由器一样，这个节点集成了记忆来指导决策。它获取 `response_preferences` 和 `cal_preferences`，确保行为与用户学到的风格保持一致。\n\n代码语言：javascript\n\n复制\n\n```\n # This is the primary reasoning node for the response agent. def llm_call(state: State, store: BaseStore): """LLM decides whether to call a tool or not, using stored preferences.""" # Fetch the user\'s latest calendar preferences from the memory store. cal_preferences = get_memory(store, ("email_assistant", "cal_preferences"), default_cal_preferences) # Fetch the user\'s latest response (writing style) preferences. response_preferences = get_memory(store, ("email_assistant", "response_preferences"), default_response_preferences) # Filter out previous AI messages with tool calls to prevent API errors. messages_to_send = [ msg for msg in state["messages"] if not (isinstance(msg, AIMessage) and hasattr(msg, \'tool_calls\') and msg.tool_calls) ] # Invoke the main LLM, which is bound to our set of tools. # The prompt is formatted with the preferences retrieved from memory. response = llm_with_tools.invoke( [ {"role": "system", "content": agent_system_prompt_hitl_memory.format( tools_prompt=HITL_MEMORY_TOOLS_PROMPT, background=default_background, response_preferences=response_preferences, cal_preferences=cal_preferences )} ] + messages_to_send ) # Return the LLM\'s response to be added to the state. return {"messages": [response]}\n```\n\n这个节点展示了长期记忆的重要性。每次执行时都会提取用户最新的写作风格和日历偏好。\n\n当用户提供反馈说更喜欢较短的邮件或 30 分钟的会议时，`update_memory` 函数会修改存储。下次这个 `llm_call` 节点运行时，会自动获取那些新偏好并注入到提示词中，立即改变代理行为，无需任何代码更改。\n\n这就创建了一个反馈循环，代理在其中不断适应用户。\n\n### 通过人机协作捕获反馈\n\n代理不仅仅是在行动，还得知道何时请求帮助或确认。接下来要构建的节点集是中断处理器。\n\n这些特殊节点会暂停图的执行并等待人类输入。神奇的地方就发生在这里：在这些步骤中提供的反馈会被捕获并用于更新代理的长期记忆。\n\n会有两个中断点：\n\n一个在初始分类之后（用于 `notify` 分类），另一个更复杂，用于审查代理建议的工具调用。\n\n首先构建 `triage_interrupt_handler`。当 `triage_router` 将邮件分类为 `notify` 时，这个节点被触发。代理不会对邮件采取行动，而是呈现给用户并请求决策：\n\n> 应该忽略，还是实际回复？用户在这里的选择是关于分类偏好的宝贵反馈。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the `interrupt` function from LangGraph. from langgraph.types import interrupt # Define the interrupt handler for the triage step. def triage_interrupt_handler(state: State, store: BaseStore) -> Command: """Handles interrupts from the triage step, pausing for user input.""" # Parse the email input to format it for display. author, to, subject, email_thread = parse_email(state["email_input"]) email_markdown = format_email_markdown(subject, author, to, email_thread) # This is the data structure that defines the interrupt. # It specifies the action, the allowed user responses, and the content to display. request = { "action_request": { "action": f"Email Assistant: {state[\'classification_decision\']}", "args": {} }, "config": { "allow_ignore": True, "allow_respond": True }, "description": email_markdown, } # The `interrupt()` function pauses the graph and sends the request to the user. # It waits here until it receives a response. response = interrupt([request])[0] # Now, we process the user\'s response. if response["type"] == "response": # The user decided to respond, overriding the \'notify\' classification. user_input = response["args"] # We create a message to pass to the memory updater. messages = [{"role": "user", "content": f"The user decided to respond to the email, so update the triage preferences to capture this."}] # This is a key step: we call `update_memory` to teach the agent. update_memory(store, ("email_assistant", "triage_preferences"), messages) # Prepare to route to the main response agent. goto = "response_agent" # Update the state with the user\'s feedback. update = {"messages": [{"role": "user", "content": f"User wants to reply. Use this feedback: {user_input}"}]} elif response["type"] == "ignore": # The user confirmed the email should be ignored. messages = [{"role": "user", "content": f"The user decided to ignore the email even though it was classified as notify. Update triage preferences to capture this."}] # We still update memory to reinforce this preference. update_memory(store, ("email_assistant", "triage_preferences"), messages) # End the workflow. goto = END update = {} # No message update needed. else: raise ValueError(f"Invalid response: {response}") # Return a Command to direct the graph\'s next step. return Command(goto=goto, update=update)\n```\n\n如果代理认为一封邮件只是通知，但用户决定回复，`update_memory` 就会被调用。记忆管理器 LLM 会看到消息"用户决定回复..."并分析邮件内容。\n\n然后它会精确地更新 `triage_preferences` 字符串，也许通过将"构建系统通知"从 `NOTIFY` 类别移动到 `RESPOND` 类别。下次类似邮件到达时，`triage_router` 会做出更好、更个性化的决定。\n\n但还需要一个主中断处理器，这是图中最复杂的节点。`llm_call` 节点提出要使用的工具（如 `write_email` 或 `schedule_meeting`）后，这个 `interrupt_handler` 就会介入。它将代理建议的操作呈现给用户审查。\n\n然后用户可以接受、忽略、提供自然语言反馈或直接编辑。每种选择都为记忆系统提供了不同且有价值的信号。\n\n代码语言：javascript\n\n复制\n\n```\n # The main interrupt handler for reviewing tool calls. def interrupt_handler(state: State, store: BaseStore) -> Command: """Creates an interrupt for human review of tool calls and updates memory.""" # We\'ll build up a list of new messages to add to the state. result = [] # By default, we\'ll loop back to the LLM after this. goto = "llm_call" # The agent can propose multiple tool calls, so we loop through them. for tool_call in state["messages"][-1].tool_calls: # We only want to interrupt for certain "high-stakes" tools. hitl_tools = ["write_email", "schedule_meeting", "Question"] if tool_call["name"] not in hitl_tools: # For other tools (like check_calendar), execute them without interruption. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(tool_call["args"]) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) continue # Format the proposed action for display to the human reviewer. tool_display = format_for_display(tool_call) # Define the interrupt request payload. request = { "action_request": {"action": tool_call["name"], "args": tool_call["args"]}, "config": { "allow_ignore": True, "allow_respond": True, "allow_edit": True, "allow_accept": True }, "description": tool_display, } # Pause the graph and wait for the user\'s response. response = interrupt([request])[0] # --- MEMORY UPDATE LOGIC BASED ON USER RESPONSE --- if response["type"] == "edit": # The user directly edited the agent\'s proposed action. initial_tool_call = tool_call["args"] edited_args = response["args"]["args"] # This is the most direct form of feedback. We call `update_memory`. if tool_call["name"] == "write_email": update_memory(store, ("email_assistant", "response_preferences"), [{"role": "user", "content": f"User edited the email. Initial draft: {initial_tool_call}. Edited draft: {edited_args}."}]) elif tool_call["name"] == "schedule_meeting": update_memory(store, ("email_assistant", "cal_preferences"), [{"role": "user", "content": f"User edited the meeting. Initial invite: {initial_tool_call}. Edited invite: {edited_args}."}]) # Execute the tool with the user\'s edited arguments. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(edited_args) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) elif response["type"] == "response": # The user gave natural language feedback. user_feedback = response["args"] # We capture this feedback and use it to update memory. if tool_call["name"] == "write_email": update_memory(store, ("email_assistant", "response_preferences"), [{"role": "user", "content": f"User gave feedback on the email draft: {user_feedback}"}]) elif tool_call["name"] == "schedule_meeting": update_memory(store, ("email_assistant", "cal_preferences"), [{"role": "user", "content": f"User gave feedback on the meeting invite: {user_feedback}"}]) # We don\'t execute the tool. Instead, we pass the feedback back to the agent. result.append({"role": "tool", "content": f"User gave feedback: {user_feedback}", "tool_call_id": tool_call["id"]}) elif response["type"] == "ignore": # The user decided this action should not be taken. This is triage feedback. update_memory(store, ("email_assistant", "triage_preferences"), [{"role": "user", "content": f"User ignored the proposal to {tool_call[\'name\']}. This email should not have been classified as \'respond\'."}]) result.append({"role": "tool", "content": "User ignored this. End the workflow.", "tool_call_id": tool_call["id"]}) goto = END elif response["type"] == "accept": # The user approved the action. No memory update is needed. tool = tools_by_name[tool_call["name"]] observation = tool.invoke(tool_call["args"]) result.append({"role": "tool", "content": observation, "tool_call_id": tool_call["id"]}) # Return a command with the next node and the messages to add to the state. return Command(goto=goto, update={"messages": result})\n```\n\n这个节点是学习系统的核心。注意到没有，每种类型的用户反馈——编辑、响应和忽略——都会触发对 `update_memory` 的调用，并带有特定的、上下文相关的消息。\n\n当用户将会议时长从 45 分钟编辑为 30 分钟时，记忆管理器 LLM 会看到这个明确信号，并更新 `cal_preferences` 以在未来偏好 30 分钟的会议。当用户说"让它不那么正式"时，LLM 会对此进行概括，并在 `response_preferences` 中添加新规则。这种持续、细粒度的反馈循环让代理能够随时间成为高度个性化的助手。\n\n### 组装成工作流\n\n代理的所有单独组件都构建好了：模式、提示词、工具、实用函数和图节点。现在用 LangGraph 将它们组装成功能性的状态机。这涉及定义图结构、添加节点以及指定连接它们的边。\n\n主 `llm_call` 节点运行后，代理会提出一个或多个工具调用。需要一种方法来决定接下来会发生什么。代理应该停止，还是应该进入人工审查步骤？这由条件边来处理。它是一个简单的函数，检查状态中的最后一条消息并指导图的流程。\n\n代码语言：javascript\n\n复制\n\n```\n # This function determines the next step after the LLM has made its decision. def should_continue(state: State) -> Literal["interrupt_handler", END]: """Route to the interrupt handler or end the workflow if the \'Done\' tool is called.""" # Get the list of messages from the current state. messages = state["messages"] # Get the most recent message, which contains the agent\'s proposed action. last_message = messages[-1] # Check if the last message contains any tool calls. if last_message.tool_calls: # Loop through each proposed tool call. for tool_call in last_message.tool_calls: # If the agent has decided it\'s finished, we end the workflow. if tool_call["name"] == "Done": return END # For any other tool, we proceed to the human review step. else: return "interrupt_handler"\n```\n\n这个函数是响应代理的主路由器。它检查代理的决定并充当交通警察。如果调用了 `Done` 工具，通过返回 `END` 表示过程已完成。\n\n对于任何其他工具调用，它将图路由到 `interrupt_handler` 节点进行人工审查，确保在未经批准的情况下不采取任何行动。\n\n现在可以组装图，直观地看到它的样子。用 `StateGraph` 定义结构。该过程涉及两个主要阶段：\n\n构建 `response_agent` 子图——包含 `llm_call` -> `interrupt_handler` 的核心循环。构建 `overall_workflow`——这个主图从 `triage_router` 开始，并将 `response_agent` 子图作为其节点之一。\n\n这种方式保持架构清晰易懂。\n\n代码语言：javascript\n\n复制\n\n```\n # Import the main graph-building class from LangGraph. from langgraph.graph import StateGraph, START, END # --- Part 1: Build the Response Agent Subgraph --- # Initialize a new state graph with our defined `State` schema. agent_builder = StateGraph(State) # Add the \'llm_call\' node to the graph. agent_builder.add_node("llm_call", llm_call) # Add the \'interrupt_handler\' node to the graph. agent_builder.add_node("interrupt_handler", interrupt_handler) # Set the entry point of this subgraph to be the \'llm_call\' node. agent_builder.add_edge(START, "llm_call") # Add the conditional edge that routes from \'llm_call\' to either \'interrupt_handler\' or END. agent_builder.add_conditional_edges( "llm_call", should_continue, { "interrupt_handler": "interrupt_handler", END: END, }, ) # After the interrupt handler, the graph always loops back to the LLM to continue the task. agent_builder.add_edge("interrupt_handler", "llm_call") # Compile the subgraph into a runnable object. response_agent = agent_builder.compile() # --- Part 2: Build the Overall Workflow --- # Initialize the main graph, defining its input schema as `StateInput`. overall_workflow = ( StateGraph(State, input=StateInput) # Add the triage router as the first node. .add_node("triage_router", triage_router) # Add the triage interrupt handler node. .add_node("triage_interrupt_handler", triage_interrupt_handler) # Add our entire compiled `response_agent` subgraph as a single node. .add_node("response_agent", response_agent) # Set the entry point for the entire workflow. .add_edge(START, "triage_router") # Define the edges from the triage router to the appropriate next steps. .add_edge("triage_router", "response_agent") .add_edge("triage_router", "triage_interrupt_handler") .add_edge("triage_interrupt_handler", "response_agent") ) # Compile the final, complete graph. email_assistant = overall_workflow.compile()\n```\n\n`triage_router` 做初步决定，然后分支到结束流程、通过 `triage_interrupt_handler` 询问用户输入，或者将控制权交给 `response_agent`。\n\n`response_agent` 进入自己的思考（`llm_call`）和请求审查（`interrupt_handler`）循环，在此过程中更新记忆，直到任务完成。\n\n这种有状态架构使得 LangGraph 非常适合构建复杂的、能够学习的代理。现在可以拿着这个编译好的 `email_assistant` 开始测试它从反馈中学习的能力。\n\n### 使用记忆测试代理\n\n记忆功能已经实现到电子邮件助手中，接下来测试系统如何从用户反馈中学习并随时间适应。这部分测试会探讨不同类型的用户交互如何创建不同的记忆更新，从而提高助手未来的性能。\n\n主要解决一下的问题：\n\n* 系统如何捕获和持久化用户偏好？\n* 存储的偏好以何种方式影响后续决策过程？\n* 哪些用户交互模式会触发特定类型的记忆更新？\n\n首先构建一个辅助函数来显示记忆内容，以便在整个测试过程中跟踪其演变。\n\n代码语言：javascript\n\n复制\n\n```\n # Import necessary libraries for testing. import uuid from langgraph.checkpoint.memory import MemorySaver from langgraph.types import Command from langgraph.store.memory import InMemoryStore # Define a helper function to display the content of our memory store. def display_memory_content(store, namespace=None): """A utility to print the current state of the memory store.""" # Print a header for clarity. print("\\n======= CURRENT MEMORY CONTENT =======") # If a specific namespace is requested, show only that one. if namespace: # Retrieve the memory item for the specified namespace. memory = store.get(namespace, "user_preferences") print(f"\\n--- {namespace[1]} ---") if memory: print(memory.value) else: print("No memory found") # If no specific namespace is given, show all of them. else: # Define the list of all possible namespaces we are using. for ns in [ ("email_assistant", "triage_preferences"), ("email_assistant", "response_preferences"), ("email_assistant", "cal_preferences"), ("email_assistant", "background") ]: # Retrieve and print the memory content for each namespace. memory = store.get(ns, "user_preferences") print(f"\\n--- {ns[1]} ---") if memory: print(memory.value) else: print("No memory found") print("=======================================\\n")\n```\n\n这个实用程序提供了一个实时窗口，可以观察代理不断演变的知识库，能轻松看到每次交互后到底学到了什么。\n\n开始执行不同的测试用例。\n\n#### 测试用例 1：基线——接受提议\n\n第一个测试检查当用户未经修改地接受代理操作时会发生什么。这个基线案例帮助理解在没有提供反馈时系统的行为。期望代理使用记忆来做决定，但不会更新它。\n\n先设置一个新的测试运行。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the input email for our test case. email_input_respond = { "to": "Lance Martin ", "author": "Project Manager ", "subject": "Tax season let\'s schedule call", "email_thread": "Lance,\\n\\nIt\'s tax season again... Are you available sometime next week? ... for about 45 minutes." } # --- Setup for a new test run --- # Initialize a new checkpointer and a fresh, empty memory store. checkpointer = MemorySaver() store = InMemoryStore() # Compile our graph, connecting it to our new checkpointer and store. graph = overall_workflow.compile(checkpointer=checkpointer, store=store) # Create a unique ID and configuration for this conversation. thread_id_1 = uuid.uuid4() thread_config_1 = {"configurable": {"thread_id": thread_id_1}} # Run the graph until its first interrupt. print("Running the graph until the first interrupt...") for chunk in graph.stream({"email_input": email_input_respond}, config=thread_config_1): if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the memory state after the first interrupt. display_memory_content(store)\n```\n\n图会一直运行直到代理提出第一个动作并暂停供审查。\n\n代码语言：javascript\n\n复制\n\n```\n ####### OUTPUT ######### Running the graph until the first interrupt... 📧 Classification: RESPOND - This email requires a response INTERRUPT OBJECT: Action Request: {\'action\': \'schedule_meeting\', \'args\': {\'attendees\': [\'lance@company.com\', \'pm@client.com\'], \'subject\': \'Tax Planning Strategies\', \'duration_minutes\': 45, ...}} ======= CURRENT MEMORY CONTENT ======= --- triage_preferences --- Emails that are not worth responding to: ... --- response_preferences --- Use professional and concise language. ... --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. --- background --- No memory found =======================================\n```\n\n输出显示了两件关键的事。首先，代理正确提出了一个 45 分钟的 `schedule_meeting` 工具调用，尊重了发件人的请求，即使默认偏好是 30 分钟。\n\n其次，`display_memory_content` 函数确认所有记忆命名空间都已使用默认值初始化。尚未发生学习。\n\n现在接受代理的提议。\n\n代码语言：javascript\n\n复制\n\n```\n # Resume the graph by sending an \'accept\' command. print(f"\\nSimulating user accepting the {Interrupt_Object.value[0][\'action_request\'][\'action\']} tool call...") for chunk in graph.stream(Command(resume=[{"type": "accept"}]), config=thread_config_1): # Let the graph run until its next natural pause point. if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}")\n```\n\n代理执行会议工具并进入下一个逻辑步骤：起草一封确认邮件。然后再次中断供审查。\n\n代码语言：javascript\n\n复制\n\n```\n Simulating user accepting the schedule_meeting tool call... INTERRUPT OBJECT: Action Request: {\'action\': \'write_email\', \'args\': {\'to\': \'pm@client.com\', \'subject\': "Re: Tax season let\'s schedule call", \'content\': \'Dear Project Manager, I have scheduled a meeting...for 45 minutes...\'}}\n```\n\n代理已经起草了适当的确认邮件，等待最终批准。现在接受这第二个提议并检查记忆的最终状态。\n\n代码语言：javascript\n\n复制\n\n```\n # Resume the graph one last time with another \'accept\' command. print(f"\\nSimulating user accepting the {Interrupt_Object.value[0][\'action_request\'][\'action\']} tool call...") for chunk in graph.stream(Command(resume=[{"type": "accept"}]), config=thread_config_1): pass # Let the graph finish. # Check the final state of all memory namespaces. display_memory_content(store)\n```\n\n工作流完成。用户批准了代理的所有操作。\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ####### Simulating user accepting the write_email tool call... ======= CURRENT MEMORY CONTENT ======= --- triage_preferences --- Emails that are not worth responding to: ... --- response_preferences --- Use professional and concise language. ... --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. --- background --- No memory found =======================================\n```\n\n最终的记忆检查证实了假设。即使在一次完整、成功的运行之后，记忆内容也与其初始默认状态相同。这是正确的行为。简单的接受不能提供强烈的学习信号，所以代理明智地没有改变其长期知识。它使用记忆，但没有明确反馈就不会改变它。\n\n#### 测试用例 2：从直接编辑中学习\n\n下面我们测试一个有趣的例子，看看当通过直接编辑代理提议来提供明确反馈时会发生什么。这创建了一个清晰的"之前"和"之后"场景，记忆管理器 LLM 可以从中学习。\n\n用相同的邮件开始一次新的运行。\n\n代码语言：javascript\n\n复制\n\n```\n # --- Setup for a new edit test run --- checkpointer = MemorySaver() store = InMemoryStore() graph = overall_workflow.compile(checkpointer=checkpointer, store=store) thread_id_2 = uuid.uuid4() thread_config_2 = {"configurable": {"thread_id": thread_id_2}} # Run the graph until the first interrupt. print("Running the graph until the first interrupt...") for chunk in graph.stream({"email_input": email_input_respond}, config=thread_config_2): if \'__interrupt__\' in chunk: Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT:") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the initial memory state. display_memory_content(store,("email_assistant", "cal_preferences"))\n```\n\n代理暂停，再次提议一个 45 分钟的会议。现在不接受，而是将提议编辑成真正的偏好：一个 30 分钟的会议，主题更简洁。\n\n代码语言：javascript\n\n复制\n\n```\n # Define the user\'s edits to the proposed `schedule_meeting` tool call. edited_schedule_args = { "attendees": ["pm@client.com", "lance@company.com"], "subject": "Tax Planning Discussion", # Changed from "Tax Planning Strategies" "duration_minutes": 30, # Changed from 45 to 30 "preferred_day": "2025-04-22", "start_time": 14 } # Resume the graph by sending an \'edit\' command with our new arguments. print("\\nSimulating user editing the schedule_meeting tool call...") for chunk in graph.stream(Command(resume=[{"type": "edit", "args": {"args": edited_schedule_args}}]), config=thread_config_2): if \'__interrupt__\' in chunk: # Capture the next interrupt Interrupt_Object = chunk[\'__interrupt__\'][0] print("\\nINTERRUPT OBJECT (Second Interrupt):") print(f"Action Request: {Interrupt_Object.value[0][\'action_request\']}") # Check the memory AGAIN, after the edit has been processed. print("\\nChecking memory after editing schedule_meeting:") display_memory_content(store,("email_assistant", "cal_preferences"))\n```\n\n运行这个看看效果。\n\n代码语言：javascript\n\n复制\n\n```\n ###### OUTPUT ####### Simulating user editing the schedule_meeting tool call... INTERRUPT OBJECT (Second Interrupt): Action Request: {\'action\': \'write_email\', \'args\': {\'to\': \'pm@client.com\', ...}} Checking memory after editing schedule_meeting: ======= CURRENT MEMORY CONTENT ======= --- cal_preferences --- 30 minute meetings are preferred, but 15 minute meetings are also acceptable. The subject of the meeting should be \'Tax Planning Discussion\' instead of \'Tax Planning Strategies\'. The meeting duration should be 30 minutes instead of 45 minutes. ...\n```\n\n这个输出证明了系统有效。`cal_preferences` 记忆不再是简单的默认值。记忆管理器 LLM 分析了代理提议和编辑之间的差异，将更改概括为更广泛的规则。\n\n它已经学会了对较短会议和更简洁主题的偏好，这个新知识现在成为了代理记忆的永久组成部分。\n\n现在通过编辑邮件草稿来完成工作流。\n\n代码语言：javascript\n\n复制\n\n```\n # The graph is paused. Let\'s define our edits for the email draft. edited_email_args = { "to": "pm@client.com", "subject": "Re: Tax Planning Discussion", "content": "Thanks for reaching out. Sounds good. I\'ve scheduled a 30-minute call for us next Tuesday. Looking forward to it!\\n\\nBest,\\nLance" } # Resume the graph with the \'edit\' command for the write_email tool. print("\\nSimulating user editing the write_email tool call...") for chunk in graph.stream(Command(resume=[{"type": "edit", "args": {"args": edited_email_args}}]), config=thread_config_2): pass # Check the \'response_preferences\' memory to see what was learned. print("\\nChecking memory after editing write_email:") display_memory_content(store, ("email_assistant", "response_preferences")) print("\\n--- Workflow Complete ---")\n```\n\n看看终端中显示了什么。\n\n代码语言：javascript\n\n复制\n\n```\n ######## OUTPUT ######### Simulating user editing the write_email tool call... Checking memory after editing write_email: ======= CURRENT MEMORY CONTENT ======= --- response_preferences --- When responding to meeting scheduling requests, the assistant should schedule a meeting for 30 minutes instead of 45 minutes. The assistant should also use the subject line "Re: Tax Planning Discussion" instead of "Re: Tax season let\'s schedule call". The rest of the user preferences remain the same. --- Workflow Complete ---\n```\n\n再一次学习显而易见。`response_preferences` 已被更新。记忆管理器 LLM 正确识别了语气和结构上的关键差异，提取了关于主题行和会议时长的可概括规则。\n\n通过在单次运行中提供两次编辑，已经在两个不同领域个性化了代理行为，展示了这种反馈循环的力量。\n\n### 长期记忆系统的工作原理\n\n我们通过本文已经看到代理从反馈中学习，但是背后发生了什么呢？这是一个简单而强大的四步循环，将更正转化为代理的新规则。\n\n整个过程的分解：\n\n**反馈是触发器**。学习过程只在提供反馈时才会开始。简单接受一个提议不会改变记忆。只有当编辑一个动作或给出对话式响应时，学习才会被触发。\n\n**调用专用的记忆管理器**。不只是保存原始反馈。而是进行一次特殊用途的 LLM 调用。这个"记忆管理器"使用严格的 `MEMORY_UPDATE_INSTRUCTIONS` 提示词来分析反馈。\n\n**对记忆进行精确更新**。记忆管理器的任务是进行有针对性的更新。它将反馈与现有偏好进行比较，在不覆盖或删除旧规则的情况下集成新规则。这确保代理永远不会忘记过去的教训。\n\n**新知识在下次运行时注入**。更新后的偏好字符串被保存到 `Store` 中。下次代理开始新任务时会获取这个新字符串，将学到的行为注入其提示词中，改变其未来的行为方式。\n\n这个"触发 -> 管理 -> 更新 -> 注入"循环使代理能够从一个通用工具演变为个性化的助手。\n\n作者：Fareed Khan\n\n本文参与\xa0[腾讯云自媒体同步曝光计划](/developer/support-plan)，分享自微信公众号。\n\n原始发表：2025-10-20，如有侵权请联系\xa0[cloudcommunity@tencent.com](mailto:cloudcommunity@tencent.com) 删除\n\n[存储](/developer/tag/10665)\n\n[prompt](/developer/tag/15406)\n\n[代理](/developer/tag/17225)\n\n[工具](/developer/tag/17276)\n\n[函数](/developer/tag/17290)\n\n本文分享自 DeepHub IMBA 微信公众号，前往查看\n\n如有侵权，请联系 [cloudcommunity@tencent.com](mailto:cloudcommunity@tencent.com) 删除。\n\n本文参与\xa0[腾讯云自媒体同步曝光计划](/developer/support-plan)\xa0 ，欢迎热爱写作的你一起参与！\n\n[存储](/developer/tag/10665)\n\n[prompt](/developer/tag/15406)\n\n[代理](/developer/tag/17225)\n\n[工具](/developer/tag/17276)\n\n[函数](/developer/tag/17290)\n\n登录后参与评论\n\n登录 后参与评论\n\n* 线程级记忆（短期）\n\n* 跨线程记忆（长期）\n\n* LangGraph 数据持久层\n  + InMemory Store（用于 notebook 和快速测试）\n  + 本地开发存储（langgraph dev）\n  + 生产存储（LangGraph Platform 或自托管）\n\n* InMemory 功能实践\n\n* 构建代理架构\n  + 定义模式\n  + 创建提示词\n  + 定义工具和实用函数\n\n* 记忆功能与图节点\n\n* 通过人机协作捕获反馈\n\n* 组装成工作流\n\n* 使用记忆测试代理\n  + 测试用例 1：基线——接受提议\n  + 测试用例 2：从直接编辑中学习\n\n* 长期记忆系统的工作原理\n\n对象存储（Cloud Object Storage，COS）是由腾讯云推出的无目录层次结构、无数据格式限制，可容纳海量数据且支持 HTTP/HTTPS 协议访问的分布式存储服务。腾讯云 COS 的存储桶空间无容量上限，无需分区管理，适用于 CDN 数据分发、数据万象处理或大数据计算与分析的数据湖等多种场景。\n\n[产品介绍](https://cloud.tencent.com/product/cos?from=21341&from_column=21341)[产品文档](https://cloud.tencent.com/document/product/436?from=21342&from_column=21342)\n\n* ### 社区\n\n  + [技术文章](/developer/column)\n  + [技术问答](/developer/ask)\n  + [技术沙龙](/developer/salon)\n  + [技术视频](/developer/video)\n  + [学习中心](/developer/learning)\n  + [技术百科](/developer/techpedia)\n  + [技术专区](/developer/zone/list)\n* ### 活动\n\n  + [自媒体同步曝光计划](/developer/support-plan)\n  + [邀请作者入驻](/developer/support-plan-invitation)\n  + [自荐上首页](/developer/article/1535830)\n  + [技术竞赛](/developer/competition)\n* ### 圈层\n\n  + [腾讯云最具价值专家](/tvp)\n  + [腾讯云架构师技术同盟](/developer/program/tm)\n  + [腾讯云创作之星](/developer/program/tci)\n  + [腾讯云TDP](/developer/program/tdp)\n* ### 关于\n\n  + [社区规范](/developer/article/1006434)\n  + [免责声明](/developer/article/1006435)\n  + [联系我们](mailto:cloudcommunity@tencent.com)\n  + [友情链接](/developer/friendlink)\n  + [MCP广场开源版权声明](/developer/article/2537547)\n\n### 腾讯云开发者\n\n### 热门产品\n\n* [域名注册](/product/domain?from=20064&from_column=20064)\n* [云服务器](/product/cvm?from=20064&from_column=20064)\n* [区块链服务](/product/tbaas?from=20064&from_column=20064)\n* [消息队列](/product/message-queue-catalog?from=20064&from_column=20064)\n* [网络加速](/product/ecdn?from=20064&from_column=20064)\n* [云数据库](/product/tencentdb-catalog?from=20064&from_column=20064)\n* [域名解析](/product/dns?from=20064&from_column=20064)\n* [云存储](/product/cos?from=20064&from_column=20064)\n* [视频直播](/product/css?from=20064&from_column=20064)\n\n### 热门推荐\n\n* [人脸识别](/product/facerecognition?from=20064&from_column=20064)\n* [腾讯会议](/product/tm?from=20064&from_column=20064)\n* [企业云](/act/pro/enterprise2022?from=20064&from_column=20064)\n* [CDN加速](/product/cdn?from=20064&from_column=20064)\n* [视频通话](/product/trtc?from=20064&from_column=20064)\n* [图像分析](/product/imagerecognition?from=20064&from_column=20064)\n* [MySQL 数据库](/product/cdb?from=20064&from_column=20064)\n* [SSL 证书](/product/ssl?from=20064&from_column=20064)\n* [语音识别](/product/asr?from=20064&from_column=20064)\n\n### 更多推荐\n\n* [数据安全](/solution/data_protection?from=20064&from_column=20064)\n* [负载均衡](/product/clb?from=20064&from_column=20064)\n* [短信](/product/sms?from=20064&from_column=20064)\n* [文字识别](/product/ocr?from=20064&from_column=20064)\n* [云点播](/product/vod?from=20064&from_column=20064)\n* [大数据](/product/bigdata-class?from=20064&from_column=20064)\n* [小程序开发](/solution/la?from=20064&from_column=20064)\n* [网站监控](/product/tcop?from=20064&from_column=20064)\n* [数据迁移](/product/cdm?from=20064&from_column=20064)\n\nCopyright © 2013 - 2026Tencent Cloud. All Rights Reserved. 腾讯云 版权所有\n\n[深圳市腾讯计算机系统有限公司](https://qcloudimg.tencent-cloud.cn/raw/986376a919726e0c35e96b311f54184d.jpg)\xa0ICP备案/许可证号：[粤B2-20090059](https://beian.miit.gov.cn/#/Integrated/index)[粤公网安备44030502008569号](https://beian.mps.gov.cn/#/query/webSearch?code=44030502008569)\n\n[腾讯云计算（北京）有限责任公司](https://qcloudimg.tencent-cloud.cn/raw/a2390663ee4a95ceeead8fdc34d4b207.jpg) 京ICP证150476号 | \xa0[京ICP备11018762号](https://beian.miit.gov.cn/#/Integrated/index)\n\n[问题归档](/developer/ask/archives.html)[专栏文章](/developer/column/archives.html)[快讯文章归档](/developer/news/archives.html)[关键词归档](/developer/information/all.html)[开发者手册归档](/developer/devdocs/archives.html)[开发者手册 Section 归档](/developer/devdocs/sections_p1.html)\n\nCopyright © 2013 - 2026Tencent Cloud.\n\nAll Rights Reserved. 腾讯云 版权所有\n\n登录 后参与评论', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://cloud.tencent.com/developer/article/2588385', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.8591094, 'saved_path': '/home/qinshan/widthresearch/data/downloads/tavily_LangGraph .md'}}
2026-02-22 20:38:05,168 - __main__ - INFO - call_tool: name=exa_context_download, args={'papers': [{'paper_id': '', 'title': "LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学 | Ningto's Blog", 'authors': [], 'abstract': 'LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学 | Ningto&#x27;s Blog\n[\nNingto&#x27;s Blog\n] \n[Blog] [LLM] [Tags] [CSS] [Projects] [About] \n# LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学\n[AI] [LangChain] [LLM] [Memory-Management] [LangGraph] \n•\n2026年1月22日星期四\n目录\n* [LangChain内存管理的核心概念与设计哲学] \n* [短时内存：会话上下文的守护者] \n* [长时内存：持久化知识与经验的基石] \n* [与LangGraph的集成：构建有状态的、持久的智能代理] \n* [最佳实践与性能优化建议] \n* [总结] \n在构建基于大型语言模型（LLM）的智能应用时，一个核心挑战是如何高效、优雅地管理“记忆”。无论是简单的聊天机器人需要记住上一条消息，还是复杂的个人助理需要从过往数月的交互中检索关键信息，内存管理都是决定应用智能程度与用户体验的关键。LangChain，作为当前最流行的LLM应用开发框架之一，其内存管理系统正是为解决这一系列问题而精心设计的。本文将深入其内部，解析其短时内存与长时内存的架构选择、实现机制，并探讨如何基于LangGraph构建具备持久化记忆的下一代AI代理。\n## [] LangChain内存管理的核心概念与设计哲学\nLangChain将“记忆”抽象为一个核心组件，其设计哲学可以概括为：**标准化、模块化与可组合性**。它不试图提供一个万能的记忆解决方案，而是定义了一套清晰的接口和抽象，允许开发者根据应用场景自由选择和组合不同的记忆策略。\n从宏观上看，LangChain将内存分为两大类别：\n1. **短时内存 (Short-term Memory)**：用于维持单次对话或单个执行流程中的上下文。它通常是易失的，生命周期与当前的会话或代理运行实例绑定。其核心作用是解决LLM有限的上下文窗口问题，通过有选择地保留、总结或压缩历史消息，确保最重要的信息能被模型“看见”。\n2. **长时内存 (Long-term Memory)**：用于跨会话、跨进程甚至跨时间的持久化信息存储与检索。它通常与外部数据库（如向量数据库、SQL数据库）连接，能够存储海量信息，并根据当前查询动态检索最相关的片段。其核心作用是赋予AI代理“经验”和“知识”，实现持续学习和个性化。\n这种划分并非LangChain独创，但它通过清晰的API和与LangGraph的深度集成，将这一理念变成了易于实现的工程实践。\n## [] 短时内存：会话上下文的守护者\n短时内存的核心任务是管理对话历史。最简单的形式就是 `ConversationBufferMemory`，它像一个FIFO队列，忠实地记录所有对话。\n```python\n<span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory\n</span><span class="code-line"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> ChatOpenAI\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 初始化一个简单的对话缓冲内存</span>\n</span><span class="code-line">memory <span class="token operator">=</span> ConversationBufferMemory<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"gpt-4"</span><span class="token punctuation">)</span>\n</span><span class="code-line">conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> memory<span class="token operator">=</span>memory<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 进行对话</span>\n</span><span class="code-line">conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你好，我叫小明。"</span><span class="token punctuation">)</span>\n</span><span class="code-line"><span class="token comment"># 内存中现在保存了：Human: 你好，我叫小明。\\nAI: [模型的回复]</span>\n</span><span class="code-line">conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"我的名字是什么？"</span><span class="token punctuation">)</span>\n</span><span class="code-line"><span class="token comment"># AI能够正确回答“小明”，因为它从内存中读取了历史。</span>\n</span>\n```\n然而，随着对话轮次增加，简单的缓冲会迅速耗尽模型的上下文窗口。为此，LangChain提供了更智能的策略：\n* `ConversationBufferWindowMemory`：只保留最近K轮对话。\n* `ConversationSummaryMemory`：定期（或按需）使用LLM对历史对话进行总结，用总结摘要替代原始长文本，从而大幅节省Token。\n* `ConversationTokenBufferMemory`：基于Token数量进行限制，而非对话轮次，更精确地控制上下文长度。\n* `ConversationSummaryBufferMemory`：结合了总结和缓冲，在Token超限时，将最早的历史进行总结，保留最近的原始对话。\n这些内存组件都实现了统一的 `BaseChatMemory` 接口，可以像乐高积木一样在Chain或Agent中替换，体现了极佳的模块化设计。\n## [] 长时内存：持久化知识与经验的基石\n当应用需要记住超越单次会话的信息时，就需要长时内存。LangChain的长时内存系统通常与 **检索器 (Retriever)** 紧密结合，其标准范式是：**存储时进行向量化嵌入并保存到数据库；检索时根据查询的向量相似度召回最相关的片段**。\n```python\n<span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> VectorStoreRetrieverMemory\n</span><span class="code-line"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> OpenAIEmbeddings\n</span><span class="code-line"><span class="token keyword">from</span> langchain_chroma <span class="token keyword">import</span> Chroma\n</span><span class="code-line"><span class="token keyword">from</span> langchain_core<span class="token punctuation">.</span>documents <span class="token keyword">import</span> Document\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 1. 准备一个支持检索的向量数据库作为记忆后端</span>\n</span><span class="code-line">embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">vectorstore <span class="token operator">=</span> Chroma<span class="token punctuation">(</span>embedding_function<span class="token operator">=</span>embeddings<span class="token punctuation">,</span> collection_name<span class="token operator">=</span><span class="token string">"long_term_memory"</span><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 2. 创建一些“记忆”文档并存储</span>\n</span><span class="code-line">memories <span class="token operator">=</span> <span class="token punctuation">[</span>\n</span><span class="code-line">    Document<span class="token punctuation">(</span>page_content<span class="token operator">=</span><span class="token string">"小明最喜欢的颜色是蓝色。"</span><span class="token punctuation">,</span> metadata<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"speaker"</span><span class="token punctuation">:</span> <span class="token string">"human"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n</span><span class="code-line">    Document<span class="token punctuation">(</span>page_content<span class="token operator">=</span><span class="token string">"小明的生日是7月15日。"</span><span class="token punctuation">,</span> metadata<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"speaker"</span><span class="token punctuation">:</span> <span class="token string">"human"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n</span><span class="code-line"><span class="token punctuation">]</span>\n</span><span class="code-line">vectorstore<span class="token punctuation">.</span>add_documents<span class="token punctuation">(</span>memories<span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 3. 创建基于向量检索的记忆组件</span>\n</span><span class="code-line">retriever <span class="token operator">=</span> vectorstore<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span>search_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"k"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">)</span>\n</span><span class="code-line">memory <span class="token operator">=</span> VectorStoreRetrieverMemory<span class="token punctuation">(</span>retriever<span class="token operator">=</span>retriever<span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 4. 在对话中使用</span>\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChain\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 提示词模板，通过 `{history}` 和 `{input}` 注入记忆和当前输入</span>\n</span><span class="code-line">prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>\n</span><span class="code-line">    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"history"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>\n</span><span class="code-line">    template<span class="token operator">=</span>`你是一个贴心的助手，以下是你已知的关于用户的背景信息：\\n<span class="token punctuation">{</span>history<span class="token punctuation">}</span>\\n\\n当前对话：\\nHuman<span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token builtin">input</span><span class="token punctuation">}</span>\\nAI<span class="token punctuation">:</span>`\n</span><span class="code-line"><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">chain <span class="token operator">=</span> LLMChain<span class="token punctuation"', 'doi': '', 'published_date': '2026-01-22T00:00:00+00:00', 'pdf_url': '', 'url': 'https://www.ningto.com/blog/2026/langchain-memory-management-deep-dive-from-short-term-conversations-to-long-term-memory-architecture-philosophy', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:05,168 - __main__ - INFO - handle_download: searcher=ExaSearcherContext, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:05,169 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:05,169 - __main__ - INFO - call_tool payload: source_tool=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:05,169 - __main__ - INFO - call_tool: name=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:05,169 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': "LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学 | Ningto's Blog", 'authors': [], 'abstract': 'LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学 | Ningto&#x27;s Blog\n[\nNingto&#x27;s Blog\n] \n[Blog] [LLM] [Tags] [CSS] [Projects] [About] \n# LangChain内存管理机制深度解析：从短时对话到长时记忆的架构哲学\n[AI] [LangChain] [LLM] [Memory-Management] [LangGraph] \n•\n2026年1月22日星期四\n目录\n* [LangChain内存管理的核心概念与设计哲学] \n* [短时内存：会话上下文的守护者] \n* [长时内存：持久化知识与经验的基石] \n* [与LangGraph的集成：构建有状态的、持久的智能代理] \n* [最佳实践与性能优化建议] \n* [总结] \n在构建基于大型语言模型（LLM）的智能应用时，一个核心挑战是如何高效、优雅地管理“记忆”。无论是简单的聊天机器人需要记住上一条消息，还是复杂的个人助理需要从过往数月的交互中检索关键信息，内存管理都是决定应用智能程度与用户体验的关键。LangChain，作为当前最流行的LLM应用开发框架之一，其内存管理系统正是为解决这一系列问题而精心设计的。本文将深入其内部，解析其短时内存与长时内存的架构选择、实现机制，并探讨如何基于LangGraph构建具备持久化记忆的下一代AI代理。\n## [] LangChain内存管理的核心概念与设计哲学\nLangChain将“记忆”抽象为一个核心组件，其设计哲学可以概括为：**标准化、模块化与可组合性**。它不试图提供一个万能的记忆解决方案，而是定义了一套清晰的接口和抽象，允许开发者根据应用场景自由选择和组合不同的记忆策略。\n从宏观上看，LangChain将内存分为两大类别：\n1. **短时内存 (Short-term Memory)**：用于维持单次对话或单个执行流程中的上下文。它通常是易失的，生命周期与当前的会话或代理运行实例绑定。其核心作用是解决LLM有限的上下文窗口问题，通过有选择地保留、总结或压缩历史消息，确保最重要的信息能被模型“看见”。\n2. **长时内存 (Long-term Memory)**：用于跨会话、跨进程甚至跨时间的持久化信息存储与检索。它通常与外部数据库（如向量数据库、SQL数据库）连接，能够存储海量信息，并根据当前查询动态检索最相关的片段。其核心作用是赋予AI代理“经验”和“知识”，实现持续学习和个性化。\n这种划分并非LangChain独创，但它通过清晰的API和与LangGraph的深度集成，将这一理念变成了易于实现的工程实践。\n## [] 短时内存：会话上下文的守护者\n短时内存的核心任务是管理对话历史。最简单的形式就是 `ConversationBufferMemory`，它像一个FIFO队列，忠实地记录所有对话。\n```python\n<span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory\n</span><span class="code-line"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> ChatOpenAI\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 初始化一个简单的对话缓冲内存</span>\n</span><span class="code-line">memory <span class="token operator">=</span> ConversationBufferMemory<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"gpt-4"</span><span class="token punctuation">)</span>\n</span><span class="code-line">conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> memory<span class="token operator">=</span>memory<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 进行对话</span>\n</span><span class="code-line">conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"你好，我叫小明。"</span><span class="token punctuation">)</span>\n</span><span class="code-line"><span class="token comment"># 内存中现在保存了：Human: 你好，我叫小明。\\nAI: [模型的回复]</span>\n</span><span class="code-line">conversation<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"我的名字是什么？"</span><span class="token punctuation">)</span>\n</span><span class="code-line"><span class="token comment"># AI能够正确回答“小明”，因为它从内存中读取了历史。</span>\n</span>\n```\n然而，随着对话轮次增加，简单的缓冲会迅速耗尽模型的上下文窗口。为此，LangChain提供了更智能的策略：\n* `ConversationBufferWindowMemory`：只保留最近K轮对话。\n* `ConversationSummaryMemory`：定期（或按需）使用LLM对历史对话进行总结，用总结摘要替代原始长文本，从而大幅节省Token。\n* `ConversationTokenBufferMemory`：基于Token数量进行限制，而非对话轮次，更精确地控制上下文长度。\n* `ConversationSummaryBufferMemory`：结合了总结和缓冲，在Token超限时，将最早的历史进行总结，保留最近的原始对话。\n这些内存组件都实现了统一的 `BaseChatMemory` 接口，可以像乐高积木一样在Chain或Agent中替换，体现了极佳的模块化设计。\n## [] 长时内存：持久化知识与经验的基石\n当应用需要记住超越单次会话的信息时，就需要长时内存。LangChain的长时内存系统通常与 **检索器 (Retriever)** 紧密结合，其标准范式是：**存储时进行向量化嵌入并保存到数据库；检索时根据查询的向量相似度召回最相关的片段**。\n```python\n<span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> VectorStoreRetrieverMemory\n</span><span class="code-line"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> OpenAIEmbeddings\n</span><span class="code-line"><span class="token keyword">from</span> langchain_chroma <span class="token keyword">import</span> Chroma\n</span><span class="code-line"><span class="token keyword">from</span> langchain_core<span class="token punctuation">.</span>documents <span class="token keyword">import</span> Document\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 1. 准备一个支持检索的向量数据库作为记忆后端</span>\n</span><span class="code-line">embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">vectorstore <span class="token operator">=</span> Chroma<span class="token punctuation">(</span>embedding_function<span class="token operator">=</span>embeddings<span class="token punctuation">,</span> collection_name<span class="token operator">=</span><span class="token string">"long_term_memory"</span><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 2. 创建一些“记忆”文档并存储</span>\n</span><span class="code-line">memories <span class="token operator">=</span> <span class="token punctuation">[</span>\n</span><span class="code-line">    Document<span class="token punctuation">(</span>page_content<span class="token operator">=</span><span class="token string">"小明最喜欢的颜色是蓝色。"</span><span class="token punctuation">,</span> metadata<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"speaker"</span><span class="token punctuation">:</span> <span class="token string">"human"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n</span><span class="code-line">    Document<span class="token punctuation">(</span>page_content<span class="token operator">=</span><span class="token string">"小明的生日是7月15日。"</span><span class="token punctuation">,</span> metadata<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"speaker"</span><span class="token punctuation">:</span> <span class="token string">"human"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n</span><span class="code-line"><span class="token punctuation">]</span>\n</span><span class="code-line">vectorstore<span class="token punctuation">.</span>add_documents<span class="token punctuation">(</span>memories<span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 3. 创建基于向量检索的记忆组件</span>\n</span><span class="code-line">retriever <span class="token operator">=</span> vectorstore<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span>search_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"k"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">)</span>\n</span><span class="code-line">memory <span class="token operator">=</span> VectorStoreRetrieverMemory<span class="token punctuation">(</span>retriever<span class="token operator">=</span>retriever<span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 4. 在对话中使用</span>\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate\n</span><span class="code-line"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChain\n</span><span class="code-line">\n</span><span class="code-line"><span class="token comment"># 提示词模板，通过 `{history}` 和 `{input}` 注入记忆和当前输入</span>\n</span><span class="code-line">prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>\n</span><span class="code-line">    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"history"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>\n</span><span class="code-line">    template<span class="token operator">=</span>`你是一个贴心的助手，以下是你已知的关于用户的背景信息：\\n<span class="token punctuation">{</span>history<span class="token punctuation">}</span>\\n\\n当前对话：\\nHuman<span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token builtin">input</span><span class="token punctuation">}</span>\\nAI<span class="token punctuation">:</span>`\n</span><span class="code-line"><span class="token punctuation">)</span>\n</span><span class="code-line">\n</span><span class="code-line">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>\n</span><span class="code-line">chain <span class="token operator">=</span> LLMChain<span class="token punctuation"', 'doi': '', 'published_date': '2026-01-22T00:00:00+00:00', 'pdf_url': '', 'url': 'https://www.ningto.com/blog/2026/langchain-memory-management-deep-dive-from-short-term-conversations-to-long-term-memory-architecture-philosophy', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'saved_path': '/home/qinshan/widthresearch/data/downloads/exa_LangChain内.md'}}
2026-02-22 20:38:18,081 - __main__ - INFO - call_tool: name=tavily_download, args={'papers': [{'paper_id': '', 'title': 'LangGraph 持久化 - 概述- LangChain 教程', 'authors': [], 'abstract': '* [线程](#threads)\n* [检查点](#checkpoints) \n\n  + [获取状态](#get-state)\n  + [获取状态历史](#get-state-history)\n  + [重放](#replay)\n  + [更新状态](#update-state) \n\n    - [config](#config)\n    - [values](#values)\n    - [as\\_node](#as_node)\n* [记忆存储](#memory-store) \n\n  + [基本用法](#basic-usage)\n  + [语义搜索](#semantic-search)\n  + [在 LangGraph 中使用](#using-in-langgraph)\n* [检查点库](#checkpointer-libraries) \n\n  + [检查点接口](#checkpointer-interface)\n  + [序列化器](#serializer) \n\n    - [使用 pickle 进行序列化](#serialization-with-pickle)\n    - [加密](#encryption)\n* [功能](#capabilities) \n\n  + [人工干预](#human-in-the-loop)\n  + [记忆](#memory)\n  + [时光穿梭](#time-travel)\n  + [容错](#fault-tolerance) \n\n    - [待定写入](#pending-writes)\n\n# 持久性[¶](#persistence "Permanent link")\n\nLangGraph 有一个内置的持久化层，通过检查点（checkpointer）实现。当您使用检查点编译一个图时，检查点会在每个超级步骤（super-step）保存图状态的`checkpoint`。这些检查点被保存到一个`thread`（线程）中，可以在图执行后访问。因为`threads`允许在执行后访问图的状态，所以可以实现诸如人在环路、记忆、时间旅行和容错等多种强大功能。下面，我们将更详细地讨论这些概念。\n\nLangGraph API 自动处理检查点\n\n使用 LangGraph API 时，您无需手动实现或配置检查点。API 在后台为您处理所有持久化基础架构。\n\n## 线程[¶](#threads "Permanent link")\n\n线程（thread）是由检查点保存的每个检查点的唯一 ID 或线程标识符。它包含一系列[运行](../assistants/#execution)的累积状态。当一次运行被执行时，助手的底层图的[状态](../low_level/#state)将被持久化到该线程中。\n\n当使用检查点调用一个图时，您**必须**在配置的`configurable`部分指定一个`thread_id`。\n\n```\n{"configurable": {"thread_id": "1"}} {"configurable":{"thread_id": "1"}}\n```\n\n可以检索一个线程的当前和历史状态。为了持久化状态，必须在执行一次运行之前创建一个线程。LangGraph 平台 API 提供了几个用于创建和管理线程及线程状态的端点。更多详情请参见[API 参考](../../cloud/reference/api/api_ref.html#tag/threads)。\n\n## 检查点[¶](#checkpoints "Permanent link")\n\n一个线程在特定时间点的状态称为检查点（checkpoint）。检查点是在每个超级步骤保存的图状态的快照，由一个`StateSnapshot`对象表示，具有以下关键属性：\n\n* `config`: 与此检查点关联的配置。\n* `metadata`: 与此检查点关联的元数据。\n* `values`: 在此时间点的状态通道的值。\n* `next`: 一个元组，包含图中接下来要执行的节点名称。\n* `tasks`: 一个`PregelTask`对象的元组，包含有关接下来要执行的任务的信息。如果该步骤之前尝试过，它将包含错误信息。如果图在节点内部被[动态](../../how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt)中断，任务将包含与中断相关的额外数据。\n\n检查点是持久化的，可以用来在稍后恢复线程的状态。\n\n让我们看看当一个简单的图被如下调用时，会保存哪些检查点：\n\n*API 参考：[StateGraph](https://github.langchain.ac.cn/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://github.langchain.ac.cn/langgraph/reference/constants/#langgraph.constants.START) | [END](https://github.langchain.ac.cn/langgraph/reference/constants/#langgraph.constants.END) | [InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)*\n\n```\nfrom langgraph.graph import StateGraph, START, END from  langgraph.graph  import StateGraph, START, ENDfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaver from typing import Annotated from  typing  import Annotated from typing_extensions import TypedDict from  typing_extensions  import TypedDict from operator import add from  operator  import add  class State(TypedDict): class  State(TypedDict): foo: str foo: str bar: Annotated[list[str], add] bar: Annotated[list[str], add]  def node_a(state: State): def  node_a(state: State): return {"foo": "a", "bar": ["a"]} return{"foo": "a", "bar":["a"]}  def node_b(state: State): def  node_b(state: State): return {"foo": "b", "bar": ["b"]} return{"foo": "b", "bar":["b"]}   workflow = StateGraph(State) workflow = StateGraph(State)workflow.add_node(node_a) workflow. add_node(node_a)workflow.add_node(node_b) workflow. add_node(node_b)workflow.add_edge(START, "node_a") workflow. add_edge(START, "node_a")workflow.add_edge("node_a", "node_b") workflow. add_edge("node_a", "node_b")workflow.add_edge("node_b", END) workflow. add_edge("node_b", END)  checkpointer = InMemorySaver() checkpointer = InMemorySaver()graph = workflow.compile(checkpointer=checkpointer) graph = workflow. compile(checkpointer = checkpointer)  config = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}graph.invoke({"foo": ""}, config) graph. invoke({"foo": ""}, config)\n```\n\n在我们运行图之后，我们期望看到正好 4 个检查点：\n\n* 一个空的检查点，以`START`作为下一个要执行的节点。\n* 一个包含用户输入`{\'foo\': \'\', \'bar\': []}`的检查点，以`node_a`作为下一个要执行的节点。\n* 一个包含`node_a`输出`{\'foo\': \'a\', \'bar\': [\'a\']}`的检查点，以`node_b`作为下一个要执行的节点。\n* 一个包含`node_b`输出`{\'foo\': \'b\', \'bar\': [\'a\', \'b\']}`的检查点，没有下一个要执行的节点。\n\n请注意，`bar`通道的值包含了两个节点的输出，因为我们为`bar`通道定义了一个归约器（reducer）。\n\n### 获取状态[¶](#get-state "Permanent link")\n\n当与保存的图状态进行交互时，您**必须**指定一个[线程标识符](#threads)。您可以通过调用`graph.get_state(config)`来查看图的*最新*状态。这将返回一个`StateSnapshot`对象，它对应于配置中提供的线程 ID 的最新检查点，或者（如果提供了）对应于该线程的某个检查点 ID 的检查点。\n\n```\n # get the latest state snapshot # get the latest state snapshotconfig = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}graph.get_state(config) graph. get_state(config)  # get a state snapshot for a specific checkpoint_id # get a state snapshot for a specific checkpoint_idconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}} config ={"configurable":{"thread_id": "1", "checkpoint_id":"1ef663ba-28fe-6528-8002-5a559208592c"}}graph.get_state(config) graph. get_state(config)\n```\n\n在我们的例子中，`get_state`的输出将如下所示：\n\n```\nStateSnapshot(  values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},  next=(),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},  created_at=\'2024-08-29T19:19:38.821749+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}}, tasks=() ) \n```\n\n### 获取状态历史[¶](#get-state-history "Permanent link")\n\n您可以通过调用`graph.get_state_history(config)`来获取给定线程的完整图执行历史。这将返回一个与配置中提供的线程 ID 相关联的`StateSnapshot`对象列表。重要的是，检查点将按时间顺序排列，最新的检查点 / `StateSnapshot`位于列表的第一个。\n\n```\nconfig = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}list(graph.get_state_history(config)) list(graph. get_state_history(config))\n```\n\n在我们的例子中，`get_state_history`的输出将如下所示：\n\n```\n[  StateSnapshot(  values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},  next=(),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},  created_at=\'2024-08-29T19:19:38.821749+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},  tasks=(),  ),  StateSnapshot(  values={\'foo\': \'a\', \'bar\': [\'a\']}, next=(\'node_b\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_a\': {\'foo\': \'a\', \'bar\': [\'a\']}}, \'step\': 1},  created_at=\'2024-08-29T19:19:38.819946+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},  tasks=(PregelTask(id=\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\', name=\'node_b\', error=None, interrupts=()),),  ),  StateSnapshot(  values={\'foo\': \'\', \'bar\': []},  next=(\'node_a\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},  metadata={\'source\': \'loop\', \'writes\': None, \'step\': 0},  created_at=\'2024-08-29T19:19:38.817813+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},  tasks=(PregelTask(id=\'f1b14528-5ee5-579c-949b-23ef9bfbed58\', name=\'node_a\', error=None, interrupts=()),),  ),  StateSnapshot(  values={\'bar\': []},  next=(\'__start__\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},  metadata={\'source\': \'input\', \'writes\': {\'foo\': \'\'}, \'step\': -1},  created_at=\'2024-08-29T19:19:38.816205+00:00\',  parent_config=None,  tasks=(PregelTask(id=\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\', name=\'__start__\', error=None, interrupts=()),),  ) ] \n```\n\n### 重放[¶](#replay "Permanent link")\n\n也可以回放之前的图执行。如果我们使用一个`thread_id`和一个`checkpoint_id`来`invoke`一个图，那么我们将*重放*与`checkpoint_id`对应的检查点*之前*已执行的步骤，而只执行该检查点*之后*的步骤。\n\n* `thread_id`是线程的 ID。\n* `checkpoint_id`是引用线程内特定检查点的标识符。\n\n您必须在调用图时将这些作为配置的`configurable`部分传递。\n\n```\nconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}} config ={"configurable":{"thread_id": "1", "checkpoint_id":"0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}graph.invoke(None, config=config) graph. invoke(None, config = config)\n```\n\n重要的是，LangGraph 知道某个特定步骤之前是否已执行过。如果是，LangGraph 只是*重放*图中该特定步骤，而不会重新执行它，但这仅限于所提供的`checkpoint_id`*之前*的步骤。所有在`checkpoint_id`*之后*的步骤都将被执行（即一个新的分支），即使它们之前已经执行过。请参阅关于[时间旅行的这份操作指南，以了解更多关于重放的信息](../../how-tos/human_in_the_loop/time-travel/)。\n\n### 更新状态[¶](#update-state "永久链接")\n\n除了从特定的`checkpoints`重放图之外，我们还可以*编辑*图状态。我们使用`graph.update_state()`来做到这一点。该方法接受三个不同的参数：\n\n#### `config`[¶](#config "Permanent link")\n\n配置应包含`thread_id`，指定要更新哪个线程。当只传递`thread_id`时，我们更新（或分叉）当前状态。或者，如果我们包含`checkpoint_id`字段，那么我们将分叉该选定的检查点。\n\n#### `values`[¶](#values "Permanent link")\n\n这些是用于更新状态的值。请注意，此更新的处理方式与来自节点的任何更新完全相同。这意味着这些值将被传递给[归约器](../low_level/#reducers)函数（如果为图状态中的某些通道定义了归约器）。这意味着`update_state`不会自动覆盖每个通道的值，而只会覆盖没有归约器的通道。让我们看一个例子。\n\n假设您使用以下模式定义了图的状态（参见上面的完整示例）：\n\n```\n from typing import Annotated from  typing  import Annotated from typing_extensions import TypedDict from  typing_extensions  import TypedDict from operator import add from  operator  import add  class State(TypedDict): class  State(TypedDict): foo: int foo: int bar: Annotated[list[str], add] bar: Annotated[list[str], add]\n```\n\n现在假设图的当前状态是：\n\n```\n{"foo": 1, "bar": ["a"]} \n```\n\n如果您如下更新状态：\n\n```\ngraph.update_state(config, {"foo": 2, "bar": ["b"]}) graph. update_state(config,{"foo": 2, "bar":["b"]})\n```\n\n那么图的新状态将是：\n\n```\n{"foo": 2, "bar": ["a", "b"]} \n```\n\n`foo`键（通道）被完全更改（因为没有为该通道指定归约器，所以`update_state`会覆盖它）。然而，为`bar`键指定了一个归约器，所以它将`"b"`附加到`bar`的状态中。\n\n#### `as_node`[¶](#as_node "Permanent link")\n\n在调用`update_state`时，您可以选择性地指定最后一个参数是`as_node`。如果您提供了它，更新将像来自节点`as_node`一样被应用。如果未提供`as_node`，它将被设置为最后更新状态的节点（如果不模糊）。这之所以重要，是因为下一步要执行的步骤取决于最后给出更新的节点，因此这可以用来控制接下来执行哪个节点。请参阅关于[分叉状态的这份操作指南，以了解更多信息](../../how-tos/human_in_the_loop/time-travel/)。\n\n## 记忆存储[¶](#memory-store "Permanent link")\n\n一个[状态模式](../low_level/#schema)指定了一组在图执行过程中被填充的键。如上所述，状态可以由检查点在每个图步骤写入线程，从而实现状态持久化。\n\n但是，如果我们想在*多个线程之间*保留一些信息怎么办？考虑一个聊天机器人的案例，我们希望在与该用户的所有聊天对话（即线程）中保留关于该用户的特定信息！\n\n仅使用检查点，我们无法在线程之间共享信息。这就需要[`Store`](../../reference/store/#langgraph.store.base.BaseStore)接口。作为示例，我们可以定义一个`InMemoryStore`来跨线程存储关于用户的信息。我们像以前一样用检查点编译我们的图，并使用我们新的`in_memory_store`变量。\n\nLangGraph API 自动处理存储\n\n当使用 LangGraph API 时，您无需手动实现或配置存储。API 在后台为您处理所有存储基础架构。\n\n### 基本用法[¶](#basic-usage "Permanent link")\n\n首先，让我们在不使用 LangGraph 的情况下单独展示这一点。\n\n```\nfrom langgraph.store.memory import InMemoryStore from  langgraph.store.memory  import InMemoryStorein_memory_store = InMemoryStore() in_memory_store = InMemoryStore()\n```\n\n记忆由一个`tuple`进行命名空间划分，在这个特定示例中是`(, "memories")`。命名空间可以是任何长度，可以代表任何东西，不一定是特定于用户的。\n\n```\nuser_id = "1" user_id = "1"namespace_for_memory = (user_id, "memories") namespace_for_memory =(user_id, "memories")\n```\n\n我们使用`store.put`方法将记忆保存到存储中的命名空间。当我们这样做时，我们指定如上定义的命名空间，以及记忆的键值对：键是记忆的唯一标识符（`memory_id`），值（一个字典）是记忆本身。\n\n```\nmemory_id = str(uuid.uuid4()) memory_id = str(uuid. uuid4())memory = {"food_preference" : "I like pizza"} memory ={"food_preference": "I like pizza"}in_memory_store.put(namespace_for_memory, memory_id, memory) in_memory_store. put(namespace_for_memory, memory_id, memory)\n```\n\n我们可以使用`store.search`方法读出我们命名空间中的记忆，它将返回给定用户的所有记忆作为一个列表。最新的记忆是列表中的最后一个。\n\n```\nmemories = in_memory_store.search(namespace_for_memory) memories = in_memory_store. search(namespace_for_memory)memories[-1].dict() memories[- 1]. dict(){\'value\': {\'food_preference\': \'I like pizza\'}, {\'value\':{\'food_preference\': \'I like pizza\'}, \'key\': \'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'namespace\': [\'1\', \'memories\'], \'namespace\':[\'1\', \'memories\'], \'created_at\': \'2024-10-02T17:22:31.590602+00:00\', \'created_at\':\'2024-10-02T17:22:31.590602+00:00\', \'updated_at\': \'2024-10-02T17:22:31.590605+00:00\'} \'updated_at\':\'2024-10-02T17:22:31.590605+00:00\'}\n```\n\n每种记忆类型都是一个 Python 类 ([`Item`](https://github.langchain.ac.cn/langgraph/reference/store/#langgraph.store.base.Item))，具有某些属性。我们可以通过`.dict`转换将其作为字典访问，如上所示。\n\n它具有的属性是：\n\n* `value`: 该记忆的值（本身是一个字典）\n* `key`: 该记忆在此命名空间中的唯一键\n* `namespace`: 一个字符串列表，该记忆类型的命名空间\n* `created_at`: 该记忆被创建时的时间戳\n* `updated_at`: 该记忆被更新时的时间戳\n\n### 语义搜索[¶](#semantic-search "Permanent link")\n\n除了简单的检索，存储还支持语义搜索，允许您根据含义而不是精确匹配来查找记忆。要启用此功能，请使用嵌入模型配置存储：\n\n*API 参考：[init\\_embeddings](https://python.langchain.ac.cn/api_reference/langchain/embeddings/langchain.embeddings.base.init_embeddings.html)*\n\n```\nfrom langchain.embeddings import init_embeddings from  langchain.embeddings  import init_embeddings  store = InMemoryStore( store = InMemoryStore( index={ index ={ "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "dims": 1536, # Embedding dimensions "dims": 1536, # Embedding dimensions "fields": ["food_preference", "$"] # Fields to embed "fields":["food_preference", "$"] # Fields to embed } }) )\n```\n\n现在搜索时，您可以使用自然语言查询来查找相关的记忆：\n\n```\n # Find memories about food preferences # Find memories about food preferences# (This can be done after putting memories into the store) # (This can be done after putting memories into the store)memories = store.search( memories = store. search( namespace_for_memory, namespace_for_memory, query="What does the user like to eat?", query = "What does the user like to eat?", limit=3 # Return top 3 matches limit = 3 # Return top 3 matches) )\n```\n\n您可以通过配置`fields`参数或在存储记忆时指定`index`参数来控制记忆的哪些部分被嵌入。\n\n```\n # Store with specific fields to embed # Store with specific fields to embedstore.put( store. put( namespace_for_memory, namespace_for_memory, str(uuid.uuid4()), str(uuid. uuid4()), { { "food_preference": "I love Italian cuisine", "food_preference": "I love Italian cuisine", "context": "Discussing dinner plans" "context": "Discussing dinner plans" }, }, index=["food_preference"] # Only embed "food_preferences" field index =["food_preference"] # Only embed "food_preferences" field) )  # Store without embedding (still retrievable, but not searchable) # Store without embedding (still retrievable, but not searchable)store.put( store. put( namespace_for_memory, namespace_for_memory, str(uuid.uuid4()), str(uuid. uuid4()), {"system_info": "Last updated: 2024-01-01"}, {"system_info":"Last updated: 2024-01-01"}, index=False index = False) )\n```\n\n### 在 LangGraph 中使用[¶](#using-in-langgraph "Permanent link")\n\n有了这些，我们就可以在 LangGraph 中使用`in_memory_store`了。`in_memory_store`与检查点协同工作：检查点将状态保存到线程中（如上所述），而`in_memory_store`允许我们存储任意信息以便在*多个*线程之间访问。我们用检查点和`in_memory_store`两者来编译图，如下所示。\n\n*API 参考：[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)*\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaver  # We need this because we want to enable threads (conversations) # We need this because we want to enable threads (conversations)checkpointer = InMemorySaver() checkpointer = InMemorySaver()  # ... Define the graph ... # ... Define the graph ...  # Compile the graph with the checkpointer and store # Compile the graph with the checkpointer and storegraph = graph.compile(checkpointer=checkpointer, store=in_memory_store) graph = graph. compile(checkpointer = checkpointer, store = in_memory_store)\n```\n\n我们像以前一样用`thread_id`调用图，同时也用`user_id`，我们将用它来将我们的记忆命名空间限定到这个特定用户，如上所示。\n\n```\n # Invoke the graph # Invoke the graphuser_id = "1" user_id = "1"config = {"configurable": {"thread_id": "1", "user_id": user_id}} config ={"configurable":{"thread_id": "1", "user_id": user_id}}  # First let\'s just say hi to the AI # First let\'s just say hi to the AIfor update in graph.stream( for update in graph. stream( {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates" {"messages":[{"role": "user", "content": "hi"}]}, config, stream_mode = "updates"): ): print(update) print(update)\n```\n\n我们可以通过将`store: BaseStore`和`config: RunnableConfig`作为节点参数，在*任何节点*中访问`in_memory_store`和`user_id`。以下是我们如何在节点中使用语义搜索来查找相关记忆：\n\n```\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore): def  update_memory(state: MessagesState, config: RunnableConfig,*, store: BaseStore):   # Get the user id from the config # Get the user id from the config user_id = config["configurable"]["user_id"] user_id = config["configurable"]["user_id"]   # Namespace the memory # Namespace the memory namespace = (user_id, "memories") namespace =(user_id, "memories")   # ... Analyze conversation and create a new memory # ... Analyze conversation and create a new memory   # Create a new memory ID # Create a new memory ID memory_id = str(uuid.uuid4()) memory_id = str(uuid. uuid4())   # We create a new memory # We create a new memory store.put(namespace, memory_id, {"memory": memory}) store. put(namespace, memory_id,{"memory": memory})\n```\n\n如上所示，我们也可以在任何节点中访问存储并使用`store.search`方法来获取记忆。回想一下，记忆是作为可以转换为字典的对象列表返回的。\n\n```\nmemories[-1].dict() memories[- 1]. dict(){\'value\': {\'food_preference\': \'I like pizza\'}, {\'value\':{\'food_preference\': \'I like pizza\'}, \'key\': \'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'namespace\': [\'1\', \'memories\'], \'namespace\':[\'1\', \'memories\'], \'created_at\': \'2024-10-02T17:22:31.590602+00:00\', \'created_at\':\'2024-10-02T17:22:31.590602+00:00\', \'updated_at\': \'2024-10-02T17:22:31.590605+00:00\'} \'updated_at\':\'2024-10-02T17:22:31.590605+00:00\'}\n```\n\n我们可以访问记忆并在我们的模型调用中使用它们。\n\n```\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore): def  call_model(state: MessagesState, config: RunnableConfig,*, store: BaseStore):  # Get the user id from the config # Get the user id from the config user_id = config["configurable"]["user_id"] user_id = config["configurable"]["user_id"]   # Namespace the memory # Namespace the memory namespace = (user_id, "memories") namespace =(user_id, "memories")   # Search based on the most recent message # Search based on the most recent message memories = store.search( memories = store. search( namespace, namespace, query=state["messages"][-1].content, query = state["messages"][- 1]. content, limit=3 limit = 3 ) ) info = "\\n".join([d.value["memory"] for d in memories]) info = " \\n ". join([d. value["memory"] for d in memories])   # ... Use memories in the model call # ... Use memories in the model call\n```\n\n如果我们创建一个新线程，只要`user_id`相同，我们仍然可以访问相同的记忆。\n\n```\n # Invoke the graph # Invoke the graphconfig = {"configurable": {"thread_id": "2", "user_id": "1"}} config ={"configurable":{"thread_id": "2", "user_id": "1"}}  # Let\'s say hi again # Let\'s say hi againfor update in graph.stream( for update in graph. stream( {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates" {"messages":[{"role": "user", "content":"hi, tell me about my memories"}]}, config, stream_mode = "updates"): ): print(update) print(update)\n```\n\n当我们使用 LangGraph 平台时，无论是在本地（例如，在 LangGraph Studio 中）还是在 LangGraph 平台上，基础存储默认可用，无需在图编译期间指定。然而，要启用语义搜索，您**确实**需要在您的`langgraph.json`文件中配置索引设置。例如：\n\n```\n{ { ...  ... "store": {  "store":  { "index": {  "index":  { "embed": "openai:text-embeddings-3-small",  "embed":  "openai:text-embeddings-3-small", "dims": 1536,  "dims":  1536, "fields": ["$"]  "fields":  ["$"] }  } }  }} }\n```\n\n更多详情和配置选项，请参见[部署指南](../../cloud/deployment/semantic_search/)。\n\n## 检查点库[¶](#checkpointer-libraries "Permanent link")\n\n在底层，检查点功能由符合[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)接口的检查点对象提供支持。LangGraph 提供了几种检查点实现，都是通过独立、可安装的库实现的：\n\n* `langgraph-checkpoint`: 检查点保存器（[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)）和序列化/反序列化接口（[SerializerProtocol](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)）的基础接口。包括用于实验的内存检查点实现（[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)）。LangGraph 自带`langgraph-checkpoint`。\n* `langgraph-checkpoint-sqlite`: 使用 SQLite 数据库的 LangGraph 检查点实现（[SqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [AsyncSqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)）。非常适合实验和本地工作流。需要单独安装。\n* `langgraph-checkpoint-postgres`: 使用 Postgres 数据库的高级检查点（[PostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [AsyncPostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)），在 LangGraph 平台中使用。非常适合在生产环境中使用。需要单独安装。\n\n### 检查点接口[¶](#checkpointer-interface "Permanent link")\n\n每个检查点都符合[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)接口，并实现以下方法：\n\n* `.put` - 存储一个检查点及其配置和元数据。\n* `.put_writes` - 存储与一个检查点相关联的中间写入（即[待定写入](#pending-writes)）。\n* `.get_tuple` - 使用给定的配置（`thread_id`和`checkpoint_id`）获取一个检查点元组。这用于在`graph.get_state()`中填充`StateSnapshot`。\n* `.list` - 列出匹配给定配置和筛选条件的检查点。这用于在`graph.get_state_history()`中填充状态历史。\n\n如果检查点用于异步图执行（即通过`.ainvoke`、`.astream`、`.abatch`执行图），则将使用上述方法的异步版本（`.aput`、`.aput_writes`、`.aget_tuple`、`.alist`）。\n\n注意\n\n为了异步运行您的图，您可以使用`InMemorySaver`，或 Sqlite/Postgres 检查点的异步版本——`AsyncSqliteSaver` / `AsyncPostgresSaver`检查点。\n\n### 序列化器[¶](#serializer "Permanent link")\n\n当检查点保存图状态时，它们需要序列化状态中的通道值。这是使用序列化器对象完成的。\n\n`langgraph_checkpoint`定义了实现序列化器的[协议](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)，并提供了一个默认实现（[JsonPlusSerializer](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)），该实现处理多种类型，包括 LangChain 和 LangGraph 的原生类型、日期时间、枚举等。\n\n#### 使用`pickle`进行序列化[¶](#serialization-with-pickle "Permanent link")\n\n默认的序列化器[`JsonPlusSerializer`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)在底层使用 ormsgpack 和 JSON，这不适用于所有类型的对象。\n\n如果您想对我们 msgpack 编码器目前不支持的对象（如 Pandas 数据帧）回退到使用 pickle，您可以使用`JsonPlusSerializer`的`pickle_fallback`参数。\n\n*API 参考：[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) | [JsonPlusSerializer](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)*\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaverfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer from  langgraph.checkpoint.serde.jsonplus  import JsonPlusSerializer  # ... Define the graph ... # ... Define the graph ...graph.compile( graph. compile( checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True)) checkpointer = InMemorySaver(serde = JsonPlusSerializer(pickle_fallback = True))) )\n```\n\n#### 加密[¶](#encryption "Permanent link")\n\n检查点可以有选择地加密所有持久化的状态。要启用此功能，请将[`EncryptedSerializer`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer)的实例传递给任何`BaseCheckpointSaver`实现的`serde`参数。创建加密序列化器的最简单方法是通过[`from_pycryptodome_aes`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes)，它从`LANGGRAPH_AES_KEY`环境变量中读取 AES 密钥（或接受一个`key`参数）。\n\n*API 参考：[SqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver)*\n\n```\n import sqlite3 import  sqlite3  from langgraph.checkpoint.serde.encrypted import EncryptedSerializer from  langgraph.checkpoint.serde.encrypted  import EncryptedSerializerfrom langgraph.checkpoint.sqlite import SqliteSaver from  langgraph.checkpoint.sqlite  import SqliteSaver  serde = EncryptedSerializer.from_pycryptodome_aes() # reads LANGGRAPH_AES_KEY serde = EncryptedSerializer. from_pycryptodome_aes() # reads LANGGRAPH_AES_KEYcheckpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde) checkpointer = SqliteSaver(sqlite3. connect("checkpoint.db"), serde = serde)\n```\n\n*API 参考：[PostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver)*\n\n```\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer from  langgraph.checkpoint.serde.encrypted  import EncryptedSerializerfrom langgraph.checkpoint.postgres import PostgresSaver from  langgraph.checkpoint.postgres  import PostgresSaver  serde = EncryptedSerializer.from_pycryptodome_aes() serde = EncryptedSerializer. from_pycryptodome_aes()checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde) checkpointer = PostgresSaver. from_conn_string("postgresql://...", serde = serde)checkpointer.setup() checkpointer. setup()\n```\n\n在 LangGraph 平台上运行时，只要存在`LANGGRAPH_AES_KEY`，加密就会自动启用，因此您只需提供环境变量即可。可以通过实现[`CipherProtocol`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol)并将其提供给`EncryptedSerializer`来使用其他加密方案。\n\n## 功能[¶](#capabilities "Permanent link")\n\n### 人机协作 (Human-in-the-loop)[¶](#human-in-the-loop "Permanent link")\n\n首先，检查点通过允许人类检查、中断和批准图步骤，促进了[人在环路的工作流](../agentic_concepts/#human-in-the-loop)。这些工作流需要检查点，因为人类必须能够随时查看图的状态，并且图必须能够在人类对状态进行任何更新后恢复执行。请参阅[操作指南](../../how-tos/human_in_the_loop/add-human-in-the-loop/)中的示例。\n\n### 内存[¶](#memory "Permanent link")\n\n其次，检查点允许在交互之间建立[“记忆”](../memory/)。在重复的人类交互（如对话）的情况下，任何后续消息都可以发送到该线程，该线程将保留其先前交互的记忆。有关如何使用检查点添加和管理对话记忆的信息，请参见[添加记忆](../../how-tos/memory/add-memory/)。\n\n### 时光穿梭[¶](#time-travel "Permanent link")\n\n第三，检查点允许[“时间旅行”](../time-travel/)，允许用户重放先前的图执行，以审查和/或调试特定的图步骤。此外，检查点还可以在任意检查点分叉图状态，以探索不同的轨迹。\n\n### 容错[¶](#fault-tolerance "Permanent link")\n\n最后，检查点还提供了容错和错误恢复功能：如果一个或多个节点在给定的超级步骤中失败，您可以从最后一个成功的步骤重新启动您的图。此外，当一个图节点在给定的超级步骤中执行失败时，LangGraph 会存储在该超级步骤中成功完成的任何其他节点的待定检查点写入，这样每当我们从该超级步骤恢复图执行时，我们就不必重新运行成功的节点。\n\n#### 待定写入[¶](#pending-writes "Permanent link")\n\n此外，当一个图节点在给定的超级步骤中执行失败时，LangGraph 会存储在该超级步骤中成功完成的任何其他节点的待定检查点写入，这样每当我们从该超级步骤恢复图执行时，我们就不必重新运行成功的节点。', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraph/concepts/persistence/', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.9999943, 'saved_path': None}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:18,081 - __main__ - INFO - handle_download: searcher=TavilySearch, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:18,082 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:18,082 - __main__ - INFO - call_tool payload: source_tool=tavily_download, result_type=papers, count=1
2026-02-22 20:38:18,082 - __main__ - INFO - call_tool: name=tavily_download, result_type=papers, count=1
2026-02-22 20:38:18,082 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': 'LangGraph 持久化 - 概述- LangChain 教程', 'authors': [], 'abstract': '* [线程](#threads)\n* [检查点](#checkpoints) \n\n  + [获取状态](#get-state)\n  + [获取状态历史](#get-state-history)\n  + [重放](#replay)\n  + [更新状态](#update-state) \n\n    - [config](#config)\n    - [values](#values)\n    - [as\\_node](#as_node)\n* [记忆存储](#memory-store) \n\n  + [基本用法](#basic-usage)\n  + [语义搜索](#semantic-search)\n  + [在 LangGraph 中使用](#using-in-langgraph)\n* [检查点库](#checkpointer-libraries) \n\n  + [检查点接口](#checkpointer-interface)\n  + [序列化器](#serializer) \n\n    - [使用 pickle 进行序列化](#serialization-with-pickle)\n    - [加密](#encryption)\n* [功能](#capabilities) \n\n  + [人工干预](#human-in-the-loop)\n  + [记忆](#memory)\n  + [时光穿梭](#time-travel)\n  + [容错](#fault-tolerance) \n\n    - [待定写入](#pending-writes)\n\n# 持久性[¶](#persistence "Permanent link")\n\nLangGraph 有一个内置的持久化层，通过检查点（checkpointer）实现。当您使用检查点编译一个图时，检查点会在每个超级步骤（super-step）保存图状态的`checkpoint`。这些检查点被保存到一个`thread`（线程）中，可以在图执行后访问。因为`threads`允许在执行后访问图的状态，所以可以实现诸如人在环路、记忆、时间旅行和容错等多种强大功能。下面，我们将更详细地讨论这些概念。\n\nLangGraph API 自动处理检查点\n\n使用 LangGraph API 时，您无需手动实现或配置检查点。API 在后台为您处理所有持久化基础架构。\n\n## 线程[¶](#threads "Permanent link")\n\n线程（thread）是由检查点保存的每个检查点的唯一 ID 或线程标识符。它包含一系列[运行](../assistants/#execution)的累积状态。当一次运行被执行时，助手的底层图的[状态](../low_level/#state)将被持久化到该线程中。\n\n当使用检查点调用一个图时，您**必须**在配置的`configurable`部分指定一个`thread_id`。\n\n```\n{"configurable": {"thread_id": "1"}} {"configurable":{"thread_id": "1"}}\n```\n\n可以检索一个线程的当前和历史状态。为了持久化状态，必须在执行一次运行之前创建一个线程。LangGraph 平台 API 提供了几个用于创建和管理线程及线程状态的端点。更多详情请参见[API 参考](../../cloud/reference/api/api_ref.html#tag/threads)。\n\n## 检查点[¶](#checkpoints "Permanent link")\n\n一个线程在特定时间点的状态称为检查点（checkpoint）。检查点是在每个超级步骤保存的图状态的快照，由一个`StateSnapshot`对象表示，具有以下关键属性：\n\n* `config`: 与此检查点关联的配置。\n* `metadata`: 与此检查点关联的元数据。\n* `values`: 在此时间点的状态通道的值。\n* `next`: 一个元组，包含图中接下来要执行的节点名称。\n* `tasks`: 一个`PregelTask`对象的元组，包含有关接下来要执行的任务的信息。如果该步骤之前尝试过，它将包含错误信息。如果图在节点内部被[动态](../../how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt)中断，任务将包含与中断相关的额外数据。\n\n检查点是持久化的，可以用来在稍后恢复线程的状态。\n\n让我们看看当一个简单的图被如下调用时，会保存哪些检查点：\n\n*API 参考：[StateGraph](https://github.langchain.ac.cn/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://github.langchain.ac.cn/langgraph/reference/constants/#langgraph.constants.START) | [END](https://github.langchain.ac.cn/langgraph/reference/constants/#langgraph.constants.END) | [InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)*\n\n```\nfrom langgraph.graph import StateGraph, START, END from  langgraph.graph  import StateGraph, START, ENDfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaver from typing import Annotated from  typing  import Annotated from typing_extensions import TypedDict from  typing_extensions  import TypedDict from operator import add from  operator  import add  class State(TypedDict): class  State(TypedDict): foo: str foo: str bar: Annotated[list[str], add] bar: Annotated[list[str], add]  def node_a(state: State): def  node_a(state: State): return {"foo": "a", "bar": ["a"]} return{"foo": "a", "bar":["a"]}  def node_b(state: State): def  node_b(state: State): return {"foo": "b", "bar": ["b"]} return{"foo": "b", "bar":["b"]}   workflow = StateGraph(State) workflow = StateGraph(State)workflow.add_node(node_a) workflow. add_node(node_a)workflow.add_node(node_b) workflow. add_node(node_b)workflow.add_edge(START, "node_a") workflow. add_edge(START, "node_a")workflow.add_edge("node_a", "node_b") workflow. add_edge("node_a", "node_b")workflow.add_edge("node_b", END) workflow. add_edge("node_b", END)  checkpointer = InMemorySaver() checkpointer = InMemorySaver()graph = workflow.compile(checkpointer=checkpointer) graph = workflow. compile(checkpointer = checkpointer)  config = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}graph.invoke({"foo": ""}, config) graph. invoke({"foo": ""}, config)\n```\n\n在我们运行图之后，我们期望看到正好 4 个检查点：\n\n* 一个空的检查点，以`START`作为下一个要执行的节点。\n* 一个包含用户输入`{\'foo\': \'\', \'bar\': []}`的检查点，以`node_a`作为下一个要执行的节点。\n* 一个包含`node_a`输出`{\'foo\': \'a\', \'bar\': [\'a\']}`的检查点，以`node_b`作为下一个要执行的节点。\n* 一个包含`node_b`输出`{\'foo\': \'b\', \'bar\': [\'a\', \'b\']}`的检查点，没有下一个要执行的节点。\n\n请注意，`bar`通道的值包含了两个节点的输出，因为我们为`bar`通道定义了一个归约器（reducer）。\n\n### 获取状态[¶](#get-state "Permanent link")\n\n当与保存的图状态进行交互时，您**必须**指定一个[线程标识符](#threads)。您可以通过调用`graph.get_state(config)`来查看图的*最新*状态。这将返回一个`StateSnapshot`对象，它对应于配置中提供的线程 ID 的最新检查点，或者（如果提供了）对应于该线程的某个检查点 ID 的检查点。\n\n```\n # get the latest state snapshot # get the latest state snapshotconfig = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}graph.get_state(config) graph. get_state(config)  # get a state snapshot for a specific checkpoint_id # get a state snapshot for a specific checkpoint_idconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}} config ={"configurable":{"thread_id": "1", "checkpoint_id":"1ef663ba-28fe-6528-8002-5a559208592c"}}graph.get_state(config) graph. get_state(config)\n```\n\n在我们的例子中，`get_state`的输出将如下所示：\n\n```\nStateSnapshot(  values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},  next=(),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},  created_at=\'2024-08-29T19:19:38.821749+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}}, tasks=() ) \n```\n\n### 获取状态历史[¶](#get-state-history "Permanent link")\n\n您可以通过调用`graph.get_state_history(config)`来获取给定线程的完整图执行历史。这将返回一个与配置中提供的线程 ID 相关联的`StateSnapshot`对象列表。重要的是，检查点将按时间顺序排列，最新的检查点 / `StateSnapshot`位于列表的第一个。\n\n```\nconfig = {"configurable": {"thread_id": "1"}} config ={"configurable":{"thread_id": "1"}}list(graph.get_state_history(config)) list(graph. get_state_history(config))\n```\n\n在我们的例子中，`get_state_history`的输出将如下所示：\n\n```\n[  StateSnapshot(  values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},  next=(),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},  created_at=\'2024-08-29T19:19:38.821749+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},  tasks=(),  ),  StateSnapshot(  values={\'foo\': \'a\', \'bar\': [\'a\']}, next=(\'node_b\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},  metadata={\'source\': \'loop\', \'writes\': {\'node_a\': {\'foo\': \'a\', \'bar\': [\'a\']}}, \'step\': 1},  created_at=\'2024-08-29T19:19:38.819946+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},  tasks=(PregelTask(id=\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\', name=\'node_b\', error=None, interrupts=()),),  ),  StateSnapshot(  values={\'foo\': \'\', \'bar\': []},  next=(\'node_a\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},  metadata={\'source\': \'loop\', \'writes\': None, \'step\': 0},  created_at=\'2024-08-29T19:19:38.817813+00:00\',  parent_config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},  tasks=(PregelTask(id=\'f1b14528-5ee5-579c-949b-23ef9bfbed58\', name=\'node_a\', error=None, interrupts=()),),  ),  StateSnapshot(  values={\'bar\': []},  next=(\'__start__\',),  config={\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},  metadata={\'source\': \'input\', \'writes\': {\'foo\': \'\'}, \'step\': -1},  created_at=\'2024-08-29T19:19:38.816205+00:00\',  parent_config=None,  tasks=(PregelTask(id=\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\', name=\'__start__\', error=None, interrupts=()),),  ) ] \n```\n\n### 重放[¶](#replay "Permanent link")\n\n也可以回放之前的图执行。如果我们使用一个`thread_id`和一个`checkpoint_id`来`invoke`一个图，那么我们将*重放*与`checkpoint_id`对应的检查点*之前*已执行的步骤，而只执行该检查点*之后*的步骤。\n\n* `thread_id`是线程的 ID。\n* `checkpoint_id`是引用线程内特定检查点的标识符。\n\n您必须在调用图时将这些作为配置的`configurable`部分传递。\n\n```\nconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}} config ={"configurable":{"thread_id": "1", "checkpoint_id":"0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}graph.invoke(None, config=config) graph. invoke(None, config = config)\n```\n\n重要的是，LangGraph 知道某个特定步骤之前是否已执行过。如果是，LangGraph 只是*重放*图中该特定步骤，而不会重新执行它，但这仅限于所提供的`checkpoint_id`*之前*的步骤。所有在`checkpoint_id`*之后*的步骤都将被执行（即一个新的分支），即使它们之前已经执行过。请参阅关于[时间旅行的这份操作指南，以了解更多关于重放的信息](../../how-tos/human_in_the_loop/time-travel/)。\n\n### 更新状态[¶](#update-state "永久链接")\n\n除了从特定的`checkpoints`重放图之外，我们还可以*编辑*图状态。我们使用`graph.update_state()`来做到这一点。该方法接受三个不同的参数：\n\n#### `config`[¶](#config "Permanent link")\n\n配置应包含`thread_id`，指定要更新哪个线程。当只传递`thread_id`时，我们更新（或分叉）当前状态。或者，如果我们包含`checkpoint_id`字段，那么我们将分叉该选定的检查点。\n\n#### `values`[¶](#values "Permanent link")\n\n这些是用于更新状态的值。请注意，此更新的处理方式与来自节点的任何更新完全相同。这意味着这些值将被传递给[归约器](../low_level/#reducers)函数（如果为图状态中的某些通道定义了归约器）。这意味着`update_state`不会自动覆盖每个通道的值，而只会覆盖没有归约器的通道。让我们看一个例子。\n\n假设您使用以下模式定义了图的状态（参见上面的完整示例）：\n\n```\n from typing import Annotated from  typing  import Annotated from typing_extensions import TypedDict from  typing_extensions  import TypedDict from operator import add from  operator  import add  class State(TypedDict): class  State(TypedDict): foo: int foo: int bar: Annotated[list[str], add] bar: Annotated[list[str], add]\n```\n\n现在假设图的当前状态是：\n\n```\n{"foo": 1, "bar": ["a"]} \n```\n\n如果您如下更新状态：\n\n```\ngraph.update_state(config, {"foo": 2, "bar": ["b"]}) graph. update_state(config,{"foo": 2, "bar":["b"]})\n```\n\n那么图的新状态将是：\n\n```\n{"foo": 2, "bar": ["a", "b"]} \n```\n\n`foo`键（通道）被完全更改（因为没有为该通道指定归约器，所以`update_state`会覆盖它）。然而，为`bar`键指定了一个归约器，所以它将`"b"`附加到`bar`的状态中。\n\n#### `as_node`[¶](#as_node "Permanent link")\n\n在调用`update_state`时，您可以选择性地指定最后一个参数是`as_node`。如果您提供了它，更新将像来自节点`as_node`一样被应用。如果未提供`as_node`，它将被设置为最后更新状态的节点（如果不模糊）。这之所以重要，是因为下一步要执行的步骤取决于最后给出更新的节点，因此这可以用来控制接下来执行哪个节点。请参阅关于[分叉状态的这份操作指南，以了解更多信息](../../how-tos/human_in_the_loop/time-travel/)。\n\n## 记忆存储[¶](#memory-store "Permanent link")\n\n一个[状态模式](../low_level/#schema)指定了一组在图执行过程中被填充的键。如上所述，状态可以由检查点在每个图步骤写入线程，从而实现状态持久化。\n\n但是，如果我们想在*多个线程之间*保留一些信息怎么办？考虑一个聊天机器人的案例，我们希望在与该用户的所有聊天对话（即线程）中保留关于该用户的特定信息！\n\n仅使用检查点，我们无法在线程之间共享信息。这就需要[`Store`](../../reference/store/#langgraph.store.base.BaseStore)接口。作为示例，我们可以定义一个`InMemoryStore`来跨线程存储关于用户的信息。我们像以前一样用检查点编译我们的图，并使用我们新的`in_memory_store`变量。\n\nLangGraph API 自动处理存储\n\n当使用 LangGraph API 时，您无需手动实现或配置存储。API 在后台为您处理所有存储基础架构。\n\n### 基本用法[¶](#basic-usage "Permanent link")\n\n首先，让我们在不使用 LangGraph 的情况下单独展示这一点。\n\n```\nfrom langgraph.store.memory import InMemoryStore from  langgraph.store.memory  import InMemoryStorein_memory_store = InMemoryStore() in_memory_store = InMemoryStore()\n```\n\n记忆由一个`tuple`进行命名空间划分，在这个特定示例中是`(, "memories")`。命名空间可以是任何长度，可以代表任何东西，不一定是特定于用户的。\n\n```\nuser_id = "1" user_id = "1"namespace_for_memory = (user_id, "memories") namespace_for_memory =(user_id, "memories")\n```\n\n我们使用`store.put`方法将记忆保存到存储中的命名空间。当我们这样做时，我们指定如上定义的命名空间，以及记忆的键值对：键是记忆的唯一标识符（`memory_id`），值（一个字典）是记忆本身。\n\n```\nmemory_id = str(uuid.uuid4()) memory_id = str(uuid. uuid4())memory = {"food_preference" : "I like pizza"} memory ={"food_preference": "I like pizza"}in_memory_store.put(namespace_for_memory, memory_id, memory) in_memory_store. put(namespace_for_memory, memory_id, memory)\n```\n\n我们可以使用`store.search`方法读出我们命名空间中的记忆，它将返回给定用户的所有记忆作为一个列表。最新的记忆是列表中的最后一个。\n\n```\nmemories = in_memory_store.search(namespace_for_memory) memories = in_memory_store. search(namespace_for_memory)memories[-1].dict() memories[- 1]. dict(){\'value\': {\'food_preference\': \'I like pizza\'}, {\'value\':{\'food_preference\': \'I like pizza\'}, \'key\': \'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'namespace\': [\'1\', \'memories\'], \'namespace\':[\'1\', \'memories\'], \'created_at\': \'2024-10-02T17:22:31.590602+00:00\', \'created_at\':\'2024-10-02T17:22:31.590602+00:00\', \'updated_at\': \'2024-10-02T17:22:31.590605+00:00\'} \'updated_at\':\'2024-10-02T17:22:31.590605+00:00\'}\n```\n\n每种记忆类型都是一个 Python 类 ([`Item`](https://github.langchain.ac.cn/langgraph/reference/store/#langgraph.store.base.Item))，具有某些属性。我们可以通过`.dict`转换将其作为字典访问，如上所示。\n\n它具有的属性是：\n\n* `value`: 该记忆的值（本身是一个字典）\n* `key`: 该记忆在此命名空间中的唯一键\n* `namespace`: 一个字符串列表，该记忆类型的命名空间\n* `created_at`: 该记忆被创建时的时间戳\n* `updated_at`: 该记忆被更新时的时间戳\n\n### 语义搜索[¶](#semantic-search "Permanent link")\n\n除了简单的检索，存储还支持语义搜索，允许您根据含义而不是精确匹配来查找记忆。要启用此功能，请使用嵌入模型配置存储：\n\n*API 参考：[init\\_embeddings](https://python.langchain.ac.cn/api_reference/langchain/embeddings/langchain.embeddings.base.init_embeddings.html)*\n\n```\nfrom langchain.embeddings import init_embeddings from  langchain.embeddings  import init_embeddings  store = InMemoryStore( store = InMemoryStore( index={ index ={ "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "dims": 1536, # Embedding dimensions "dims": 1536, # Embedding dimensions "fields": ["food_preference", "$"] # Fields to embed "fields":["food_preference", "$"] # Fields to embed } }) )\n```\n\n现在搜索时，您可以使用自然语言查询来查找相关的记忆：\n\n```\n # Find memories about food preferences # Find memories about food preferences# (This can be done after putting memories into the store) # (This can be done after putting memories into the store)memories = store.search( memories = store. search( namespace_for_memory, namespace_for_memory, query="What does the user like to eat?", query = "What does the user like to eat?", limit=3 # Return top 3 matches limit = 3 # Return top 3 matches) )\n```\n\n您可以通过配置`fields`参数或在存储记忆时指定`index`参数来控制记忆的哪些部分被嵌入。\n\n```\n # Store with specific fields to embed # Store with specific fields to embedstore.put( store. put( namespace_for_memory, namespace_for_memory, str(uuid.uuid4()), str(uuid. uuid4()), { { "food_preference": "I love Italian cuisine", "food_preference": "I love Italian cuisine", "context": "Discussing dinner plans" "context": "Discussing dinner plans" }, }, index=["food_preference"] # Only embed "food_preferences" field index =["food_preference"] # Only embed "food_preferences" field) )  # Store without embedding (still retrievable, but not searchable) # Store without embedding (still retrievable, but not searchable)store.put( store. put( namespace_for_memory, namespace_for_memory, str(uuid.uuid4()), str(uuid. uuid4()), {"system_info": "Last updated: 2024-01-01"}, {"system_info":"Last updated: 2024-01-01"}, index=False index = False) )\n```\n\n### 在 LangGraph 中使用[¶](#using-in-langgraph "Permanent link")\n\n有了这些，我们就可以在 LangGraph 中使用`in_memory_store`了。`in_memory_store`与检查点协同工作：检查点将状态保存到线程中（如上所述），而`in_memory_store`允许我们存储任意信息以便在*多个*线程之间访问。我们用检查点和`in_memory_store`两者来编译图，如下所示。\n\n*API 参考：[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)*\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaver  # We need this because we want to enable threads (conversations) # We need this because we want to enable threads (conversations)checkpointer = InMemorySaver() checkpointer = InMemorySaver()  # ... Define the graph ... # ... Define the graph ...  # Compile the graph with the checkpointer and store # Compile the graph with the checkpointer and storegraph = graph.compile(checkpointer=checkpointer, store=in_memory_store) graph = graph. compile(checkpointer = checkpointer, store = in_memory_store)\n```\n\n我们像以前一样用`thread_id`调用图，同时也用`user_id`，我们将用它来将我们的记忆命名空间限定到这个特定用户，如上所示。\n\n```\n # Invoke the graph # Invoke the graphuser_id = "1" user_id = "1"config = {"configurable": {"thread_id": "1", "user_id": user_id}} config ={"configurable":{"thread_id": "1", "user_id": user_id}}  # First let\'s just say hi to the AI # First let\'s just say hi to the AIfor update in graph.stream( for update in graph. stream( {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates" {"messages":[{"role": "user", "content": "hi"}]}, config, stream_mode = "updates"): ): print(update) print(update)\n```\n\n我们可以通过将`store: BaseStore`和`config: RunnableConfig`作为节点参数，在*任何节点*中访问`in_memory_store`和`user_id`。以下是我们如何在节点中使用语义搜索来查找相关记忆：\n\n```\ndef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore): def  update_memory(state: MessagesState, config: RunnableConfig,*, store: BaseStore):   # Get the user id from the config # Get the user id from the config user_id = config["configurable"]["user_id"] user_id = config["configurable"]["user_id"]   # Namespace the memory # Namespace the memory namespace = (user_id, "memories") namespace =(user_id, "memories")   # ... Analyze conversation and create a new memory # ... Analyze conversation and create a new memory   # Create a new memory ID # Create a new memory ID memory_id = str(uuid.uuid4()) memory_id = str(uuid. uuid4())   # We create a new memory # We create a new memory store.put(namespace, memory_id, {"memory": memory}) store. put(namespace, memory_id,{"memory": memory})\n```\n\n如上所示，我们也可以在任何节点中访问存储并使用`store.search`方法来获取记忆。回想一下，记忆是作为可以转换为字典的对象列表返回的。\n\n```\nmemories[-1].dict() memories[- 1]. dict(){\'value\': {\'food_preference\': \'I like pizza\'}, {\'value\':{\'food_preference\': \'I like pizza\'}, \'key\': \'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\', \'namespace\': [\'1\', \'memories\'], \'namespace\':[\'1\', \'memories\'], \'created_at\': \'2024-10-02T17:22:31.590602+00:00\', \'created_at\':\'2024-10-02T17:22:31.590602+00:00\', \'updated_at\': \'2024-10-02T17:22:31.590605+00:00\'} \'updated_at\':\'2024-10-02T17:22:31.590605+00:00\'}\n```\n\n我们可以访问记忆并在我们的模型调用中使用它们。\n\n```\ndef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore): def  call_model(state: MessagesState, config: RunnableConfig,*, store: BaseStore):  # Get the user id from the config # Get the user id from the config user_id = config["configurable"]["user_id"] user_id = config["configurable"]["user_id"]   # Namespace the memory # Namespace the memory namespace = (user_id, "memories") namespace =(user_id, "memories")   # Search based on the most recent message # Search based on the most recent message memories = store.search( memories = store. search( namespace, namespace, query=state["messages"][-1].content, query = state["messages"][- 1]. content, limit=3 limit = 3 ) ) info = "\\n".join([d.value["memory"] for d in memories]) info = " \\n ". join([d. value["memory"] for d in memories])   # ... Use memories in the model call # ... Use memories in the model call\n```\n\n如果我们创建一个新线程，只要`user_id`相同，我们仍然可以访问相同的记忆。\n\n```\n # Invoke the graph # Invoke the graphconfig = {"configurable": {"thread_id": "2", "user_id": "1"}} config ={"configurable":{"thread_id": "2", "user_id": "1"}}  # Let\'s say hi again # Let\'s say hi againfor update in graph.stream( for update in graph. stream( {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates" {"messages":[{"role": "user", "content":"hi, tell me about my memories"}]}, config, stream_mode = "updates"): ): print(update) print(update)\n```\n\n当我们使用 LangGraph 平台时，无论是在本地（例如，在 LangGraph Studio 中）还是在 LangGraph 平台上，基础存储默认可用，无需在图编译期间指定。然而，要启用语义搜索，您**确实**需要在您的`langgraph.json`文件中配置索引设置。例如：\n\n```\n{ { ...  ... "store": {  "store":  { "index": {  "index":  { "embed": "openai:text-embeddings-3-small",  "embed":  "openai:text-embeddings-3-small", "dims": 1536,  "dims":  1536, "fields": ["$"]  "fields":  ["$"] }  } }  }} }\n```\n\n更多详情和配置选项，请参见[部署指南](../../cloud/deployment/semantic_search/)。\n\n## 检查点库[¶](#checkpointer-libraries "Permanent link")\n\n在底层，检查点功能由符合[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)接口的检查点对象提供支持。LangGraph 提供了几种检查点实现，都是通过独立、可安装的库实现的：\n\n* `langgraph-checkpoint`: 检查点保存器（[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)）和序列化/反序列化接口（[SerializerProtocol](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)）的基础接口。包括用于实验的内存检查点实现（[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)）。LangGraph 自带`langgraph-checkpoint`。\n* `langgraph-checkpoint-sqlite`: 使用 SQLite 数据库的 LangGraph 检查点实现（[SqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [AsyncSqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)）。非常适合实验和本地工作流。需要单独安装。\n* `langgraph-checkpoint-postgres`: 使用 Postgres 数据库的高级检查点（[PostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [AsyncPostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)），在 LangGraph 平台中使用。非常适合在生产环境中使用。需要单独安装。\n\n### 检查点接口[¶](#checkpointer-interface "Permanent link")\n\n每个检查点都符合[BaseCheckpointSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)接口，并实现以下方法：\n\n* `.put` - 存储一个检查点及其配置和元数据。\n* `.put_writes` - 存储与一个检查点相关联的中间写入（即[待定写入](#pending-writes)）。\n* `.get_tuple` - 使用给定的配置（`thread_id`和`checkpoint_id`）获取一个检查点元组。这用于在`graph.get_state()`中填充`StateSnapshot`。\n* `.list` - 列出匹配给定配置和筛选条件的检查点。这用于在`graph.get_state_history()`中填充状态历史。\n\n如果检查点用于异步图执行（即通过`.ainvoke`、`.astream`、`.abatch`执行图），则将使用上述方法的异步版本（`.aput`、`.aput_writes`、`.aget_tuple`、`.alist`）。\n\n注意\n\n为了异步运行您的图，您可以使用`InMemorySaver`，或 Sqlite/Postgres 检查点的异步版本——`AsyncSqliteSaver` / `AsyncPostgresSaver`检查点。\n\n### 序列化器[¶](#serializer "Permanent link")\n\n当检查点保存图状态时，它们需要序列化状态中的通道值。这是使用序列化器对象完成的。\n\n`langgraph_checkpoint`定义了实现序列化器的[协议](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)，并提供了一个默认实现（[JsonPlusSerializer](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)），该实现处理多种类型，包括 LangChain 和 LangGraph 的原生类型、日期时间、枚举等。\n\n#### 使用`pickle`进行序列化[¶](#serialization-with-pickle "Permanent link")\n\n默认的序列化器[`JsonPlusSerializer`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)在底层使用 ormsgpack 和 JSON，这不适用于所有类型的对象。\n\n如果您想对我们 msgpack 编码器目前不支持的对象（如 Pandas 数据帧）回退到使用 pickle，您可以使用`JsonPlusSerializer`的`pickle_fallback`参数。\n\n*API 参考：[InMemorySaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) | [JsonPlusSerializer](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)*\n\n```\nfrom langgraph.checkpoint.memory import InMemorySaver from  langgraph.checkpoint.memory  import InMemorySaverfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer from  langgraph.checkpoint.serde.jsonplus  import JsonPlusSerializer  # ... Define the graph ... # ... Define the graph ...graph.compile( graph. compile( checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True)) checkpointer = InMemorySaver(serde = JsonPlusSerializer(pickle_fallback = True))) )\n```\n\n#### 加密[¶](#encryption "Permanent link")\n\n检查点可以有选择地加密所有持久化的状态。要启用此功能，请将[`EncryptedSerializer`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer)的实例传递给任何`BaseCheckpointSaver`实现的`serde`参数。创建加密序列化器的最简单方法是通过[`from_pycryptodome_aes`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes)，它从`LANGGRAPH_AES_KEY`环境变量中读取 AES 密钥（或接受一个`key`参数）。\n\n*API 参考：[SqliteSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver)*\n\n```\n import sqlite3 import  sqlite3  from langgraph.checkpoint.serde.encrypted import EncryptedSerializer from  langgraph.checkpoint.serde.encrypted  import EncryptedSerializerfrom langgraph.checkpoint.sqlite import SqliteSaver from  langgraph.checkpoint.sqlite  import SqliteSaver  serde = EncryptedSerializer.from_pycryptodome_aes() # reads LANGGRAPH_AES_KEY serde = EncryptedSerializer. from_pycryptodome_aes() # reads LANGGRAPH_AES_KEYcheckpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde) checkpointer = SqliteSaver(sqlite3. connect("checkpoint.db"), serde = serde)\n```\n\n*API 参考：[PostgresSaver](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver)*\n\n```\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer from  langgraph.checkpoint.serde.encrypted  import EncryptedSerializerfrom langgraph.checkpoint.postgres import PostgresSaver from  langgraph.checkpoint.postgres  import PostgresSaver  serde = EncryptedSerializer.from_pycryptodome_aes() serde = EncryptedSerializer. from_pycryptodome_aes()checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde) checkpointer = PostgresSaver. from_conn_string("postgresql://...", serde = serde)checkpointer.setup() checkpointer. setup()\n```\n\n在 LangGraph 平台上运行时，只要存在`LANGGRAPH_AES_KEY`，加密就会自动启用，因此您只需提供环境变量即可。可以通过实现[`CipherProtocol`](https://github.langchain.ac.cn/langgraph/reference/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol)并将其提供给`EncryptedSerializer`来使用其他加密方案。\n\n## 功能[¶](#capabilities "Permanent link")\n\n### 人机协作 (Human-in-the-loop)[¶](#human-in-the-loop "Permanent link")\n\n首先，检查点通过允许人类检查、中断和批准图步骤，促进了[人在环路的工作流](../agentic_concepts/#human-in-the-loop)。这些工作流需要检查点，因为人类必须能够随时查看图的状态，并且图必须能够在人类对状态进行任何更新后恢复执行。请参阅[操作指南](../../how-tos/human_in_the_loop/add-human-in-the-loop/)中的示例。\n\n### 内存[¶](#memory "Permanent link")\n\n其次，检查点允许在交互之间建立[“记忆”](../memory/)。在重复的人类交互（如对话）的情况下，任何后续消息都可以发送到该线程，该线程将保留其先前交互的记忆。有关如何使用检查点添加和管理对话记忆的信息，请参见[添加记忆](../../how-tos/memory/add-memory/)。\n\n### 时光穿梭[¶](#time-travel "Permanent link")\n\n第三，检查点允许[“时间旅行”](../time-travel/)，允许用户重放先前的图执行，以审查和/或调试特定的图步骤。此外，检查点还可以在任意检查点分叉图状态，以探索不同的轨迹。\n\n### 容错[¶](#fault-tolerance "Permanent link")\n\n最后，检查点还提供了容错和错误恢复功能：如果一个或多个节点在给定的超级步骤中失败，您可以从最后一个成功的步骤重新启动您的图。此外，当一个图节点在给定的超级步骤中执行失败时，LangGraph 会存储在该超级步骤中成功完成的任何其他节点的待定检查点写入，这样每当我们从该超级步骤恢复图执行时，我们就不必重新运行成功的节点。\n\n#### 待定写入[¶](#pending-writes "Permanent link")\n\n此外，当一个图节点在给定的超级步骤中执行失败时，LangGraph 会存储在该超级步骤中成功完成的任何其他节点的待定检查点写入，这样每当我们从该超级步骤恢复图执行时，我们就不必重新运行成功的节点。', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraph/concepts/persistence/', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.9999943, 'saved_path': '/home/qinshan/widthresearch/data/downloads/tavily_LangGraph .md'}}
2026-02-22 20:38:18,127 - __main__ - INFO - call_tool: name=exa_context_download, args={'papers': [{'paper_id': '', 'title': '持久化 ¶', 'authors': [], 'abstract': '概述 - LangChain 框架[跳过内容] \n**我们正在壮大，并为 LangChain、LangGraph 和LangSmith 招聘多个职位。[加入我们的团队！] **\n[] # 持久化[¶] \nLangGraph 有一个内置的持久化层，通过检查点器实现。当你使用检查点器编译图时，检查点器会在每个超级步骤保存图状态的`checkpoint`。这些检查点保存到`thread`中，可以在图执行后访问。因为`threads`允许在执行后访问图的状态，所以人机协作、记忆、时间旅行和容错等多种强大功能都成为可能。请参阅[此操作指南] ，了解如何在图中添加和使用检查点器的端到端示例。下面，我们将更详细地讨论这些概念。\n![Checkpoints] \nLangGraph API 自动处理检查点使用LangGraph API 时，你无需手动实现或配置检查点器。API 在幕后为你处理所有持久化基础设施。## 线程[¶] \n线程是分配给检查点器保存的每个检查点的唯一ID 或[线程标识符] 。当使用检查点器调用图时，你**必须**在配置的`configurable`部分中指定`thread\\_id`\n```\n`[]<web_link>{"configurable":{"thread\\_id":"1"}}`\n```\n## 检查点[¶] \n检查点是每个超级步骤保存的图状态的快照，由具有以下关键属性的`StateSnapshot`对象表示\n* `config`：与此检查点关联的配置。\n* `metadata`：与此检查点关联的元数据。\n* `values`：此时状态通道的值。\n* `next`：要在图中接下来执行的节点名称的元组。\n* `tasks`：包含要执行的后续任务信息的`PregelTask`对象元组。如果该步骤之前曾尝试过，它将包含错误信息。如果图在节点内部被[动态] 中断，`tasks`将包含与中断相关的附加数据。\n让我们看看以如下方式调用简单图时保存的检查点*API 参考：[StateGraph] |[START] |[END] |[InMemorySaver] *\n```\n`[]<web_link>fromlanggraph.graphimportStateGraph,START,END[]<web_link>fromlanggraph.checkpoint.memoryimportInMemorySaver[]<web_link>fromtypingimportAnnotated[]<web_link>fromtyping\\_extensionsimportTypedDict[]<web_link>fromoperatorimportadd[]<web_link>[]<web_link>classState(TypedDict):[]<web_link>foo:str[]<web_link>bar:Annotated[list[str],add][]<web_link>[]<web_link>defnode\\_a(state:State):[]<web_link>return{"foo":"a","bar":["a"]}[]<web_link>[]<web_link>defnode\\_b(state:State):[]<web_link>return{"foo":"b","bar":["b"]}[]<web_link>[]<web_link>[]<web_link>workflow=StateGraph(State)[]<web_link>workflow.add\\_node(node\\_a)[]<web_link>workflow.add\\_node(node\\_b)[]<web_link>workflow.add\\_edge(START,"node\\_a")[]<web_link>workflow.add\\_edge("node\\_a","node\\_b")[]<web_link>workflow.add\\_edge("node\\_b",END)[]<web_link>[]<web_link>checkpointer=InMemorySaver()[]<web_link>graph=workflow.compile(checkpointer=checkpointer)[]<web_link>[]<web_link>config={"configurable":{"thread\\_id":"1"}}[]<web_link>graph.invoke({"foo":""},config)`\n```\n运行图后，我们预期会看到恰好4 个检查点* 空检查点，其中`START`是下一个要执行的节点\n* 包含用户输入`{\'foo\': \'\', \'bar\': []}`的检查点，`node\\_a`是下一个要执行的节点\n* 包含`node\\_a`输出`{\'foo\': \'a\', \'bar\': [\'a\']}`的检查点，`node\\_b`是下一个要执行的节点\n* 包含`node\\_b`输出`{\'foo\': \'b\', \'bar\': [\'a\', \'b\']}`的检查点，并且没有要执行的下一个节点\n请注意，`bar`通道的值包含来自两个节点的输出，因为我们为`bar`通道设置了 reducer。\n### 获取状态[¶] \n当与保存的图状态交互时，你**必须**指定一个[线程标识符] 。你可以通过调用`graph.get\\_state(config)`来查看图的\\*最新\\*状态。这将返回一个`StateSnapshot`对象，该对象对应于配置中提供的线程 ID 关联的最新检查点，或者如果提供了检查点ID，则返回与该线程的检查点 ID 关联的检查点。```\n`[] # get the latest state snapshot[] config={"configurable":{"thread\\_id":"1"}}[] graph.get\\_state(config)[] [] # get a state snapshot for a specific checkpoint\\_id[] config={"configurable":{"thread\\_id":"1","checkpoint\\_id":"1ef663ba-28fe-6528-8002-5a559208592c"}}[] graph.get\\_state(config)`\n```\n在我们的示例中，`get\\_state`的输出将如下所示\n```\n`[] StateSnapshot([] values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},[] next=(),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},[] created\\_at=\'2024-08-29T19:19:38.821749+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}}, tasks=()[])`\n```\n### 获取状态历史[¶]<web_link>\n你可以通过调用`graph.get\\_state\\_history(config)`获取给定线程的图执行的完整历史记录。这将返回一个与配置中提供的线程 ID 关联的`StateSnapshot`对象列表。重要的是，检查点将按时间顺序排列，最近的检查点 /`StateSnapshot`位于列表的首位。\n```\n`[] config={"configurable":{"thread\\_id":"1"}}[] list(graph.get\\_state\\_history(config))`\n```\n在我们的示例中，`get\\_state\\_history`的输出将如下所示\n```\n`[] [[] StateSnapshot([] values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},[] next=(),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},[] created\\_at=\'2024-08-29T19:19:38.821749+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},[] tasks=(),[]),[] StateSnapshot([] values={\'foo\': \'a\', \'bar\': [\'a\']}, next=(\'node\\_b\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_a\': {\'foo\': \'a\', \'bar\': [\'a\']}}, \'step\': 1},[] created\\_at=\'2024-08-29T19:19:38.819946+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},[] tasks=(PregelTask(id=\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\', name=\'node\\_b\', error=None, interrupts=()),),[]),[] StateSnapshot([] values={\'foo\': \'\', \'bar\': []},[] next=(\'node\\_a\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},[] metadata={\'source\': \'loop\', \'writes\': None, \'step\': 0},[] created\\_at=\'2024-08-29T19:19:38.817813+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},[] tasks=(PregelTask(id=\'f1b14528-5ee5-579c-949b-23ef9bfbed58\', name=\'node\\_a\', error=None, interrupts=()),),[]),[] StateSnapshot([] values={\'bar\': []},[] next=(\'\\_\\_start\\_\\_\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},[] metadata={\'source\': \'input\', \'writes\': {\'foo\': \'\'}, \'step\': -1},[] created\\_at=\'2024-08-29T19:19:38.816205+00:00\',[] parent\\_config=None,[] tasks=(PregelTask(id=\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\', name=\'\\_\\_start\\_\\_\', error=None, interrupts=()),),[])[]]`\n```\n![State]<image_link>\n### 回放[¶]<web_link>\n也可以回放之前的图执行。如果我们使用`thread\\_id`和`checkpoint\\_id`调用图，那么我们将\\*回放\\*`checkpoint\\_id`对应的检查点\\*之前\\*已执行的步骤，并且只执行检查点\\*之后\\*的步骤。\n* `thread\\_id`是线程的 ID。\n* `checkpoint\\_id`是指线程中特定检查点的标识符。\n在调用图时，你必须将这些作为配置的`configurable`部分传入\n```\n`[] config={"configurable":{"thread\\_id":"1","checkpoint\\_id":"0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}[] graph.invoke(None,config=config)`\n```\n重要的是，LangGraph 知道特定步骤是否以前执行过。如果执行过，LangGraph 只会简单地在图中\\*回放\\*该特定步骤，而不会重新执行该步骤，但这仅适用于所提供的`checkpoint\\_id`\\*之前\\*的步骤。`checkpoint\\_id`\\*之后\\*的所有步骤都将被执行（即，一个新的分支），即使它们以前执行过。请参阅此[时间旅行操作指南，了解有关回放的更多信息]<web_link>。\n![Replay]<image_link>\n### 更新状态[¶]<web_link>\n除了从特定的`checkpoints`回放图之外，我们还可以\\*编辑\\*图的状态。我们使用`graph.update\\_state()`来实现。此方法接受三个不同的参数\n#### `config`[¶]<web_link>\n配置应包含`thread\\_id`，指定要更新哪个线程。当只传递`thread\\_id`时，我们更新（或分叉）当前状态。可选地，如果我们包含`checkpoint\\_id`字段，那么我们分叉该选定的检查点。\n#### `values`[¶]<web_link>\n这些是将用于更新状态的值。请注意，此更新的处理方式与来自节点的任何更新完全相同。这意味着如果为图状态中的某些通道定义了reducer 函数，这些值将传递给[reducer]<web_link>函数。这意味着`update\\_state`不会自动覆盖每个通道的通道值，而只覆盖没有 reducer 的通道。让我们通过一个例子来了解。假设你已经使用以下模式定义了图的状态（参见上面的完整示例）```\n`[]<web_link>fromtypingimportAnnotated[]<web_link>fromtyping\\_extensionsimportTypedDict[]<web_link>fromoperatorimportadd[]<web_link>[]<web_link>classState(TypedDict):[]<web_link>foo:int[]<web_link>bar:Annotated[list[str],add]`\n```\n现在假设图的当前状态是```\n`[] {"foo": 1, "bar": ["a"]}`\n```\n如果你如下更新状态```\n`[]<web_link>graph.update\\_state(config, {"foo": 2, "bar": ["b"]})`\n```\n那么图的新状态将是```\n`[] {"foo": 2, "bar": ["a", "b"]}`\n```\n`foo`键（通道）完全改变了（因为没有为该通道指定 reducer，所以`update\\_state`会覆盖它）。然而，为`bar`键指定了一个 reducer，因此它将`"b"`附加到`bar`的状态。\n#### `as\\_node`[¶]<web_link>\n调用`update\\_state`时，你可以选择指定的最后一项是`as\\_node`。如果你提供了它，更新将像来自节点`as\\_node`一样应用。如果未提供`as\\_node`，则在不模糊的情况下，它将被设置为最后更新状态的节点。这之所以重要，是因为接下来要执行的步骤取决于最后给出更新的节点，因此这可用于控制哪个节点接下来执行。请参阅此[时间旅行操作指南，了解有关分叉状态的更多信息]<web_link>。\n![Update]<image_link>\n## 记忆存储[¶]<web_link>\n![Model of shared state]<image_link>\n[状态模式]<web_link>指定了一组在图执行时填充的键。如上所述，状态可以通过检查点器在每个图步骤写入线程，从而实现状态持久化。\n但是，如果我们要\\*跨线程\\*保留一些信息怎么办？考虑一个聊天机器人的情况，我们希望在与该用户进行的所有聊天对话（例如，线程）中保留有关该用户的特定信息！\n仅凭检查点器，我们无法跨线程共享信息。这促使了对[`Store`]<web_link>接口的需求。为了说明这一点，我们可以定义一个`InMemoryStore`来存储用户在所有线程中的信息。我们只需像以前一样，使用检查点器和我们新的`in\\_memory\\_store`变量来编译我们的图。\nLangGraph API 自动处理存储使用LangGraph API 时，你无需手动实现或配置存储。API 在幕后为你处理所有存储基础设施。### 基本用法[¶]<web_link>\n首先，让我们在不使用LangGraph 的情况下单独展示这一点。```\n`[]<web_link>fromlanggraph.store.memoryimportInMemoryStore[]<web_link>in\\_memory\\_store=InMemoryStore()`\n```\n记忆通过`tuple`进行命名空间管理，在此特定示例中为`(&lt;&lt;user\\_id&gt;&gt;, "memories")`。命名空间可以是任意长度，并表示任何内容，不必是用户特定的。\n```\n`[]<web_link>user\\_id="1"[]<web_link>namespace\\_for\\_memory=(user\\_id,"memories")`\n```\n我们使用`store.put`方法将记忆保存到存储中的命名空间。当我们这样做时，我们指定命名空间（如上定义），以及记忆的键值对：键是记忆的唯一标识符（`memory\\_id`），值（一个字典）是记忆本身。\n```\n`[]<web_link>memory\\_id=str(uuid.uuid4())[]<web_link>memory={"food\\_preference":"I like pizza"}[]<web_link>in\\_memory\\_store.put(namespace\\_for\\_memory,memory\\_id,memory)`\n```\n我们可以使用`store.search`方法读取命名空间中的记忆，该方法将以列表形式返回给定用户的所有记忆。最近的记忆在列表的末尾。\n```\n`[]<web_link>memories=in\\_memory\\_store.search(namespace\\_for\\_memory)[]<web_link>memories[-1].dict()[]<web_link>{\'value\':{\'food\\_preference\':\'I like pizza\'},[]<web_link>\'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\',[]<web_link>\'namespace\':[\'1\',\'memories\'],[]<web_link>\'created\\_at\':\'2024-10-02T17:22:31.590602+00:00\',[]<web_link>\'updated\\_at\':\'2024-10-02T17:22:31.590605+00:00\'}`\n```\n每种记忆类型都是一个Python 类([`Item`])，具有特定属性。我们可以像上面那样通过`.dict`转换来将其作为字典访问。它具有的属性是\n* `value`：此记忆的值（本身是一个字典）\n* `key`：此记忆在此命名空间中的唯一键\n* `namespace`：一个字符串列表，此记忆类型的命名空间\n* `created\\_at`：此记忆创建时的时间戳\n* `updated\\_at`：此记忆更新时的时间戳### 语义搜索[¶] \n除了简单检索，该存储还支持语义搜索，允许你根据含义而非精确匹配来查找记忆。要启用此功能，请使用嵌入模型配置存储*API', 'doi': '', 'published_date': '2026-02-22T20:37:58.521593', 'pdf_url': '', 'url': 'https://langgraph.com.cn/concepts/persistence.1.html', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:18,127 - __main__ - INFO - handle_download: searcher=ExaSearcherContext, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:18,128 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:18,128 - __main__ - INFO - call_tool payload: source_tool=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:18,128 - __main__ - INFO - call_tool: name=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:18,128 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '持久化 ¶', 'authors': [], 'abstract': '概述 - LangChain 框架[跳过内容] \n**我们正在壮大，并为 LangChain、LangGraph 和LangSmith 招聘多个职位。[加入我们的团队！] **\n[] # 持久化[¶] \nLangGraph 有一个内置的持久化层，通过检查点器实现。当你使用检查点器编译图时，检查点器会在每个超级步骤保存图状态的`checkpoint`。这些检查点保存到`thread`中，可以在图执行后访问。因为`threads`允许在执行后访问图的状态，所以人机协作、记忆、时间旅行和容错等多种强大功能都成为可能。请参阅[此操作指南] ，了解如何在图中添加和使用检查点器的端到端示例。下面，我们将更详细地讨论这些概念。\n![Checkpoints] \nLangGraph API 自动处理检查点使用LangGraph API 时，你无需手动实现或配置检查点器。API 在幕后为你处理所有持久化基础设施。## 线程[¶] \n线程是分配给检查点器保存的每个检查点的唯一ID 或[线程标识符] 。当使用检查点器调用图时，你**必须**在配置的`configurable`部分中指定`thread\\_id`\n```\n`[]<web_link>{"configurable":{"thread\\_id":"1"}}`\n```\n## 检查点[¶] \n检查点是每个超级步骤保存的图状态的快照，由具有以下关键属性的`StateSnapshot`对象表示\n* `config`：与此检查点关联的配置。\n* `metadata`：与此检查点关联的元数据。\n* `values`：此时状态通道的值。\n* `next`：要在图中接下来执行的节点名称的元组。\n* `tasks`：包含要执行的后续任务信息的`PregelTask`对象元组。如果该步骤之前曾尝试过，它将包含错误信息。如果图在节点内部被[动态] 中断，`tasks`将包含与中断相关的附加数据。\n让我们看看以如下方式调用简单图时保存的检查点*API 参考：[StateGraph] |[START] |[END] |[InMemorySaver] *\n```\n`[]<web_link>fromlanggraph.graphimportStateGraph,START,END[]<web_link>fromlanggraph.checkpoint.memoryimportInMemorySaver[]<web_link>fromtypingimportAnnotated[]<web_link>fromtyping\\_extensionsimportTypedDict[]<web_link>fromoperatorimportadd[]<web_link>[]<web_link>classState(TypedDict):[]<web_link>foo:str[]<web_link>bar:Annotated[list[str],add][]<web_link>[]<web_link>defnode\\_a(state:State):[]<web_link>return{"foo":"a","bar":["a"]}[]<web_link>[]<web_link>defnode\\_b(state:State):[]<web_link>return{"foo":"b","bar":["b"]}[]<web_link>[]<web_link>[]<web_link>workflow=StateGraph(State)[]<web_link>workflow.add\\_node(node\\_a)[]<web_link>workflow.add\\_node(node\\_b)[]<web_link>workflow.add\\_edge(START,"node\\_a")[]<web_link>workflow.add\\_edge("node\\_a","node\\_b")[]<web_link>workflow.add\\_edge("node\\_b",END)[]<web_link>[]<web_link>checkpointer=InMemorySaver()[]<web_link>graph=workflow.compile(checkpointer=checkpointer)[]<web_link>[]<web_link>config={"configurable":{"thread\\_id":"1"}}[]<web_link>graph.invoke({"foo":""},config)`\n```\n运行图后，我们预期会看到恰好4 个检查点* 空检查点，其中`START`是下一个要执行的节点\n* 包含用户输入`{\'foo\': \'\', \'bar\': []}`的检查点，`node\\_a`是下一个要执行的节点\n* 包含`node\\_a`输出`{\'foo\': \'a\', \'bar\': [\'a\']}`的检查点，`node\\_b`是下一个要执行的节点\n* 包含`node\\_b`输出`{\'foo\': \'b\', \'bar\': [\'a\', \'b\']}`的检查点，并且没有要执行的下一个节点\n请注意，`bar`通道的值包含来自两个节点的输出，因为我们为`bar`通道设置了 reducer。\n### 获取状态[¶] \n当与保存的图状态交互时，你**必须**指定一个[线程标识符] 。你可以通过调用`graph.get\\_state(config)`来查看图的\\*最新\\*状态。这将返回一个`StateSnapshot`对象，该对象对应于配置中提供的线程 ID 关联的最新检查点，或者如果提供了检查点ID，则返回与该线程的检查点 ID 关联的检查点。```\n`[] # get the latest state snapshot[] config={"configurable":{"thread\\_id":"1"}}[] graph.get\\_state(config)[] [] # get a state snapshot for a specific checkpoint\\_id[] config={"configurable":{"thread\\_id":"1","checkpoint\\_id":"1ef663ba-28fe-6528-8002-5a559208592c"}}[] graph.get\\_state(config)`\n```\n在我们的示例中，`get\\_state`的输出将如下所示\n```\n`[] StateSnapshot([] values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},[] next=(),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},[] created\\_at=\'2024-08-29T19:19:38.821749+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}}, tasks=()[])`\n```\n### 获取状态历史[¶]<web_link>\n你可以通过调用`graph.get\\_state\\_history(config)`获取给定线程的图执行的完整历史记录。这将返回一个与配置中提供的线程 ID 关联的`StateSnapshot`对象列表。重要的是，检查点将按时间顺序排列，最近的检查点 /`StateSnapshot`位于列表的首位。\n```\n`[] config={"configurable":{"thread\\_id":"1"}}[] list(graph.get\\_state\\_history(config))`\n```\n在我们的示例中，`get\\_state\\_history`的输出将如下所示\n```\n`[] [[] StateSnapshot([] values={\'foo\': \'b\', \'bar\': [\'a\', \'b\']},[] next=(),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28fe-6528-8002-5a559208592c\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_b\': {\'foo\': \'b\', \'bar\': [\'b\']}}, \'step\': 2},[] created\\_at=\'2024-08-29T19:19:38.821749+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},[] tasks=(),[]),[] StateSnapshot([] values={\'foo\': \'a\', \'bar\': [\'a\']}, next=(\'node\\_b\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f9-6ec4-8001-31981c2c39f8\'}},[] metadata={\'source\': \'loop\', \'writes\': {\'node\\_a\': {\'foo\': \'a\', \'bar\': [\'a\']}}, \'step\': 1},[] created\\_at=\'2024-08-29T19:19:38.819946+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},[] tasks=(PregelTask(id=\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\', name=\'node\\_b\', error=None, interrupts=()),),[]),[] StateSnapshot([] values={\'foo\': \'\', \'bar\': []},[] next=(\'node\\_a\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f4-6b4a-8000-ca575a13d36a\'}},[] metadata={\'source\': \'loop\', \'writes\': None, \'step\': 0},[] created\\_at=\'2024-08-29T19:19:38.817813+00:00\',[] parent\\_config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},[] tasks=(PregelTask(id=\'f1b14528-5ee5-579c-949b-23ef9bfbed58\', name=\'node\\_a\', error=None, interrupts=()),),[]),[] StateSnapshot([] values={\'bar\': []},[] next=(\'\\_\\_start\\_\\_\',),[] config={\'configurable\': {\'thread\\_id\': \'1\', \'checkpoint\\_ns\': \'\', \'checkpoint\\_id\': \'1ef663ba-28f0-6c66-bfff-6723431e8481\'}},[] metadata={\'source\': \'input\', \'writes\': {\'foo\': \'\'}, \'step\': -1},[] created\\_at=\'2024-08-29T19:19:38.816205+00:00\',[] parent\\_config=None,[] tasks=(PregelTask(id=\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\', name=\'\\_\\_start\\_\\_\', error=None, interrupts=()),),[])[]]`\n```\n![State]<image_link>\n### 回放[¶]<web_link>\n也可以回放之前的图执行。如果我们使用`thread\\_id`和`checkpoint\\_id`调用图，那么我们将\\*回放\\*`checkpoint\\_id`对应的检查点\\*之前\\*已执行的步骤，并且只执行检查点\\*之后\\*的步骤。\n* `thread\\_id`是线程的 ID。\n* `checkpoint\\_id`是指线程中特定检查点的标识符。\n在调用图时，你必须将这些作为配置的`configurable`部分传入\n```\n`[] config={"configurable":{"thread\\_id":"1","checkpoint\\_id":"0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}[] graph.invoke(None,config=config)`\n```\n重要的是，LangGraph 知道特定步骤是否以前执行过。如果执行过，LangGraph 只会简单地在图中\\*回放\\*该特定步骤，而不会重新执行该步骤，但这仅适用于所提供的`checkpoint\\_id`\\*之前\\*的步骤。`checkpoint\\_id`\\*之后\\*的所有步骤都将被执行（即，一个新的分支），即使它们以前执行过。请参阅此[时间旅行操作指南，了解有关回放的更多信息]<web_link>。\n![Replay]<image_link>\n### 更新状态[¶]<web_link>\n除了从特定的`checkpoints`回放图之外，我们还可以\\*编辑\\*图的状态。我们使用`graph.update\\_state()`来实现。此方法接受三个不同的参数\n#### `config`[¶]<web_link>\n配置应包含`thread\\_id`，指定要更新哪个线程。当只传递`thread\\_id`时，我们更新（或分叉）当前状态。可选地，如果我们包含`checkpoint\\_id`字段，那么我们分叉该选定的检查点。\n#### `values`[¶]<web_link>\n这些是将用于更新状态的值。请注意，此更新的处理方式与来自节点的任何更新完全相同。这意味着如果为图状态中的某些通道定义了reducer 函数，这些值将传递给[reducer]<web_link>函数。这意味着`update\\_state`不会自动覆盖每个通道的通道值，而只覆盖没有 reducer 的通道。让我们通过一个例子来了解。假设你已经使用以下模式定义了图的状态（参见上面的完整示例）```\n`[]<web_link>fromtypingimportAnnotated[]<web_link>fromtyping\\_extensionsimportTypedDict[]<web_link>fromoperatorimportadd[]<web_link>[]<web_link>classState(TypedDict):[]<web_link>foo:int[]<web_link>bar:Annotated[list[str],add]`\n```\n现在假设图的当前状态是```\n`[] {"foo": 1, "bar": ["a"]}`\n```\n如果你如下更新状态```\n`[]<web_link>graph.update\\_state(config, {"foo": 2, "bar": ["b"]})`\n```\n那么图的新状态将是```\n`[] {"foo": 2, "bar": ["a", "b"]}`\n```\n`foo`键（通道）完全改变了（因为没有为该通道指定 reducer，所以`update\\_state`会覆盖它）。然而，为`bar`键指定了一个 reducer，因此它将`"b"`附加到`bar`的状态。\n#### `as\\_node`[¶]<web_link>\n调用`update\\_state`时，你可以选择指定的最后一项是`as\\_node`。如果你提供了它，更新将像来自节点`as\\_node`一样应用。如果未提供`as\\_node`，则在不模糊的情况下，它将被设置为最后更新状态的节点。这之所以重要，是因为接下来要执行的步骤取决于最后给出更新的节点，因此这可用于控制哪个节点接下来执行。请参阅此[时间旅行操作指南，了解有关分叉状态的更多信息]<web_link>。\n![Update]<image_link>\n## 记忆存储[¶]<web_link>\n![Model of shared state]<image_link>\n[状态模式]<web_link>指定了一组在图执行时填充的键。如上所述，状态可以通过检查点器在每个图步骤写入线程，从而实现状态持久化。\n但是，如果我们要\\*跨线程\\*保留一些信息怎么办？考虑一个聊天机器人的情况，我们希望在与该用户进行的所有聊天对话（例如，线程）中保留有关该用户的特定信息！\n仅凭检查点器，我们无法跨线程共享信息。这促使了对[`Store`]<web_link>接口的需求。为了说明这一点，我们可以定义一个`InMemoryStore`来存储用户在所有线程中的信息。我们只需像以前一样，使用检查点器和我们新的`in\\_memory\\_store`变量来编译我们的图。\nLangGraph API 自动处理存储使用LangGraph API 时，你无需手动实现或配置存储。API 在幕后为你处理所有存储基础设施。### 基本用法[¶]<web_link>\n首先，让我们在不使用LangGraph 的情况下单独展示这一点。```\n`[]<web_link>fromlanggraph.store.memoryimportInMemoryStore[]<web_link>in\\_memory\\_store=InMemoryStore()`\n```\n记忆通过`tuple`进行命名空间管理，在此特定示例中为`(&lt;&lt;user\\_id&gt;&gt;, "memories")`。命名空间可以是任意长度，并表示任何内容，不必是用户特定的。\n```\n`[]<web_link>user\\_id="1"[]<web_link>namespace\\_for\\_memory=(user\\_id,"memories")`\n```\n我们使用`store.put`方法将记忆保存到存储中的命名空间。当我们这样做时，我们指定命名空间（如上定义），以及记忆的键值对：键是记忆的唯一标识符（`memory\\_id`），值（一个字典）是记忆本身。\n```\n`[]<web_link>memory\\_id=str(uuid.uuid4())[]<web_link>memory={"food\\_preference":"I like pizza"}[]<web_link>in\\_memory\\_store.put(namespace\\_for\\_memory,memory\\_id,memory)`\n```\n我们可以使用`store.search`方法读取命名空间中的记忆，该方法将以列表形式返回给定用户的所有记忆。最近的记忆在列表的末尾。\n```\n`[]<web_link>memories=in\\_memory\\_store.search(namespace\\_for\\_memory)[]<web_link>memories[-1].dict()[]<web_link>{\'value\':{\'food\\_preference\':\'I like pizza\'},[]<web_link>\'key\':\'07e0caf4-1631-47b7-b15f-65515d4c1843\',[]<web_link>\'namespace\':[\'1\',\'memories\'],[]<web_link>\'created\\_at\':\'2024-10-02T17:22:31.590602+00:00\',[]<web_link>\'updated\\_at\':\'2024-10-02T17:22:31.590605+00:00\'}`\n```\n每种记忆类型都是一个Python 类([`Item`])，具有特定属性。我们可以像上面那样通过`.dict`转换来将其作为字典访问。它具有的属性是\n* `value`：此记忆的值（本身是一个字典）\n* `key`：此记忆在此命名空间中的唯一键\n* `namespace`：一个字符串列表，此记忆类型的命名空间\n* `created\\_at`：此记忆创建时的时间戳\n* `updated\\_at`：此记忆更新时的时间戳### 语义搜索[¶] \n除了简单检索，该存储还支持语义搜索，允许你根据含义而非精确匹配来查找记忆。要启用此功能，请使用嵌入模型配置存储*API', 'doi': '', 'published_date': '2026-02-22T20:37:58.521593', 'pdf_url': '', 'url': 'https://langgraph.com.cn/concepts/persistence.1.html', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'saved_path': '/home/qinshan/widthresearch/data/downloads/exa_持久化 ¶.md'}}
2026-02-22 20:38:19,710 - __main__ - INFO - call_tool: name=tavily_download, args={'papers': [{'paper_id': '', 'title': '人机协作(Human-in-the-Loop) - LangChain 文档', 'authors': [], 'abstract': '* [用例](#use-cases)\n* [中断](#interrupt)\n* [要求](#requirements)\n* [设计模式](#design-patterns) \n\n  + [批准或拒绝](#approve-or-reject)\n  + [审查与编辑状态](#review-edit-state)\n  + [审查工具调用](#review-tool-calls)\n  + [多轮对话](#multi-turn-conversation)\n  + [验证人类输入](#validating-human-input)\n* [Command 原语](#the-command-primitive)\n* [与 invoke 结合使用](#using-with-invoke)\n* [从中断恢复如何工作？](#how-does-resuming-from-an-interrupt-work)\n* [常见陷阱](#common-pitfalls) \n\n  + [副作用](#side-effects)\n  + [作为函数调用的子图](#subgraphs-called-as-functions)\n  + [使用多个中断](#using-multiple-interrupts)\n* [附加资源 📚](#additional-resources)\n\n1. [LangGraph](../..)\n2. [指南](../../how-tos/)\n3. [概念](../)\n4. [LangGraph](../../concepts#langgraph)\n\n# 人机协作 (Human-in-the-loop)[¶](#human-in-the-loop "Permanent link")\n\n本指南使用新的 `interrupt` 函数。\n\n自 LangGraph 0.2.31 起，推荐使用 [`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 来设置断点，因为它简化了**人机协作（human-in-the-loop）**模式。\n\n如果您正在寻找此概念指南的先前版本，该版本依赖于静态断点和 `NodeInterrupt` 异常，请点击[此处](../v0-human-in-the-loop/)。\n\n**人机协作（human-in-the-loop）**（或“在环”）工作流将人类输入集成到自动化流程中，允许在关键阶段进行决策、验证或更正。这在**基于 LLM 的应用程序**中尤其有用，因为底层模型可能会偶尔产生不准确的内容。在合规、决策或内容生成等低容错场景中，人类参与通过允许审查、更正或覆盖模型输出来确保可靠性。\n\n## 用例[¶](#use-cases "Permanent link")\n\n基于 LLM 应用程序中**人机协作**工作流的主要用例包括：\n\n1. [**🛠️ 审查工具调用**](#review-tool-calls)：人类可以在工具执行前审查、编辑或批准 LLM 请求的工具调用。\n2. **✅ 验证 LLM 输出**：人类可以审查、编辑或批准 LLM 生成的内容。\n3. **💡 提供上下文**：使 LLM 能够明确请求人类输入以进行澄清或提供额外细节，或支持多轮对话。\n\n## `interrupt`[¶](#interrupt "Permanent link")\n\nLangGraph 中的 [`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 通过在特定节点暂停图，向人类呈现信息，并用他们的输入恢复图，从而实现人机协作工作流。此函数对于批准、编辑或收集额外输入等任务非常有用。[`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 与 [`Command`](/langgraphjs/reference/classes/langgraph.Command.html) 对象结合使用，以人类提供的值恢复图。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { const value = interrupt(  const  value  =  interrupt( // Any JSON serializable value to surface to the human.  // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state  // For example, a question or a piece of text or a set of keys in the state {  { text_to_revise: state.some_text,  text_to_revise:  state.some_text, }  } );  ); // Update the state with the human\'s input or route the graph based on the input  // Update the state with the human\'s input or route the graph based on the input return {  return  { some_text: value,  some_text:  value, };  };} }  const graph = workflow.compile({ const  graph  =  workflow. compile({ checkpointer, // Required for `interrupt` to work  checkpointer,  // Required for `interrupt` to work}); });  // Run the graph until the interrupt // Run the graph until the interruptconst threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke(someInput, threadConfig); await  graph. invoke(someInput,  threadConfig);  // Below code can run some amount of time later and/or in a different process // Below code can run some amount of time later and/or in a different process  // Human input // Human inputconst valueFromHuman = "..."; const  valueFromHuman  =  "...";  // Resume the graph with the human\'s input // Resume the graph with the human\'s inputawait graph.invoke(new Command({ resume: valueFromHuman }), threadConfig); await  graph. invoke(new  Command({ resume:  valueFromHuman  }),  threadConfig);\n```\n\n```\n{ { some_text: "Edited text";  some_text:  "Edited text";} }\n```\n\n 完整代码\n\n以下是关于如何在图中使用 `interrupt` 的完整示例，如果您想查看代码的实际运行情况。\n\n```\nimport { MemorySaver, Annotation, interrupt, Command, StateGraph } from "@langchain/langgraph"; import  { MemorySaver,  Annotation,  interrupt,  Command,  StateGraph  }  from  "@langchain/langgraph";  // Define the graph state // Define the graph stateconst StateAnnotation = Annotation.Root({ const  StateAnnotation  =  Annotation. Root({ some_text: Annotation<string>()  some_text:  Annotation< string>()}); });  function humanNode(state: typeof StateAnnotation.State) { function  humanNode(state:  typeof  StateAnnotation. State)  { const value = interrupt(  const  value  =  interrupt( // Any JSON serializable value to surface to the human.  // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state  // For example, a question or a piece of text or a set of keys in the state {  { text_to_revise: state.some_text  text_to_revise:  state.some_text }  } );  ); return {  return  { // Update the state with the human\'s input  // Update the state with the human\'s input some_text: value  some_text:  value };  };} }  // Build the graph // Build the graphconst workflow = new StateGraph(StateAnnotation) const  workflow  =  new  StateGraph(StateAnnotation)// Add the human-node to the graph // Add the human-node to the graph .addNode("human_node", humanNode)  . addNode("human_node",  humanNode) .addEdge("__start__", "human_node")  . addEdge("__start__",  "human_node")  // A checkpointer is required for `interrupt` to work. // A checkpointer is required for `interrupt` to work.const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();const graph = workflow.compile({ const  graph  =  workflow. compile({ checkpointer  checkpointer}); });  // Using stream() to directly surface the `__interrupt__` information. // Using stream() to directly surface the `__interrupt__` information.for await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( { some_text: "Original text" },  { some_text:  "Original text"  },  threadConfig  threadConfig)) { ))  { console.log(chunk);  console. log(chunk);} }  // Resume using Command // Resume using Commandfor await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( new Command({ resume: "Edited text" }),  new  Command({ resume:  "Edited text"  }),  threadConfig  threadConfig)) { ))  { console.log(chunk);  console. log(chunk);} }\n```\n\n```\n{ { __interrupt__: [  __interrupt__:  [ {  { value: { question: \'Please revise the text\', some_text: \'Original text\' },  value:  { question:  \'Please revise the text\',  some_text:  \'Original text\'  }, resumable: true,  resumable:  true, ns: [\'human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6\'],  ns:  [\'human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6\'], when: \'during\'  when:  \'during\' }  } ]  ]} }{ human_node: { some_text: \'Edited text\' } } { human_node:  { some_text:  \'Edited text\'  }  }\n```\n\n## 要求[¶](#requirements "永久链接")\n\n要在图中使用 `interrupt`，您需要：\n\n1. [**指定检查点**](../persistence/#checkpoints) 以在每一步之后保存图状态。\n2. 在适当的位置**调用 `interrupt()`**。请参阅[设计模式](#design-patterns)部分以获取示例。\n3. 使用[**线程 ID**](../persistence/#threads) **运行图**，直到触发 `interrupt`。\n4. 使用 `invoke`/`stream` **恢复执行**（请参阅[**`Command` 原语**](#the-command-primitive)）。\n\n## 设计模式[¶](#design-patterns "Permanent link")\n\n通常，您可以通过人机协作工作流执行三种不同的**操作**：\n\n1. **批准或拒绝**：在关键步骤（例如 API 调用）之前暂停图，以审查和批准操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。此模式通常涉及根据人类的输入对图进行**路由**。\n2. **编辑图状态**：暂停图以审查和编辑图状态。这对于纠正错误或使用附加信息更新状态很有用。此模式通常涉及使用人类的输入**更新**状态。\n3. **获取输入**：在图的特定步骤中明确请求人类输入。这对于收集额外信息或上下文以指导代理的决策过程或支持**多轮对话**很有用。\n\n下面我们展示了可以使用这些**操作**实现的不同设计模式。\n\n**注意：** `interrupt` 函数通过抛出特殊的 `GraphInterrupt` 错误来传播。因此，您应该避免在 `interrupt` 函数周围使用 `try/catch` 块——如果确实使用了，请确保在 `catch` 块中再次抛出 `GraphInterrupt` 错误。\n\n### 批准或拒绝[¶](#approve-or-reject "Permanent link")\n\n在关键步骤（例如 API 调用）之前暂停图，以审查和批准操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。\n\n```\nimport { interrupt, Command } from "@langchain/langgraph"; import  { interrupt,  Command  }  from  "@langchain/langgraph";  function humanApproval(state: typeof GraphAnnotation.State): Command { function  humanApproval(state:  typeof  GraphAnnotation. State):  Command  { const isApproved = interrupt({  const  isApproved  =  interrupt({ question: "Is this correct?",  question:  "Is this correct?", // Surface the output that should be  // Surface the output that should be // reviewed and approved by the human.  // reviewed and approved by the human. llm_output: state.llm_output,  llm_output:  state.llm_output, });  });   if (isApproved) {  if  (isApproved)  { return new Command({ goto: "some_node" });  return  new  Command({ goto:  "some_node"  }); } else {  }  else  { return new Command({ goto: "another_node" });  return  new  Command({ goto:  "another_node"  }); }  }} }  // Add the node to the graph in an appropriate location // Add the node to the graph in an appropriate location// and connect it to the relevant nodes. // and connect it to the relevant nodes.const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_approval", humanApproval)  . addNode("human_approval",  humanApproval) .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with either an approval or rejection. // Resume it with either an approval or rejection.const threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke(new Command({ resume: true }), threadConfig); await  graph. invoke(new  Command({ resume:  true  }),  threadConfig);\n```\n\n有关更详细的示例，请参阅[如何审查工具调用](/langgraphjs/how-tos/review-tool-calls)。\n\n### 审查与编辑状态[¶](#review-edit-state "Permanent link")\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanEditing(state: typeof GraphAnnotation.State): Command { function  humanEditing(state:  typeof  GraphAnnotation. State):  Command  { const result = interrupt({  const  result  =  interrupt({ // Interrupt information to surface to the client.  // Interrupt information to surface to the client. // Can be any JSON serializable value.  // Can be any JSON serializable value. task: "Review the output from the LLM and make any necessary edits.",  task:  "Review the output from the LLM and make any necessary edits.", llm_generated_summary: state.llm_generated_summary,  llm_generated_summary:  state.llm_generated_summary, });  });   // Update the state with the edited text  // Update the state with the edited text return {  return  { llm_generated_summary: result.edited_text,  llm_generated_summary:  result.edited_text, };  };} }  // Add the node to the graph in an appropriate location // Add the node to the graph in an appropriate location// and connect it to the relevant nodes. // and connect it to the relevant nodes.const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_editing", humanEditing)  . addNode("human_editing",  humanEditing) .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with the edited text. // Resume it with the edited text.const threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke( await  graph. invoke( new Command({ resume: { edited_text: "The edited text" } }),  new  Command({ resume:  { edited_text:  "The edited text"  }  }),  threadConfig  threadConfig); );\n```\n\n有关更详细的示例，请参阅[如何使用中断等待用户输入](/langgraphjs/how-tos/wait-user-input)。\n\n### 审查工具调用[¶](#review-tool-calls "Permanent link")\n\n```\nimport { interrupt, Command } from "@langchain/langgraph"; import  { interrupt,  Command  }  from  "@langchain/langgraph";  function humanReviewNode(state: typeof GraphAnnotation.State): Command { function  humanReviewNode(state:  typeof  GraphAnnotation. State):  Command  { // This is the value we\'ll be providing via Command.resume()  // This is the value we\'ll be providing via Command.resume() const humanReview = interrupt({  const  humanReview  =  interrupt({ question: "Is this correct?",  question:  "Is this correct?", // Surface tool calls for review  // Surface tool calls for review tool_call: toolCall,  tool_call:  toolCall, });  });   const [reviewAction, reviewData] = humanReview;  const  [reviewAction,  reviewData]  =  humanReview;   // Approve the tool call and continue  // Approve the tool call and continue if (reviewAction === "continue") {  if  (reviewAction  ===  "continue")  { return new Command({ goto: "run_tool" });  return  new  Command({ goto:  "run_tool"  }); }  } // Modify the tool call manually and then continue  // Modify the tool call manually and then continue else if (reviewAction === "update") {  else  if  (reviewAction  ===  "update")  { const updatedMsg = getUpdatedMsg(reviewData);  const  updatedMsg  =  getUpdatedMsg(reviewData); // Remember that to modify an existing message you will need  // Remember that to modify an existing message you will need // to pass the message with a matching ID.  // to pass the message with a matching ID. return new Command({  return  new  Command({ goto: "run_tool",  goto:  "run_tool", update: { messages: [updatedMsg] },  update:  { messages:  [updatedMsg]  }, });  }); }  } // Give natural language feedback, and then pass that back to the agent  // Give natural language feedback, and then pass that back to the agent else if (reviewAction === "feedback") {  else  if  (reviewAction  ===  "feedback")  { const feedbackMsg = getFeedbackMsg(reviewData);  const  feedbackMsg  =  getFeedbackMsg(reviewData); return new Command({  return  new  Command({ goto: "call_llm",  goto:  "call_llm", update: { messages: [feedbackMsg] },  update:  { messages:  [feedbackMsg]  }, });  }); }  }} }\n```\n\n有关更详细的示例，请参阅[如何审查工具调用](/langgraphjs/how-tos/review-tool-calls)。\n\n### 多轮对话[¶](#multi-turn-conversation "Permanent link")\n\n**多轮对话**涉及代理和人类之间的多次来回交互，这可以允许代理以对话方式从人类那里收集额外信息。\n\n这种设计模式在由[多个代理](../multi_agent/)组成的 LLM 应用程序中很有用。一个或多个代理可能需要与人类进行多轮对话，其中人类在对话的不同阶段提供输入或反馈。为简单起见，下面的代理实现被说明为单个节点，但实际上它可能是由多个节点组成的更大图的一部分，并包含条件边。\n\n在此模式中，每个代理都有自己的人类节点用于收集用户输入。\n\n这可以通过为人类节点使用唯一名称（例如，“代理 1 的人类节点”，“代理 2 的人类节点”）或使用子图（其中子图包含人类节点和代理节点）来实现。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanInput(state: typeof GraphAnnotation.State) { function  humanInput(state:  typeof  GraphAnnotation. State)  { const humanMessage = interrupt("human_input");  const  humanMessage  =  interrupt("human_input");   return {  return  { messages: [  messages:  [ {  { role: "human",  role:  "human", content: humanMessage  content:  humanMessage }  } ]  ] };  };} }  function agent(state: typeof GraphAnnotation.State) { function  agent(state:  typeof  GraphAnnotation. State)  { // Agent logic  // Agent logic // ...  // ...} }  const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_input", humanInput)  . addNode("human_input",  humanInput) .addEdge("human_input", "agent")  . addEdge("human_input",  "agent") .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with the human\'s input. // Resume it with the human\'s input.await graph.invoke( await  graph. invoke( new Command({ resume: "hello!" }),  new  Command({ resume:  "hello!"  }),  threadConfig  threadConfig); );\n```\n\n在此模式中，单个人类节点用于收集多个代理的用户输入。活动代理从状态中确定，因此在收集人类输入后，图可以路由到正确的代理。\n\n```\nimport { interrupt, Command, MessagesAnnotation } from "@langchain/langgraph"; import  { interrupt,  Command,  MessagesAnnotation  }  from  "@langchain/langgraph";  function humanNode(state: typeof MessagesAnnotation.State): Command { function  humanNode(state:  typeof  MessagesAnnotation. State):  Command  { /**  /** * A node for collecting user input.  * A node for collecting user input. */  */ const userInput = interrupt("Ready for user input.");  const  userInput  =  interrupt("Ready for user input.");   // Determine the **active agent** from the state, so  // Determine the **active agent** from the state, so // we can route to the correct agent after collecting input.  // we can route to the correct agent after collecting input. // For example, add a field to the state or use the last active agent.  // For example, add a field to the state or use the last active agent. // or fill in `name` attribute of AI messages generated by the agents.  // or fill in `name` attribute of AI messages generated by the agents. const activeAgent = ...;  const  activeAgent  =  ...;   return new Command({  return  new  Command({ goto: activeAgent,  goto:  activeAgent, update: {  update:  { messages: [{  messages:  [{ role: "human",  role:  "human", content: userInput,  content:  userInput, }]  }] }  } });  });} }\n```\n\n有关更详细的示例，请参阅[如何实现多轮对话](/langgraphjs/how-tos/multi-agent-multi-turn-convo)。\n\n### 验证人类输入[¶](#validating-human-input "Permanent link")\n\n如果您需要在图本身中（而不是在客户端）验证人类提供的输入，可以通过在单个节点中使用多个中断调用来实现。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */ let question = "What is your age?";  let  question  =  "What is your age?";   while (true) {  while  (true)  { const answer = interrupt(question);  const  answer  =  interrupt(question);   // Validate answer, if the answer isn\'t valid ask for input again.  // Validate answer, if the answer isn\'t valid ask for input again. if (typeof answer !== "number" || answer < 0) {  if  (typeof  answer  !==  "number"  ||  answer  <  0)  { question = `\'${answer}\' is not a valid age. What is your age?`;  question  =  `\'${answer}\' is not a valid age. What is your age?`; continue;  continue; } else {  }  else  { // If the answer is valid, we can proceed.  // If the answer is valid, we can proceed. break;  break; }  } }  }   console.log(`The human in the loop is ${answer} years old.`);  console. log(`The human in the loop is ${answer} years old.`);   return {  return  { age: answer,  age:  answer, };  };} }\n```\n\n## Command 原语[¶](#the-command-primitive "Permanent link")\n\n当使用 `interrupt` 函数时，图将在中断处暂停并等待用户输入。\n\n图的执行可以使用 [Command](/langgraphjs/reference/classes/langgraph.Command.html) 原语恢复，该原语可以通过 `invoke` 或 `stream` 方法传递。\n\n`Command` 原语提供了几个选项来控制和修改恢复期间图的状态：\n\n1. **将值传递给 `interrupt`**：使用 `new Command({ resume: value })` 向图提供数据，例如用户的响应。执行从使用 `interrupt` 的节点的开头恢复，但是，这次 `interrupt(...)` 调用将返回在 `new Command({ resume: value })` 中传递的值，而不是暂停图。\n\n```\n// Resume graph execution with the user\'s input. // Resume graph execution with the user\'s input.await graph.invoke(new Command({ resume: { age: "25" } }), threadConfig); await  graph. invoke(new  Command({ resume:  { age:  "25"  }  }),  threadConfig);\n```\n\n1. **更新图状态**：使用 `Command({ goto: ..., update: ... })` 修改图状态。请注意，恢复从使用 `interrupt` 的节点的开头开始。执行从使用 `interrupt` 的节点的开头恢复，但带有更新后的状态。\n\n```\n// Update the graph state and resume. // Update the graph state and resume.// You must provide a `resume` value if using an `interrupt`. // You must provide a `resume` value if using an `interrupt`.await graph.invoke( await  graph. invoke( new Command({ resume: "Let\'s go!!!", update: { foo: "bar" } }),  new  Command({ resume:  "Let\'s go!!!",  update:  { foo:  "bar"  }  }),  threadConfig  threadConfig); );\n```\n\n通过利用 `Command`，您可以恢复图的执行，处理用户输入，并动态调整图的状态。\n\n## 与 `invoke` 结合使用[¶](#using-with-invoke "Permanent link")\n\n当您使用 `stream` 运行图时，您将收到一个 `Interrupt` 事件，它会通知您 `interrupt` 已被触发。\n\n`invoke` 不会返回中断信息。要访问此信息，您必须在调用 `invoke` 后使用 [getState](/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#getState) 方法检索图状态。\n\n```\n// Run the graph up to the interrupt // Run the graph up to the interruptconst result = await graph.invoke(inputs, threadConfig); const  result  =  await  graph. invoke(inputs,  threadConfig);  // Get the graph state to get interrupt information. // Get the graph state to get interrupt information.const state = await graph.getState(threadConfig); const  state  =  await  graph. getState(threadConfig);  // Print the state values // Print the state valuesconsole.log(state.values); console. log(state. values);  // Print the pending tasks // Print the pending tasksconsole.log(state.tasks); console. log(state. tasks);  // Resume the graph with the user\'s input. // Resume the graph with the user\'s input.await graph.invoke(new Command({ resume: { age: "25" } }), threadConfig); await  graph. invoke(new  Command({ resume:  { age:  "25"  }  }),  threadConfig);\n```\n\n```\n{ { foo: "bar";  foo:  "bar";} // State values }  // State values  [ [ {  { id: "5d8ffc92-8011-0c9b-8b59-9d3545b7e553",  id:  "5d8ffc92-8011-0c9b-8b59-9d3545b7e553", name: "node_foo",  name:  "node_foo", path: ["__pregel_pull", "node_foo"],  path:  ["__pregel_pull",  "node_foo"], error: null,  error:  null, interrupts: [  interrupts:  [ {  { value: "value_in_interrupt",  value:  "value_in_interrupt", resumable: true,  resumable:  true, ns: ["node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553"],  ns:  ["node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553"], when: "during",  when:  "during", },  }, ],  ], state: null,  state:  null, result: null,  result:  null, },  },]; // Pending tasks. interrupts ];  // Pending tasks. interrupts\n```\n\n## 从中断恢复如何工作？[¶](#how-does-resuming-from-an-interrupt-work "Permanent link")\n\n使用 `interrupt` 的一个关键方面是理解恢复的工作原理。当您在 `interrupt` 后恢复执行时，图的执行从上次触发 `interrupt` 的**图节点**的**开头**开始。\n\n从节点开头到 `interrupt` 的**所有**代码都将重新执行。\n\n```\nlet counter = 0; let  counter  =  0;  function node(state: State) { function  node(state:  State)  { // All the code from the beginning of the node to the interrupt will be re-executed  // All the code from the beginning of the node to the interrupt will be re-executed // when the graph resumes.  // when the graph resumes. counter += 1;  counter  +=  1;   console.log(`> Entered the node: ${counter} # of times`);  console. log(`> Entered the node: ${counter}  # of times`);   // Pause the graph and wait for user input.  // Pause the graph and wait for user input. const answer = interrupt();  const  answer  =  interrupt();   console.log("The value of counter is:", counter);  console. log("The value of counter is:",  counter); // ...  // ...} }\n```\n\n在**恢复**图时，计数器将第二次递增，导致以下输出：\n\n```\n> Entered the node: 2 # of times >  Entered  the  node:  2  #  of  timesThe value of counter is: 2 The  value  of  counter  is:  2\n```\n\n## 常见陷阱[¶](#common-pitfalls "永久链接")\n\n### 副作用[¶](#side-effects "Permanent link")\n\n将带有副作用的代码（例如 API 调用）放在 `interrupt` **之后**，以避免重复，因为这些代码在每次节点恢复时都会重新触发。\n\n当节点从 `interrupt` 恢复时，此代码将再次重新执行 API 调用。如果 API 调用不是幂等的或者成本很高，这可能会导致问题。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */ apiCall(); // This code will be re-executed when the node is resumed.  apiCall();  // This code will be re-executed when the node is resumed.   const answer = interrupt(question);  const  answer  =  interrupt(question);} }\n```\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */   const answer = interrupt(question);  const  answer  =  interrupt(question);   apiCall(answer); // OK as it\'s after the interrupt  apiCall(answer);  // OK as it\'s after the interrupt} }\n```\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */   const answer = interrupt(question);  const  answer  =  interrupt(question);   return {  return  { answer  answer };  };} }  function apiCallNode(state: typeof GraphAnnotation.State) { function  apiCallNode(state:  typeof  GraphAnnotation. State)  { apiCall(); // OK as it\'s in a separate node  apiCall();  // OK as it\'s in a separate node} }\n```\n\n### 作为函数调用的子图[¶](#subgraphs-called-as-functions "Permanent link")\n\n当[作为函数](../low_level/#as-a-function)调用子图时，**父图**将从调用子图的**节点开头**（以及触发 `interrupt` 的地方）恢复执行。同样，**子图**将从调用 `interrupt()` 函数的**节点开头**恢复。\n\n例如：\n\n```\nasync function nodeInParentGraph(state: typeof GraphAnnotation.State) { async  function  nodeInParentGraph(state:  typeof  GraphAnnotation. State)  { someCode(); // <-- This will re-execute when the subgraph is resumed.  someCode();  // <-- This will re-execute when the subgraph is resumed. // Invoke a subgraph as a function.  // Invoke a subgraph as a function. // The subgraph contains an `interrupt` call.  // The subgraph contains an `interrupt` call. const subgraphResult = await subgraph.invoke(someInput);  const  subgraphResult  =  await  subgraph. invoke(someInput); ...  ...} }\n```\n\n **示例：父图和子图的执行流程**\n\n假设我们有一个包含 3 个节点的父图：\n\n**父图**：`node_1` → `node_2`（子图调用） → `node_3`\n\n子图有 3 个节点，其中第二个节点包含 `interrupt`：\n\n**子图**：`sub_node_1` → `sub_node_2`（`interrupt`） → `sub_node_3`\n\n恢复图时，执行将按以下方式进行：\n\n1. **跳过父图中的 `node_1`**（已执行，图状态已保存为快照）。\n2. **从头开始重新执行父图中的 `node_2`**。\n3. **跳过子图中的 `sub_node_1`**（已执行，图状态已保存为快照）。\n4. **从头开始重新执行子图中的 `sub_node_2`**。\n5. 继续执行 `sub_node_3` 和后续节点。\n\n这是一个缩写的示例代码，您可以用来理解子图如何与中断一起工作。它计算每个节点进入的次数并打印计数。\n\n```\nimport { import  { StateGraph,  StateGraph, START,  START, interrupt,  interrupt, Command,  Command, MemorySaver,  MemorySaver,  Annotation  Annotation} from "@langchain/langgraph"; }  from  "@langchain/langgraph";  const GraphAnnotation = Annotation.Root({ const  GraphAnnotation  =  Annotation. Root({ stateCounter: Annotation<number>({  stateCounter:  Annotation< number>({ reducer: (a, b) => a + b,  reducer:  (a,  b)  =>  a  +  b, default: () => 0  default:  ()  =>  0 })  })}) })  let counterNodeInSubgraph = 0; let  counterNodeInSubgraph  =  0;  function nodeInSubgraph(state: typeof GraphAnnotation.State) { function  nodeInSubgraph(state:  typeof  GraphAnnotation. State)  { counterNodeInSubgraph += 1; // This code will **NOT** run again!  counterNodeInSubgraph  +=  1;  // This code will **NOT** run again! console.log(`Entered \'nodeInSubgraph\' a total of ${counterNodeInSubgraph} times`);  console. log(`Entered \'nodeInSubgraph\' a total of ${counterNodeInSubgraph}  times`); return {};  return  {};} }  let counterHumanNode = 0; let  counterHumanNode  =  0;  async function humanNode(state: typeof GraphAnnotation.State) { async  function  humanNode(state:  typeof  GraphAnnotation. State)  { counterHumanNode += 1; // This code will run again!  counterHumanNode  +=  1;  // This code will run again! console.log(`Entered humanNode in sub-graph a total of ${counterHumanNode} times`);  console. log(`Entered humanNode in sub-graph a total of ${counterHumanNode}  times`); const answer = await interrupt("what is your name?");  const  answer  =  await  interrupt("what is your name?"); console.log(`Got an answer of ${answer}`);  console. log(`Got an answer of ${answer} `); return {};  return  {};} }  const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();  const subgraphBuilder = new StateGraph(GraphAnnotation) const  subgraphBuilder  =  new  StateGraph(GraphAnnotation) .addNode("some_node", nodeInSubgraph)  . addNode("some_node",  nodeInSubgraph) .addNode("human_node", humanNode)  . addNode("human_node",  humanNode) .addEdge(START, "some_node")  . addEdge(START,  "some_node") .addEdge("some_node", "human_node")  . addEdge("some_node",  "human_node")const subgraph = subgraphBuilder.compile({ checkpointer }); const  subgraph  =  subgraphBuilder. compile({ checkpointer  });  let counterParentNode = 0; let  counterParentNode  =  0;  async function parentNode(state: typeof GraphAnnotation.State) { async  function  parentNode(state:  typeof  GraphAnnotation. State)  { counterParentNode += 1; // This code will run again on resuming!  counterParentNode  +=  1;  // This code will run again on resuming! console.log(`Entered \'parentNode\' a total of ${counterParentNode} times`);  console. log(`Entered \'parentNode\' a total of ${counterParentNode}  times`);   // Please note that we\'re intentionally incrementing the state counter  // Please note that we\'re intentionally incrementing the state counter // in the graph state as well to demonstrate that the subgraph update  // in the graph state as well to demonstrate that the subgraph update // of the same key will not conflict with the parent graph (until  // of the same key will not conflict with the parent graph (until const subgraphState = await subgraph.invoke(state);  const  subgraphState  =  await  subgraph. invoke(state); return subgraphState;  return  subgraphState;} }  const builder = new StateGraph(GraphAnnotation) const  builder  =  new  StateGraph(GraphAnnotation) .addNode("parent_node", parentNode)  . addNode("parent_node",  parentNode) .addEdge(START, "parent_node")  . addEdge(START,  "parent_node")  // A checkpointer must be enabled for interrupts to work! // A checkpointer must be enabled for interrupts to work!const graph = builder.compile({ checkpointer }); const  graph  =  builder. compile({ checkpointer  });  const config = { const  config  =  { configurable: {  configurable:  { thread_id: crypto.randomUUID(),  thread_id:  crypto.randomUUID(), }  }}; };  for await (const chunk of await graph.stream({ stateCounter: 1 }, config)) { for  await  (const  chunk  of  await  graph. stream({ stateCounter:  1  },  config))  { console.log(chunk);  console. log(chunk);} }  console.log(\'--- Resuming ---\'); console. log(\'--- Resuming ---\');  for await (const chunk of await graph.stream(new Command({ resume: "35" }), config)) { for  await  (const  chunk  of  await  graph. stream(new  Command({ resume:  "35"  }),  config))  { console.log(chunk);  console. log(chunk);} }\n```\n\n这将打印出\n\n```\n --- First invocation --- ---  First  invocation  ---In parent node: { foo: \'bar\' } In  parent  node:  { foo:  \'bar\'  } Entered \'parentNode\' a total of 1 times Entered  \'parentNode\'  a  total  of  1  times Entered \'nodeInSubgraph\' a total of 1 times Entered  \'nodeInSubgraph\'  a  total  of  1  timesEntered humanNode in sub-graph a total of 1 times Entered  humanNode  in  sub - graph  a  total  of  1  times{ __interrupt__: [{ value: \'what is your name?\', resumable: true, ns: [\'parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b\', \'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c\'], when: \'during\' }] } { __interrupt__:  [{ value:  \'what is your name?\',  resumable:  true,  ns:  [\'parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b\',  \'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c\'],  when:  \'during\'  }]  }  --- Resuming --- ---  Resuming  ---In parent node: { foo: \'bar\' } In  parent  node:  { foo:  \'bar\'  } Entered \'parentNode\' a total of 2 times Entered  \'parentNode\'  a  total  of  2  timesEntered humanNode in sub-graph a total of 2 times Entered  humanNode  in  sub - graph  a  total  of  2  times Got an answer of 35 Got  an  answer  of  35{ parent_node: null } { parent_node:  null  }\n```\n\n### 使用多个中断[¶](#using-multiple-interrupts "Permanent link")\n\n在**单个**节点中使用多个中断可能有助于实现诸如[验证人类输入](#validating-human-input)之类的模式。然而，如果在同一节点中使用多个中断且不小心处理，可能会导致意外行为。\n\n当一个节点包含多个中断调用时，LangGraph 会为执行该任务的节点保留一个特定于任务的恢复值列表。每当执行恢复时，它都会从节点的开头开始。对于遇到的每个中断，LangGraph 都会检查任务的恢复列表中是否存在匹配的值。匹配是**严格基于索引**的，因此中断调用在节点中的顺序至关重要。\n\n为避免问题，请避免在执行之间动态更改节点结构。这包括添加、删除或重新排序中断调用，因为此类更改可能导致索引不匹配。这些问题通常源于非常规模式，例如通过 `Command.resume(...).update(SOME_STATE_MUTATION)` 改变状态或依赖全局变量动态修改节点结构。\n\n 不正确的代码示例\n\n```\nimport { v4 as uuidv4 } from "uuid"; import  { v4  as  uuidv4  }  from  "uuid";import { import  { StateGraph,  StateGraph, MemorySaver,  MemorySaver, START,  START, interrupt,  interrupt, Command,  Command,  Annotation  Annotation} from "@langchain/langgraph"; }  from  "@langchain/langgraph";  const GraphAnnotation = Annotation.Root({ const  GraphAnnotation  =  Annotation. Root({ name: Annotation<string>(),  name:  Annotation< string>(), age: Annotation<string>()  age:  Annotation< string>()}); });  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { let name;  let  name; if (!state.name) {  if  (! state. name)  { name = interrupt("what is your name?");  name  =  interrupt("what is your name?"); } else {  }  else  { name = "N/A";  name  =  "N/A"; }  }   let age;  let  age; if (!state.age) {  if  (! state. age)  { age = interrupt("what is your age?");  age  =  interrupt("what is your age?"); } else {  }  else  { age = "N/A";  age  =  "N/A"; }  }   console.log(`Name: ${name}. Age: ${age}`);  console. log(`Name: ${name}. Age: ${age} `);   return {  return  { age,  age, name,  name, };  };} }  const builder = new StateGraph(GraphAnnotation) const  builder  =  new  StateGraph(GraphAnnotation) .addNode("human_node", humanNode);  . addNode("human_node",  humanNode); .addEdge(START, "human_node");  . addEdge(START,  "human_node");  // A checkpointer must be enabled for interrupts to work! // A checkpointer must be enabled for interrupts to work!const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();  const graph = builder.compile({ checkpointer }); const  graph  =  builder. compile({ checkpointer  });  const config = { const  config  =  { configurable: {  configurable:  { thread_id: uuidv4(),  thread_id:  uuidv4(), }  }}; };  for await (const chunk of await graph.stream({ age: undefined, name: undefined }, config)) { for  await  (const  chunk  of  await  graph. stream({ age:  undefined,  name:  undefined  },  config))  { console.log(chunk);  console. log(chunk);} }  for await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( new Command({ resume: "John", update: { name: "foo" } }),  new  Command({ resume:  "John",  update:  { name:  "foo"  }  }),  config  config)) { ))  { console.log(chunk);  console. log(chunk);} }\n```\n\n```\n{ __interrupt__: [{ { __interrupt__:  [{ value: \'what is your name?\',  value:  \'what is your name?\', resumable: true,  resumable:  true, ns: [\'human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba\'],  ns:  [\'human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba\'], when: \'during\'  when:  \'during\'}]} }]}Name: N/A. Age: John Name:  N/ A.  Age:  John{ human_node: { age: \'John\', name: \'N/A\' } } { human_node:  { age:  \'John\',  name:  \'N/A\'  }  }\n```\n\n## 附加资源 📚[¶](#additional-resources "Permanent link")\n\n* [**概念指南：持久化**](../persistence/#replay)：阅读持久化指南以获取有关重放的更多上下文。\n* [**操作指南：人机协作**](/langgraphjs/how-tos/#human-in-the-loop)：了解如何在 LangGraph 中实现人机协作工作流。\n* [**如何实现多轮对话**](/langgraphjs/how-tos/multi-agent-multi-turn-convo)：了解如何在 LangGraph 中实现多轮对话。', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraphjs/concepts/human_in_the_loop/', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.99990237, 'saved_path': None}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:19,710 - __main__ - INFO - handle_download: searcher=TavilySearch, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:19,711 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:19,711 - __main__ - INFO - call_tool payload: source_tool=tavily_download, result_type=papers, count=1
2026-02-22 20:38:19,711 - __main__ - INFO - call_tool: name=tavily_download, result_type=papers, count=1
2026-02-22 20:38:19,711 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '人机协作(Human-in-the-Loop) - LangChain 文档', 'authors': [], 'abstract': '* [用例](#use-cases)\n* [中断](#interrupt)\n* [要求](#requirements)\n* [设计模式](#design-patterns) \n\n  + [批准或拒绝](#approve-or-reject)\n  + [审查与编辑状态](#review-edit-state)\n  + [审查工具调用](#review-tool-calls)\n  + [多轮对话](#multi-turn-conversation)\n  + [验证人类输入](#validating-human-input)\n* [Command 原语](#the-command-primitive)\n* [与 invoke 结合使用](#using-with-invoke)\n* [从中断恢复如何工作？](#how-does-resuming-from-an-interrupt-work)\n* [常见陷阱](#common-pitfalls) \n\n  + [副作用](#side-effects)\n  + [作为函数调用的子图](#subgraphs-called-as-functions)\n  + [使用多个中断](#using-multiple-interrupts)\n* [附加资源 📚](#additional-resources)\n\n1. [LangGraph](../..)\n2. [指南](../../how-tos/)\n3. [概念](../)\n4. [LangGraph](../../concepts#langgraph)\n\n# 人机协作 (Human-in-the-loop)[¶](#human-in-the-loop "Permanent link")\n\n本指南使用新的 `interrupt` 函数。\n\n自 LangGraph 0.2.31 起，推荐使用 [`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 来设置断点，因为它简化了**人机协作（human-in-the-loop）**模式。\n\n如果您正在寻找此概念指南的先前版本，该版本依赖于静态断点和 `NodeInterrupt` 异常，请点击[此处](../v0-human-in-the-loop/)。\n\n**人机协作（human-in-the-loop）**（或“在环”）工作流将人类输入集成到自动化流程中，允许在关键阶段进行决策、验证或更正。这在**基于 LLM 的应用程序**中尤其有用，因为底层模型可能会偶尔产生不准确的内容。在合规、决策或内容生成等低容错场景中，人类参与通过允许审查、更正或覆盖模型输出来确保可靠性。\n\n## 用例[¶](#use-cases "Permanent link")\n\n基于 LLM 应用程序中**人机协作**工作流的主要用例包括：\n\n1. [**🛠️ 审查工具调用**](#review-tool-calls)：人类可以在工具执行前审查、编辑或批准 LLM 请求的工具调用。\n2. **✅ 验证 LLM 输出**：人类可以审查、编辑或批准 LLM 生成的内容。\n3. **💡 提供上下文**：使 LLM 能够明确请求人类输入以进行澄清或提供额外细节，或支持多轮对话。\n\n## `interrupt`[¶](#interrupt "Permanent link")\n\nLangGraph 中的 [`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 通过在特定节点暂停图，向人类呈现信息，并用他们的输入恢复图，从而实现人机协作工作流。此函数对于批准、编辑或收集额外输入等任务非常有用。[`interrupt` 函数](/langgraphjs/reference/functions/langgraph.interrupt-1.html) 与 [`Command`](/langgraphjs/reference/classes/langgraph.Command.html) 对象结合使用，以人类提供的值恢复图。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { const value = interrupt(  const  value  =  interrupt( // Any JSON serializable value to surface to the human.  // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state  // For example, a question or a piece of text or a set of keys in the state {  { text_to_revise: state.some_text,  text_to_revise:  state.some_text, }  } );  ); // Update the state with the human\'s input or route the graph based on the input  // Update the state with the human\'s input or route the graph based on the input return {  return  { some_text: value,  some_text:  value, };  };} }  const graph = workflow.compile({ const  graph  =  workflow. compile({ checkpointer, // Required for `interrupt` to work  checkpointer,  // Required for `interrupt` to work}); });  // Run the graph until the interrupt // Run the graph until the interruptconst threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke(someInput, threadConfig); await  graph. invoke(someInput,  threadConfig);  // Below code can run some amount of time later and/or in a different process // Below code can run some amount of time later and/or in a different process  // Human input // Human inputconst valueFromHuman = "..."; const  valueFromHuman  =  "...";  // Resume the graph with the human\'s input // Resume the graph with the human\'s inputawait graph.invoke(new Command({ resume: valueFromHuman }), threadConfig); await  graph. invoke(new  Command({ resume:  valueFromHuman  }),  threadConfig);\n```\n\n```\n{ { some_text: "Edited text";  some_text:  "Edited text";} }\n```\n\n 完整代码\n\n以下是关于如何在图中使用 `interrupt` 的完整示例，如果您想查看代码的实际运行情况。\n\n```\nimport { MemorySaver, Annotation, interrupt, Command, StateGraph } from "@langchain/langgraph"; import  { MemorySaver,  Annotation,  interrupt,  Command,  StateGraph  }  from  "@langchain/langgraph";  // Define the graph state // Define the graph stateconst StateAnnotation = Annotation.Root({ const  StateAnnotation  =  Annotation. Root({ some_text: Annotation<string>()  some_text:  Annotation< string>()}); });  function humanNode(state: typeof StateAnnotation.State) { function  humanNode(state:  typeof  StateAnnotation. State)  { const value = interrupt(  const  value  =  interrupt( // Any JSON serializable value to surface to the human.  // Any JSON serializable value to surface to the human. // For example, a question or a piece of text or a set of keys in the state  // For example, a question or a piece of text or a set of keys in the state {  { text_to_revise: state.some_text  text_to_revise:  state.some_text }  } );  ); return {  return  { // Update the state with the human\'s input  // Update the state with the human\'s input some_text: value  some_text:  value };  };} }  // Build the graph // Build the graphconst workflow = new StateGraph(StateAnnotation) const  workflow  =  new  StateGraph(StateAnnotation)// Add the human-node to the graph // Add the human-node to the graph .addNode("human_node", humanNode)  . addNode("human_node",  humanNode) .addEdge("__start__", "human_node")  . addEdge("__start__",  "human_node")  // A checkpointer is required for `interrupt` to work. // A checkpointer is required for `interrupt` to work.const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();const graph = workflow.compile({ const  graph  =  workflow. compile({ checkpointer  checkpointer}); });  // Using stream() to directly surface the `__interrupt__` information. // Using stream() to directly surface the `__interrupt__` information.for await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( { some_text: "Original text" },  { some_text:  "Original text"  },  threadConfig  threadConfig)) { ))  { console.log(chunk);  console. log(chunk);} }  // Resume using Command // Resume using Commandfor await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( new Command({ resume: "Edited text" }),  new  Command({ resume:  "Edited text"  }),  threadConfig  threadConfig)) { ))  { console.log(chunk);  console. log(chunk);} }\n```\n\n```\n{ { __interrupt__: [  __interrupt__:  [ {  { value: { question: \'Please revise the text\', some_text: \'Original text\' },  value:  { question:  \'Please revise the text\',  some_text:  \'Original text\'  }, resumable: true,  resumable:  true, ns: [\'human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6\'],  ns:  [\'human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6\'], when: \'during\'  when:  \'during\' }  } ]  ]} }{ human_node: { some_text: \'Edited text\' } } { human_node:  { some_text:  \'Edited text\'  }  }\n```\n\n## 要求[¶](#requirements "永久链接")\n\n要在图中使用 `interrupt`，您需要：\n\n1. [**指定检查点**](../persistence/#checkpoints) 以在每一步之后保存图状态。\n2. 在适当的位置**调用 `interrupt()`**。请参阅[设计模式](#design-patterns)部分以获取示例。\n3. 使用[**线程 ID**](../persistence/#threads) **运行图**，直到触发 `interrupt`。\n4. 使用 `invoke`/`stream` **恢复执行**（请参阅[**`Command` 原语**](#the-command-primitive)）。\n\n## 设计模式[¶](#design-patterns "Permanent link")\n\n通常，您可以通过人机协作工作流执行三种不同的**操作**：\n\n1. **批准或拒绝**：在关键步骤（例如 API 调用）之前暂停图，以审查和批准操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。此模式通常涉及根据人类的输入对图进行**路由**。\n2. **编辑图状态**：暂停图以审查和编辑图状态。这对于纠正错误或使用附加信息更新状态很有用。此模式通常涉及使用人类的输入**更新**状态。\n3. **获取输入**：在图的特定步骤中明确请求人类输入。这对于收集额外信息或上下文以指导代理的决策过程或支持**多轮对话**很有用。\n\n下面我们展示了可以使用这些**操作**实现的不同设计模式。\n\n**注意：** `interrupt` 函数通过抛出特殊的 `GraphInterrupt` 错误来传播。因此，您应该避免在 `interrupt` 函数周围使用 `try/catch` 块——如果确实使用了，请确保在 `catch` 块中再次抛出 `GraphInterrupt` 错误。\n\n### 批准或拒绝[¶](#approve-or-reject "Permanent link")\n\n在关键步骤（例如 API 调用）之前暂停图，以审查和批准操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。\n\n```\nimport { interrupt, Command } from "@langchain/langgraph"; import  { interrupt,  Command  }  from  "@langchain/langgraph";  function humanApproval(state: typeof GraphAnnotation.State): Command { function  humanApproval(state:  typeof  GraphAnnotation. State):  Command  { const isApproved = interrupt({  const  isApproved  =  interrupt({ question: "Is this correct?",  question:  "Is this correct?", // Surface the output that should be  // Surface the output that should be // reviewed and approved by the human.  // reviewed and approved by the human. llm_output: state.llm_output,  llm_output:  state.llm_output, });  });   if (isApproved) {  if  (isApproved)  { return new Command({ goto: "some_node" });  return  new  Command({ goto:  "some_node"  }); } else {  }  else  { return new Command({ goto: "another_node" });  return  new  Command({ goto:  "another_node"  }); }  }} }  // Add the node to the graph in an appropriate location // Add the node to the graph in an appropriate location// and connect it to the relevant nodes. // and connect it to the relevant nodes.const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_approval", humanApproval)  . addNode("human_approval",  humanApproval) .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with either an approval or rejection. // Resume it with either an approval or rejection.const threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke(new Command({ resume: true }), threadConfig); await  graph. invoke(new  Command({ resume:  true  }),  threadConfig);\n```\n\n有关更详细的示例，请参阅[如何审查工具调用](/langgraphjs/how-tos/review-tool-calls)。\n\n### 审查与编辑状态[¶](#review-edit-state "Permanent link")\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanEditing(state: typeof GraphAnnotation.State): Command { function  humanEditing(state:  typeof  GraphAnnotation. State):  Command  { const result = interrupt({  const  result  =  interrupt({ // Interrupt information to surface to the client.  // Interrupt information to surface to the client. // Can be any JSON serializable value.  // Can be any JSON serializable value. task: "Review the output from the LLM and make any necessary edits.",  task:  "Review the output from the LLM and make any necessary edits.", llm_generated_summary: state.llm_generated_summary,  llm_generated_summary:  state.llm_generated_summary, });  });   // Update the state with the edited text  // Update the state with the edited text return {  return  { llm_generated_summary: result.edited_text,  llm_generated_summary:  result.edited_text, };  };} }  // Add the node to the graph in an appropriate location // Add the node to the graph in an appropriate location// and connect it to the relevant nodes. // and connect it to the relevant nodes.const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_editing", humanEditing)  . addNode("human_editing",  humanEditing) .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with the edited text. // Resume it with the edited text.const threadConfig = { configurable: { thread_id: "some_id" } }; const  threadConfig  =  { configurable:  { thread_id:  "some_id"  }  };await graph.invoke( await  graph. invoke( new Command({ resume: { edited_text: "The edited text" } }),  new  Command({ resume:  { edited_text:  "The edited text"  }  }),  threadConfig  threadConfig); );\n```\n\n有关更详细的示例，请参阅[如何使用中断等待用户输入](/langgraphjs/how-tos/wait-user-input)。\n\n### 审查工具调用[¶](#review-tool-calls "Permanent link")\n\n```\nimport { interrupt, Command } from "@langchain/langgraph"; import  { interrupt,  Command  }  from  "@langchain/langgraph";  function humanReviewNode(state: typeof GraphAnnotation.State): Command { function  humanReviewNode(state:  typeof  GraphAnnotation. State):  Command  { // This is the value we\'ll be providing via Command.resume()  // This is the value we\'ll be providing via Command.resume() const humanReview = interrupt({  const  humanReview  =  interrupt({ question: "Is this correct?",  question:  "Is this correct?", // Surface tool calls for review  // Surface tool calls for review tool_call: toolCall,  tool_call:  toolCall, });  });   const [reviewAction, reviewData] = humanReview;  const  [reviewAction,  reviewData]  =  humanReview;   // Approve the tool call and continue  // Approve the tool call and continue if (reviewAction === "continue") {  if  (reviewAction  ===  "continue")  { return new Command({ goto: "run_tool" });  return  new  Command({ goto:  "run_tool"  }); }  } // Modify the tool call manually and then continue  // Modify the tool call manually and then continue else if (reviewAction === "update") {  else  if  (reviewAction  ===  "update")  { const updatedMsg = getUpdatedMsg(reviewData);  const  updatedMsg  =  getUpdatedMsg(reviewData); // Remember that to modify an existing message you will need  // Remember that to modify an existing message you will need // to pass the message with a matching ID.  // to pass the message with a matching ID. return new Command({  return  new  Command({ goto: "run_tool",  goto:  "run_tool", update: { messages: [updatedMsg] },  update:  { messages:  [updatedMsg]  }, });  }); }  } // Give natural language feedback, and then pass that back to the agent  // Give natural language feedback, and then pass that back to the agent else if (reviewAction === "feedback") {  else  if  (reviewAction  ===  "feedback")  { const feedbackMsg = getFeedbackMsg(reviewData);  const  feedbackMsg  =  getFeedbackMsg(reviewData); return new Command({  return  new  Command({ goto: "call_llm",  goto:  "call_llm", update: { messages: [feedbackMsg] },  update:  { messages:  [feedbackMsg]  }, });  }); }  }} }\n```\n\n有关更详细的示例，请参阅[如何审查工具调用](/langgraphjs/how-tos/review-tool-calls)。\n\n### 多轮对话[¶](#multi-turn-conversation "Permanent link")\n\n**多轮对话**涉及代理和人类之间的多次来回交互，这可以允许代理以对话方式从人类那里收集额外信息。\n\n这种设计模式在由[多个代理](../multi_agent/)组成的 LLM 应用程序中很有用。一个或多个代理可能需要与人类进行多轮对话，其中人类在对话的不同阶段提供输入或反馈。为简单起见，下面的代理实现被说明为单个节点，但实际上它可能是由多个节点组成的更大图的一部分，并包含条件边。\n\n在此模式中，每个代理都有自己的人类节点用于收集用户输入。\n\n这可以通过为人类节点使用唯一名称（例如，“代理 1 的人类节点”，“代理 2 的人类节点”）或使用子图（其中子图包含人类节点和代理节点）来实现。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanInput(state: typeof GraphAnnotation.State) { function  humanInput(state:  typeof  GraphAnnotation. State)  { const humanMessage = interrupt("human_input");  const  humanMessage  =  interrupt("human_input");   return {  return  { messages: [  messages:  [ {  { role: "human",  role:  "human", content: humanMessage  content:  humanMessage }  } ]  ] };  };} }  function agent(state: typeof GraphAnnotation.State) { function  agent(state:  typeof  GraphAnnotation. State)  { // Agent logic  // Agent logic // ...  // ...} }  const graph = graphBuilder const  graph  =  graphBuilder .addNode("human_input", humanInput)  . addNode("human_input",  humanInput) .addEdge("human_input", "agent")  . addEdge("human_input",  "agent") .compile({ checkpointer });  . compile({ checkpointer  });  // After running the graph and hitting the interrupt, the graph will pause. // After running the graph and hitting the interrupt, the graph will pause.// Resume it with the human\'s input. // Resume it with the human\'s input.await graph.invoke( await  graph. invoke( new Command({ resume: "hello!" }),  new  Command({ resume:  "hello!"  }),  threadConfig  threadConfig); );\n```\n\n在此模式中，单个人类节点用于收集多个代理的用户输入。活动代理从状态中确定，因此在收集人类输入后，图可以路由到正确的代理。\n\n```\nimport { interrupt, Command, MessagesAnnotation } from "@langchain/langgraph"; import  { interrupt,  Command,  MessagesAnnotation  }  from  "@langchain/langgraph";  function humanNode(state: typeof MessagesAnnotation.State): Command { function  humanNode(state:  typeof  MessagesAnnotation. State):  Command  { /**  /** * A node for collecting user input.  * A node for collecting user input. */  */ const userInput = interrupt("Ready for user input.");  const  userInput  =  interrupt("Ready for user input.");   // Determine the **active agent** from the state, so  // Determine the **active agent** from the state, so // we can route to the correct agent after collecting input.  // we can route to the correct agent after collecting input. // For example, add a field to the state or use the last active agent.  // For example, add a field to the state or use the last active agent. // or fill in `name` attribute of AI messages generated by the agents.  // or fill in `name` attribute of AI messages generated by the agents. const activeAgent = ...;  const  activeAgent  =  ...;   return new Command({  return  new  Command({ goto: activeAgent,  goto:  activeAgent, update: {  update:  { messages: [{  messages:  [{ role: "human",  role:  "human", content: userInput,  content:  userInput, }]  }] }  } });  });} }\n```\n\n有关更详细的示例，请参阅[如何实现多轮对话](/langgraphjs/how-tos/multi-agent-multi-turn-convo)。\n\n### 验证人类输入[¶](#validating-human-input "Permanent link")\n\n如果您需要在图本身中（而不是在客户端）验证人类提供的输入，可以通过在单个节点中使用多个中断调用来实现。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */ let question = "What is your age?";  let  question  =  "What is your age?";   while (true) {  while  (true)  { const answer = interrupt(question);  const  answer  =  interrupt(question);   // Validate answer, if the answer isn\'t valid ask for input again.  // Validate answer, if the answer isn\'t valid ask for input again. if (typeof answer !== "number" || answer < 0) {  if  (typeof  answer  !==  "number"  ||  answer  <  0)  { question = `\'${answer}\' is not a valid age. What is your age?`;  question  =  `\'${answer}\' is not a valid age. What is your age?`; continue;  continue; } else {  }  else  { // If the answer is valid, we can proceed.  // If the answer is valid, we can proceed. break;  break; }  } }  }   console.log(`The human in the loop is ${answer} years old.`);  console. log(`The human in the loop is ${answer} years old.`);   return {  return  { age: answer,  age:  answer, };  };} }\n```\n\n## Command 原语[¶](#the-command-primitive "Permanent link")\n\n当使用 `interrupt` 函数时，图将在中断处暂停并等待用户输入。\n\n图的执行可以使用 [Command](/langgraphjs/reference/classes/langgraph.Command.html) 原语恢复，该原语可以通过 `invoke` 或 `stream` 方法传递。\n\n`Command` 原语提供了几个选项来控制和修改恢复期间图的状态：\n\n1. **将值传递给 `interrupt`**：使用 `new Command({ resume: value })` 向图提供数据，例如用户的响应。执行从使用 `interrupt` 的节点的开头恢复，但是，这次 `interrupt(...)` 调用将返回在 `new Command({ resume: value })` 中传递的值，而不是暂停图。\n\n```\n// Resume graph execution with the user\'s input. // Resume graph execution with the user\'s input.await graph.invoke(new Command({ resume: { age: "25" } }), threadConfig); await  graph. invoke(new  Command({ resume:  { age:  "25"  }  }),  threadConfig);\n```\n\n1. **更新图状态**：使用 `Command({ goto: ..., update: ... })` 修改图状态。请注意，恢复从使用 `interrupt` 的节点的开头开始。执行从使用 `interrupt` 的节点的开头恢复，但带有更新后的状态。\n\n```\n// Update the graph state and resume. // Update the graph state and resume.// You must provide a `resume` value if using an `interrupt`. // You must provide a `resume` value if using an `interrupt`.await graph.invoke( await  graph. invoke( new Command({ resume: "Let\'s go!!!", update: { foo: "bar" } }),  new  Command({ resume:  "Let\'s go!!!",  update:  { foo:  "bar"  }  }),  threadConfig  threadConfig); );\n```\n\n通过利用 `Command`，您可以恢复图的执行，处理用户输入，并动态调整图的状态。\n\n## 与 `invoke` 结合使用[¶](#using-with-invoke "Permanent link")\n\n当您使用 `stream` 运行图时，您将收到一个 `Interrupt` 事件，它会通知您 `interrupt` 已被触发。\n\n`invoke` 不会返回中断信息。要访问此信息，您必须在调用 `invoke` 后使用 [getState](/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#getState) 方法检索图状态。\n\n```\n// Run the graph up to the interrupt // Run the graph up to the interruptconst result = await graph.invoke(inputs, threadConfig); const  result  =  await  graph. invoke(inputs,  threadConfig);  // Get the graph state to get interrupt information. // Get the graph state to get interrupt information.const state = await graph.getState(threadConfig); const  state  =  await  graph. getState(threadConfig);  // Print the state values // Print the state valuesconsole.log(state.values); console. log(state. values);  // Print the pending tasks // Print the pending tasksconsole.log(state.tasks); console. log(state. tasks);  // Resume the graph with the user\'s input. // Resume the graph with the user\'s input.await graph.invoke(new Command({ resume: { age: "25" } }), threadConfig); await  graph. invoke(new  Command({ resume:  { age:  "25"  }  }),  threadConfig);\n```\n\n```\n{ { foo: "bar";  foo:  "bar";} // State values }  // State values  [ [ {  { id: "5d8ffc92-8011-0c9b-8b59-9d3545b7e553",  id:  "5d8ffc92-8011-0c9b-8b59-9d3545b7e553", name: "node_foo",  name:  "node_foo", path: ["__pregel_pull", "node_foo"],  path:  ["__pregel_pull",  "node_foo"], error: null,  error:  null, interrupts: [  interrupts:  [ {  { value: "value_in_interrupt",  value:  "value_in_interrupt", resumable: true,  resumable:  true, ns: ["node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553"],  ns:  ["node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553"], when: "during",  when:  "during", },  }, ],  ], state: null,  state:  null, result: null,  result:  null, },  },]; // Pending tasks. interrupts ];  // Pending tasks. interrupts\n```\n\n## 从中断恢复如何工作？[¶](#how-does-resuming-from-an-interrupt-work "Permanent link")\n\n使用 `interrupt` 的一个关键方面是理解恢复的工作原理。当您在 `interrupt` 后恢复执行时，图的执行从上次触发 `interrupt` 的**图节点**的**开头**开始。\n\n从节点开头到 `interrupt` 的**所有**代码都将重新执行。\n\n```\nlet counter = 0; let  counter  =  0;  function node(state: State) { function  node(state:  State)  { // All the code from the beginning of the node to the interrupt will be re-executed  // All the code from the beginning of the node to the interrupt will be re-executed // when the graph resumes.  // when the graph resumes. counter += 1;  counter  +=  1;   console.log(`> Entered the node: ${counter} # of times`);  console. log(`> Entered the node: ${counter}  # of times`);   // Pause the graph and wait for user input.  // Pause the graph and wait for user input. const answer = interrupt();  const  answer  =  interrupt();   console.log("The value of counter is:", counter);  console. log("The value of counter is:",  counter); // ...  // ...} }\n```\n\n在**恢复**图时，计数器将第二次递增，导致以下输出：\n\n```\n> Entered the node: 2 # of times >  Entered  the  node:  2  #  of  timesThe value of counter is: 2 The  value  of  counter  is:  2\n```\n\n## 常见陷阱[¶](#common-pitfalls "永久链接")\n\n### 副作用[¶](#side-effects "Permanent link")\n\n将带有副作用的代码（例如 API 调用）放在 `interrupt` **之后**，以避免重复，因为这些代码在每次节点恢复时都会重新触发。\n\n当节点从 `interrupt` 恢复时，此代码将再次重新执行 API 调用。如果 API 调用不是幂等的或者成本很高，这可能会导致问题。\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */ apiCall(); // This code will be re-executed when the node is resumed.  apiCall();  // This code will be re-executed when the node is resumed.   const answer = interrupt(question);  const  answer  =  interrupt(question);} }\n```\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */   const answer = interrupt(question);  const  answer  =  interrupt(question);   apiCall(answer); // OK as it\'s after the interrupt  apiCall(answer);  // OK as it\'s after the interrupt} }\n```\n\n```\nimport { interrupt } from "@langchain/langgraph"; import  { interrupt  }  from  "@langchain/langgraph";  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { /**  /** * Human node with validation.  * Human node with validation. */  */   const answer = interrupt(question);  const  answer  =  interrupt(question);   return {  return  { answer  answer };  };} }  function apiCallNode(state: typeof GraphAnnotation.State) { function  apiCallNode(state:  typeof  GraphAnnotation. State)  { apiCall(); // OK as it\'s in a separate node  apiCall();  // OK as it\'s in a separate node} }\n```\n\n### 作为函数调用的子图[¶](#subgraphs-called-as-functions "Permanent link")\n\n当[作为函数](../low_level/#as-a-function)调用子图时，**父图**将从调用子图的**节点开头**（以及触发 `interrupt` 的地方）恢复执行。同样，**子图**将从调用 `interrupt()` 函数的**节点开头**恢复。\n\n例如：\n\n```\nasync function nodeInParentGraph(state: typeof GraphAnnotation.State) { async  function  nodeInParentGraph(state:  typeof  GraphAnnotation. State)  { someCode(); // <-- This will re-execute when the subgraph is resumed.  someCode();  // <-- This will re-execute when the subgraph is resumed. // Invoke a subgraph as a function.  // Invoke a subgraph as a function. // The subgraph contains an `interrupt` call.  // The subgraph contains an `interrupt` call. const subgraphResult = await subgraph.invoke(someInput);  const  subgraphResult  =  await  subgraph. invoke(someInput); ...  ...} }\n```\n\n **示例：父图和子图的执行流程**\n\n假设我们有一个包含 3 个节点的父图：\n\n**父图**：`node_1` → `node_2`（子图调用） → `node_3`\n\n子图有 3 个节点，其中第二个节点包含 `interrupt`：\n\n**子图**：`sub_node_1` → `sub_node_2`（`interrupt`） → `sub_node_3`\n\n恢复图时，执行将按以下方式进行：\n\n1. **跳过父图中的 `node_1`**（已执行，图状态已保存为快照）。\n2. **从头开始重新执行父图中的 `node_2`**。\n3. **跳过子图中的 `sub_node_1`**（已执行，图状态已保存为快照）。\n4. **从头开始重新执行子图中的 `sub_node_2`**。\n5. 继续执行 `sub_node_3` 和后续节点。\n\n这是一个缩写的示例代码，您可以用来理解子图如何与中断一起工作。它计算每个节点进入的次数并打印计数。\n\n```\nimport { import  { StateGraph,  StateGraph, START,  START, interrupt,  interrupt, Command,  Command, MemorySaver,  MemorySaver,  Annotation  Annotation} from "@langchain/langgraph"; }  from  "@langchain/langgraph";  const GraphAnnotation = Annotation.Root({ const  GraphAnnotation  =  Annotation. Root({ stateCounter: Annotation<number>({  stateCounter:  Annotation< number>({ reducer: (a, b) => a + b,  reducer:  (a,  b)  =>  a  +  b, default: () => 0  default:  ()  =>  0 })  })}) })  let counterNodeInSubgraph = 0; let  counterNodeInSubgraph  =  0;  function nodeInSubgraph(state: typeof GraphAnnotation.State) { function  nodeInSubgraph(state:  typeof  GraphAnnotation. State)  { counterNodeInSubgraph += 1; // This code will **NOT** run again!  counterNodeInSubgraph  +=  1;  // This code will **NOT** run again! console.log(`Entered \'nodeInSubgraph\' a total of ${counterNodeInSubgraph} times`);  console. log(`Entered \'nodeInSubgraph\' a total of ${counterNodeInSubgraph}  times`); return {};  return  {};} }  let counterHumanNode = 0; let  counterHumanNode  =  0;  async function humanNode(state: typeof GraphAnnotation.State) { async  function  humanNode(state:  typeof  GraphAnnotation. State)  { counterHumanNode += 1; // This code will run again!  counterHumanNode  +=  1;  // This code will run again! console.log(`Entered humanNode in sub-graph a total of ${counterHumanNode} times`);  console. log(`Entered humanNode in sub-graph a total of ${counterHumanNode}  times`); const answer = await interrupt("what is your name?");  const  answer  =  await  interrupt("what is your name?"); console.log(`Got an answer of ${answer}`);  console. log(`Got an answer of ${answer} `); return {};  return  {};} }  const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();  const subgraphBuilder = new StateGraph(GraphAnnotation) const  subgraphBuilder  =  new  StateGraph(GraphAnnotation) .addNode("some_node", nodeInSubgraph)  . addNode("some_node",  nodeInSubgraph) .addNode("human_node", humanNode)  . addNode("human_node",  humanNode) .addEdge(START, "some_node")  . addEdge(START,  "some_node") .addEdge("some_node", "human_node")  . addEdge("some_node",  "human_node")const subgraph = subgraphBuilder.compile({ checkpointer }); const  subgraph  =  subgraphBuilder. compile({ checkpointer  });  let counterParentNode = 0; let  counterParentNode  =  0;  async function parentNode(state: typeof GraphAnnotation.State) { async  function  parentNode(state:  typeof  GraphAnnotation. State)  { counterParentNode += 1; // This code will run again on resuming!  counterParentNode  +=  1;  // This code will run again on resuming! console.log(`Entered \'parentNode\' a total of ${counterParentNode} times`);  console. log(`Entered \'parentNode\' a total of ${counterParentNode}  times`);   // Please note that we\'re intentionally incrementing the state counter  // Please note that we\'re intentionally incrementing the state counter // in the graph state as well to demonstrate that the subgraph update  // in the graph state as well to demonstrate that the subgraph update // of the same key will not conflict with the parent graph (until  // of the same key will not conflict with the parent graph (until const subgraphState = await subgraph.invoke(state);  const  subgraphState  =  await  subgraph. invoke(state); return subgraphState;  return  subgraphState;} }  const builder = new StateGraph(GraphAnnotation) const  builder  =  new  StateGraph(GraphAnnotation) .addNode("parent_node", parentNode)  . addNode("parent_node",  parentNode) .addEdge(START, "parent_node")  . addEdge(START,  "parent_node")  // A checkpointer must be enabled for interrupts to work! // A checkpointer must be enabled for interrupts to work!const graph = builder.compile({ checkpointer }); const  graph  =  builder. compile({ checkpointer  });  const config = { const  config  =  { configurable: {  configurable:  { thread_id: crypto.randomUUID(),  thread_id:  crypto.randomUUID(), }  }}; };  for await (const chunk of await graph.stream({ stateCounter: 1 }, config)) { for  await  (const  chunk  of  await  graph. stream({ stateCounter:  1  },  config))  { console.log(chunk);  console. log(chunk);} }  console.log(\'--- Resuming ---\'); console. log(\'--- Resuming ---\');  for await (const chunk of await graph.stream(new Command({ resume: "35" }), config)) { for  await  (const  chunk  of  await  graph. stream(new  Command({ resume:  "35"  }),  config))  { console.log(chunk);  console. log(chunk);} }\n```\n\n这将打印出\n\n```\n --- First invocation --- ---  First  invocation  ---In parent node: { foo: \'bar\' } In  parent  node:  { foo:  \'bar\'  } Entered \'parentNode\' a total of 1 times Entered  \'parentNode\'  a  total  of  1  times Entered \'nodeInSubgraph\' a total of 1 times Entered  \'nodeInSubgraph\'  a  total  of  1  timesEntered humanNode in sub-graph a total of 1 times Entered  humanNode  in  sub - graph  a  total  of  1  times{ __interrupt__: [{ value: \'what is your name?\', resumable: true, ns: [\'parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b\', \'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c\'], when: \'during\' }] } { __interrupt__:  [{ value:  \'what is your name?\',  resumable:  true,  ns:  [\'parent_node:0b23d72f-aaba-0329-1a59-ca4f3c8bad3b\',  \'human_node:25df717c-cb80-57b0-7410-44e20aac8f3c\'],  when:  \'during\'  }]  }  --- Resuming --- ---  Resuming  ---In parent node: { foo: \'bar\' } In  parent  node:  { foo:  \'bar\'  } Entered \'parentNode\' a total of 2 times Entered  \'parentNode\'  a  total  of  2  timesEntered humanNode in sub-graph a total of 2 times Entered  humanNode  in  sub - graph  a  total  of  2  times Got an answer of 35 Got  an  answer  of  35{ parent_node: null } { parent_node:  null  }\n```\n\n### 使用多个中断[¶](#using-multiple-interrupts "Permanent link")\n\n在**单个**节点中使用多个中断可能有助于实现诸如[验证人类输入](#validating-human-input)之类的模式。然而，如果在同一节点中使用多个中断且不小心处理，可能会导致意外行为。\n\n当一个节点包含多个中断调用时，LangGraph 会为执行该任务的节点保留一个特定于任务的恢复值列表。每当执行恢复时，它都会从节点的开头开始。对于遇到的每个中断，LangGraph 都会检查任务的恢复列表中是否存在匹配的值。匹配是**严格基于索引**的，因此中断调用在节点中的顺序至关重要。\n\n为避免问题，请避免在执行之间动态更改节点结构。这包括添加、删除或重新排序中断调用，因为此类更改可能导致索引不匹配。这些问题通常源于非常规模式，例如通过 `Command.resume(...).update(SOME_STATE_MUTATION)` 改变状态或依赖全局变量动态修改节点结构。\n\n 不正确的代码示例\n\n```\nimport { v4 as uuidv4 } from "uuid"; import  { v4  as  uuidv4  }  from  "uuid";import { import  { StateGraph,  StateGraph, MemorySaver,  MemorySaver, START,  START, interrupt,  interrupt, Command,  Command,  Annotation  Annotation} from "@langchain/langgraph"; }  from  "@langchain/langgraph";  const GraphAnnotation = Annotation.Root({ const  GraphAnnotation  =  Annotation. Root({ name: Annotation<string>(),  name:  Annotation< string>(), age: Annotation<string>()  age:  Annotation< string>()}); });  function humanNode(state: typeof GraphAnnotation.State) { function  humanNode(state:  typeof  GraphAnnotation. State)  { let name;  let  name; if (!state.name) {  if  (! state. name)  { name = interrupt("what is your name?");  name  =  interrupt("what is your name?"); } else {  }  else  { name = "N/A";  name  =  "N/A"; }  }   let age;  let  age; if (!state.age) {  if  (! state. age)  { age = interrupt("what is your age?");  age  =  interrupt("what is your age?"); } else {  }  else  { age = "N/A";  age  =  "N/A"; }  }   console.log(`Name: ${name}. Age: ${age}`);  console. log(`Name: ${name}. Age: ${age} `);   return {  return  { age,  age, name,  name, };  };} }  const builder = new StateGraph(GraphAnnotation) const  builder  =  new  StateGraph(GraphAnnotation) .addNode("human_node", humanNode);  . addNode("human_node",  humanNode); .addEdge(START, "human_node");  . addEdge(START,  "human_node");  // A checkpointer must be enabled for interrupts to work! // A checkpointer must be enabled for interrupts to work!const checkpointer = new MemorySaver(); const  checkpointer  =  new  MemorySaver();  const graph = builder.compile({ checkpointer }); const  graph  =  builder. compile({ checkpointer  });  const config = { const  config  =  { configurable: {  configurable:  { thread_id: uuidv4(),  thread_id:  uuidv4(), }  }}; };  for await (const chunk of await graph.stream({ age: undefined, name: undefined }, config)) { for  await  (const  chunk  of  await  graph. stream({ age:  undefined,  name:  undefined  },  config))  { console.log(chunk);  console. log(chunk);} }  for await (const chunk of await graph.stream( for  await  (const  chunk  of  await  graph. stream( new Command({ resume: "John", update: { name: "foo" } }),  new  Command({ resume:  "John",  update:  { name:  "foo"  }  }),  config  config)) { ))  { console.log(chunk);  console. log(chunk);} }\n```\n\n```\n{ __interrupt__: [{ { __interrupt__:  [{ value: \'what is your name?\',  value:  \'what is your name?\', resumable: true,  resumable:  true, ns: [\'human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba\'],  ns:  [\'human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba\'], when: \'during\'  when:  \'during\'}]} }]}Name: N/A. Age: John Name:  N/ A.  Age:  John{ human_node: { age: \'John\', name: \'N/A\' } } { human_node:  { age:  \'John\',  name:  \'N/A\'  }  }\n```\n\n## 附加资源 📚[¶](#additional-resources "Permanent link")\n\n* [**概念指南：持久化**](../persistence/#replay)：阅读持久化指南以获取有关重放的更多上下文。\n* [**操作指南：人机协作**](/langgraphjs/how-tos/#human-in-the-loop)：了解如何在 LangGraph 中实现人机协作工作流。\n* [**如何实现多轮对话**](/langgraphjs/how-tos/multi-agent-multi-turn-convo)：了解如何在 LangGraph 中实现多轮对话。', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraphjs/concepts/human_in_the_loop/', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.99990237, 'saved_path': '/home/qinshan/widthresearch/data/downloads/tavily_人机协作(Human.md'}}
2026-02-22 20:38:19,714 - __main__ - INFO - call_tool: name=exa_context_download, args={'papers': [{'paper_id': '', 'title': '概述 - LangChain 框架', 'authors': [], 'abstract': '概述 - LangChain 教程[跳到内容] \n**LangGraph 平台文档已迁移！**请在新的[LangChain 文档] 网站上查找 LangGraph 平台文档。[] # 人机协作(Human-in-the-loop)[¶] \n要在代理或工作流中审查、编辑和批准工具调用，请[使用 LangGraph 的人机协同（human-in-the-loop）功能] ，以便在工作流的任何时刻进行人工干预。这在大型语言模型 (LLM) 驱动的应用中尤其有用，因为模型输出可能需要验证、修正或额外的上下文。![image] \n提示有关如何使用人机协同的信息，请参阅[启用人工干预] 和[使用服务器 API 实现人机协同] 。\n## 核心功能[¶] \n* **持久化执行状态**：中断功能使用了 LangGraph 的[持久化] 层，该层会保存图的状态，从而可以无限期地暂停图的执行，直到您恢复为止。这是因为 LangGraph 在每一步之后都会为图状态创建检查点，这使得系统能够持久化执行上下文并在之后从中断处继续恢复工作流。这支持了没有时间限制的异步人工审查或输入。有两种暂停图的方法* [动态中断] ：在特定节点内部，根据图的当前状态使用`interrupt`来暂停图。\n* [静态中断] ：使用`interrupt\\_before`和`interrupt\\_after`在预定义的点暂停图，即在节点执行之前或之后。\n![image] 一个由3个顺序步骤组成的示例图，在 step\\_3 之前设有一个断点。* **灵活的集成点**：人机协同逻辑可以引入到工作流的任何一点。这允许有针对性的人工参与，例如批准 API 调用、修正输出或引导对话。## 模式[¶] \n您可以使用`interrupt`和`Command`实现四种典型的设计模式\n* [批准或拒绝] ：在关键步骤（如 API 调用）之前暂停图，以审查和批准该操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。此模式通常涉及根据人工输入来路由图。* [审查和编辑状态] ：暂停图以审查和编辑图状态。这对于纠正错误或用附加信息更新状态很有用。此模式通常涉及用人工输入来更新状态。\n* [审查工具调用] ：在工具执行之前，暂停图以审查和编辑 LLM 请求的工具调用。* [验证人工输入] ：在进入下一步之前，暂停图以验证人工输入。\n回到顶', 'doi': '', 'published_date': '2025-01-01T00:00:00+00:00', 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraph/concepts/human_in_the_loop', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:19,714 - __main__ - INFO - handle_download: searcher=ExaSearcherContext, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:19,714 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:19,714 - __main__ - INFO - call_tool payload: source_tool=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:19,714 - __main__ - INFO - call_tool: name=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:19,715 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '概述 - LangChain 框架', 'authors': [], 'abstract': '概述 - LangChain 教程[跳到内容] \n**LangGraph 平台文档已迁移！**请在新的[LangChain 文档] 网站上查找 LangGraph 平台文档。[] # 人机协作(Human-in-the-loop)[¶] \n要在代理或工作流中审查、编辑和批准工具调用，请[使用 LangGraph 的人机协同（human-in-the-loop）功能] ，以便在工作流的任何时刻进行人工干预。这在大型语言模型 (LLM) 驱动的应用中尤其有用，因为模型输出可能需要验证、修正或额外的上下文。![image] \n提示有关如何使用人机协同的信息，请参阅[启用人工干预] 和[使用服务器 API 实现人机协同] 。\n## 核心功能[¶] \n* **持久化执行状态**：中断功能使用了 LangGraph 的[持久化] 层，该层会保存图的状态，从而可以无限期地暂停图的执行，直到您恢复为止。这是因为 LangGraph 在每一步之后都会为图状态创建检查点，这使得系统能够持久化执行上下文并在之后从中断处继续恢复工作流。这支持了没有时间限制的异步人工审查或输入。有两种暂停图的方法* [动态中断] ：在特定节点内部，根据图的当前状态使用`interrupt`来暂停图。\n* [静态中断] ：使用`interrupt\\_before`和`interrupt\\_after`在预定义的点暂停图，即在节点执行之前或之后。\n![image] 一个由3个顺序步骤组成的示例图，在 step\\_3 之前设有一个断点。* **灵活的集成点**：人机协同逻辑可以引入到工作流的任何一点。这允许有针对性的人工参与，例如批准 API 调用、修正输出或引导对话。## 模式[¶] \n您可以使用`interrupt`和`Command`实现四种典型的设计模式\n* [批准或拒绝] ：在关键步骤（如 API 调用）之前暂停图，以审查和批准该操作。如果操作被拒绝，您可以阻止图执行该步骤，并可能采取替代操作。此模式通常涉及根据人工输入来路由图。* [审查和编辑状态] ：暂停图以审查和编辑图状态。这对于纠正错误或用附加信息更新状态很有用。此模式通常涉及用人工输入来更新状态。\n* [审查工具调用] ：在工具执行之前，暂停图以审查和编辑 LLM 请求的工具调用。* [验证人工输入] ：在进入下一步之前，暂停图以验证人工输入。\n回到顶', 'doi': '', 'published_date': '2025-01-01T00:00:00+00:00', 'pdf_url': '', 'url': 'https://github.langchain.ac.cn/langgraph/concepts/human_in_the_loop', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'saved_path': '/home/qinshan/widthresearch/data/downloads/exa_概述 - LangC.md'}}
2026-02-22 20:38:21,547 - __main__ - INFO - call_tool: name=wikipedia_search, args={'query': 'LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力'}
2026-02-22 20:38:21,547 - __main__ - INFO - handle_search: searcher=WikipediaSearcher, query=LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力, search_type=None
2026-02-22 20:38:21,614 - __main__ - INFO - call_tool: name=exa_context_search, args={'query': 'LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力'}
2026-02-22 20:38:21,614 - __main__ - INFO - handle_search: searcher=ExaSearcherContext, query=LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力, search_type=None
2026-02-22 20:38:21,632 - __main__ - INFO - call_tool: name=tavily_search, args={'query': 'LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力'}
2026-02-22 20:38:21,632 - __main__ - INFO - handle_search: searcher=TavilySearch, query=LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力, search_type=None
2026-02-22 20:38:22,238 - __main__ - WARNING - handle_search: returned=0 for query=LangGraph 在多智能体编排、生产部署和调试监控方面提供哪些高级能力
2026-02-22 20:38:22,239 - __main__ - INFO - call_tool payload: source_tool=wikipedia_search, result_type=papers, count=0
2026-02-22 20:38:22,239 - __main__ - INFO - call_tool: name=wikipedia_search, result_type=papers, count=0
2026-02-22 20:38:23,758 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:38:23,758 - __main__ - INFO - call_tool payload: source_tool=exa_context_search, result_type=papers, count=1
2026-02-22 20:38:23,758 - __main__ - INFO - call_tool: name=exa_context_search, result_type=papers, count=1
2026-02-22 20:38:23,758 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '在Google Cloud 上使用LangGraph、EDA 和生成式AI 构建 ...', 'authors': [], 'abstract': 'Aidemy：在 Google Cloud 上使用LangGraph、EDA 和生成式AI 构建多代理系统| Google Codelabs[跳至主要内容] \n[![Google Codelabs]] \n/\n* English\n* Deutsch\n* Español\n* Español –América Latina\n* Français\n* Indonesia\n* Italiano\n* Polski\n* Português –Brasil\n* Tiếng Việt\n* Türkçe\n* Русский* עברית* العربيّة* فارسی* हिंदी* বাংলা* ภาษาไทย* 中文–简体* 中文–繁體* 日本語* 한국어登录# Aidemy：在 Google Cloud 上使用LangGraph、EDA 和生成式AI 构建多代理系统## 1. 简介您好！您对智能体的概念很感兴趣，对吧？智能体是一种小助手，无需您动手即可帮您处理各种事务。太棒了！但实际上，一个代理并不总是能满足需求，尤其是在处理更大、更复杂的项目时。您可能需要一整个团队的他们！这时，多智能体系统就能派上用场了。与旧式的硬编码相比，由LLM 提供支持的代理可为您提供出色的灵活性。不过，总会有一些问题，这些模型也面临着一系列棘手的挑战。这正是我们将在本次研讨会中深入探讨的内容！![title] \n以下是您将学到的内容，可帮助您提升代理技能：**使用 LangGraph 构建您的第一个代理**：我们将使用热门框架 LangGraph 亲手构建您自己的代理。您将学习如何创建可连接到数据库的工具，如何利用最新的Gemini 2 API 进行一些互联网搜索，以及如何优化提示和回答，以便您的代理不仅可以与LLM 互动，还可以与现有服务互动。我们还将向您展示函数调用的运作方式。**以您的方式编排智能体**：我们将探索编排智能体的不同方式，从简单的直线路径到更复杂的多路径场景。您可以将其视为指导代理团队的运作。\n**多智能体系统**：您将了解如何设置一个智能体可以协作并共同完成任务的系统，而这一切都归功于事件驱动型架构。\n**LLM 自由**：选择最适合的 LLM：我们不必只使用一个 LLM！您将了解如何使用多个 LLM，为它们分配不同的角色，从而利用出色的“思维模型”来提升问题解决能力。\n**什么是动态内容？没问题！**：想象一下，您的代理可以实时创建专门为每位用户量身定制的动态内容。我们将向您展示如何操作！\n**使用 Google Cloud 将数据分析提升到新高度**：不要再局限于在笔记本中进行简单的数据分析。我们将向您展示如何在 Google Cloud 上设计和部署多代理系统，以便该系统能够投入实际应用！此项目将很好地展示如何使用我们讨论的所有技巧。## 2. 架构当老师或从事教育工作非常有意义，但我们必须承认，工作量（尤其是所有准备工作）可能非常具有挑战性！此外，学校往往没有足够的员工，辅导费用可能很高。因此，我们建议使用AI 赋能的教学助理。此工具可减轻教育工作者的负担，并帮助弥合因人员短缺和缺乏经济实惠的辅导而造成的差距。我们的AI 教学助理可以快速生成详细的课程计划、有趣的测验、易于理解的音频总结和个性化的作业。这样一来，教师就可以专注于自己最擅长的事情：与学生建立联系，帮助他们爱上学习。该系统包含两个网站：一个供教师创建未来几周的课程计划，![规划本] \n另一个供学生访问测验、音频总结和作业。![门户] \n好的，我们来了解一下为Aidemy 教学助理提供支持的架构。如您所见，我们已将其分解为几个关键组件，这些组件协同工作以实现此目的。![架构] \n**关键架构元素和技术**：\n**Google Cloud Platform (GCP)**：整个系统的核心：\n* Vertex AI：访问 Google 的Gemini LLM。\n* Cloud Run：用于部署容器化代理和函数的无服务器平台。\n* Cloud SQL：用于存储课程数据的 PostgreSQL 数据库。* Pub/Sub 和Eventarc：事件驱动型架构的基础，可实现组件之间的异步通信。\n* Cloud Storage：存储音频总结和作业文件。\n* Secret Manager：安全地管理数据库凭据。\n* Artifact Registry：存储代理的 Docker 映像。* Compute Engine：部署自托管 LLM，而不是依赖于供应商解决方案\n**LLM**：系统的“大脑”：\n* Google 的Gemini 模型：（Gemini x Pro、Gemini x Flash、Gemini x Flash Thinking）用于课程规划、内容生成、动态 HTML 创建、测验解释和作业合并。* DeepSeek：用于生成自学作业的专业任务\n**LangChain 和LangGraph**：用于 LLM 应用开发的框架* 有助于创建复杂的多智能体工作流。* 支持对工具（API 调用、数据库查询、网页搜索）进行智能编排。* 实现事件驱动型架构，以提高系统可伸缩性和灵活性。从本质上讲，我们的架构将LLM 的强大功能与结构化数据和事件驱动型通信相结合，所有这些都在Google Cloud 上运行。这使我们能够构建可扩缩、可靠且有效的教学助理。## 3. 准备工作**对于 Google Cloud 赠金**：为帮助您入门，请使用此[链接] 兑换免费 Google Cloud 赠金。您可以按照[此处的说明] 激活抵扣金额并创建新项目，也可以按照以下说明操作。\n在[Google Cloud 控制台] 的项目选择器页面上，选择或创建一个 Google Cloud[项目] 。确保您的 Cloud 项目已启用结算功能。[了解如何检查项目是否已启用结算功能] 。\n**在 Cloud Shell IDE 中启用Gemini Code Assist**\n👉在Google Cloud 控制台中，前往Gemini Code Assist 工具，同意相关条款及条件，即可免费启用Gemini Code Assist。\n![01-04-code-assist-enable.png] \n忽略权限设置，离开此页面。**在 Cloud Shell 编辑器中工作**\n👉点击Google Cloud 控制台顶部的**激活 Cloud Shell**（这是 Cloud Shell 窗格顶部的终端形状图标），然后点击“打开**编辑器**”按钮（看起来像一个带有铅笔的打开的文件夹）。此操作会在窗口中打开 Cloud Shell 代码编辑器。您会在左侧看到文件资源管理器。![Cloud Shell] \n👉如图所示，点击底部状态栏中的**Cloud Code 登录**按钮。按照说明对插件进行授权。如果您在状态栏中看到**Cloud Code - no project**，请选择该选项，然后在下拉菜单中选择“Select a Google Cloud Project”（选择 Google Cloud 项目），然后从您创建的项目列表中选择特定的Google Cloud 项目。![登录项目] \n👉在云IDE 中打开终端，![新终端] 或![新终端] \n👉在终端中，使用以下命令验证您是否已通过身份验证，以及项目是否已设置为您的项目ID：\n```\n`gcloudauthlist`\n```\n👉并运行，确保将*&lt;&lt;YOUR\\_PROJECT\\_ID\\>*替换为您的项目 ID：\n```\n`echo&lt;YOUR\\_PROJECT\\_ID&gt; &gt;\\~/project\\_id.txtgcloudconfigsetproject$(cat\\~/project\\_id.txt)`\n```\n👉运行以下命令以启用必要的Google Cloud API：\n```\n`gcloudservicesenablecompute.googleapis.com\\\\storage.googleapis.com\\\\run.googleapis.com\\\\artifactregistry.googleapis.com\\\\aiplatform.googleapis.com\\\\eventarc.googleapis.com\\\\sqladmin.googleapis.com\\\\secretmanager.googleapis.com\\\\cloudbuild.googleapis.com\\\\cloudresourcemanager.googleapis.com\\\\cloudfunctions.googleapis.com\\\\cloudaicompanion.googleapis.com`\n```\n这可能需要几分钟的时间。**设置权限**\n👉设置服务账号权限。在终端中，运行以下命令：```\n`gcloudconfigsetproject$(cat\\~/project\\_id.txt)exportPROJECT\\_ID=$(gcloudconfiggetproject)exportSERVICE\\_ACCOUNT\\_NAME=$(gcloudcomputeproject-infodescribe--format="value(defaultServiceAccount)")echo"Here\'s your SERVICE\\_ACCOUNT\\_NAME $SERVICE\\_ACCOUNT\\_NAME"`\n```\n👉授予权限。在终端中，运行以下命令：```\n`#Cloud Storage (Read/Write):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/storage.objectAdmin"#Pub/Sub (Publish/Receive):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/pubsub.publisher"gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/pubsub.subscriber"#Cloud SQL (Read/Write):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/cloudsql.editor"#Eventarc (Receive Events):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/iam.serviceAccountTokenCreator"gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/eventarc.eventReceiver"#Vertex AI (User):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/aiplatform.user"#Secret Manager (Read):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/secretmanager.secretAccessor"`\n```\n👉在[IAM 控制台] 中验证结果![IAM 控制台] \n👉在终端中运行以下命令，以创建名为**aidemy**的 Cloud SQL 实例。我们稍后会用到此功能，但由于此过程可能需要一些时间，因此我们现在就来完成。```\n`gcloudsqlinstancescreateaidemy\\\\--database-version=POSTGRES\\_14\\\\--cpu=2\\\\--memory=4GB\\\\--region=us-central1\\\\--root-password=1234qwer\\\\--storage-size=10GB\\\\--storage-auto-increase`\n```\n预配实例需要一段时间，请让此命令运行，然后继续执行下一步。## 4. 构建第一个代理在深入探讨复杂的多智能体系统之前，我们需要先建立一个基本构建块：单个功能性智能体。在本部分中，我们将迈出第一步，创建一个简单的“图书提供商”代理。图书提供商代理会接受一个类别作为输入，并使用Gemini LLM 生成该类别中的图书的JSON 表示形式。然后，它会以REST API 端点的形式提供这些图书推荐。![图书提供商]<image_link>\n👉在另一个浏览器标签页中，在网络浏览器中打开[Google Cloud 控制台]<web_link>。在导航菜单 (☰) 中，前往“Cloud Run”。点击“+ ... 编写函数”按钮。![创建函数]<image_link>\n👉接下来，我们将配置Cloud Run 函数的基本设置：* 服务名称：`book-provider`\n* 区域：`us-central1`\n* 运行时：`Python 3.12`\n* 身份验证：从`Allow unauthenticated invocations`更改为“已启用”。\n将“允许未经身份验证的调用”设置为启用后，外部流量无需身份验证即可访问该函数。👉将其他设置保留为默认值，然后点击**创建**。系统会将您转到源代码编辑器。\n您会看到预先填充的`main.py`和`requirements.txt`文件。\n`main.py`将包含函数的业务逻辑，`requirements.txt`将包含所需的软件包。\n👉现在，我们准备好编写一些代码了！不过，在深入了解之前，我们先看看**Gemini Code Assist**能否帮助我们抢占先机。**返回到 Cloud Shell 编辑器**，点击顶部的 Gemini Code Assist 图标，系统应会打开Gemini Code Assist 对话。![Gemini Code Assist]<image_link>\n您可能会看到一个弹出式窗口，其中包含类似“您缺少有效的Gemini Code Assist 许可，因此可能无法再访问该服务。请与您的结算管理员联系，购买或分配许可。”请忽略此消息。👉将以下请求粘贴到提示框中：```\n`Usethefunctions\\_frameworklibrarytobedeployableasanHTTPfunction.Acceptarequestwithcategoryandnumber\\_of\\_bookparameters(eitherinJSONbodyorquerystring).Uselangchainandgeminitogeneratethedataforbookwithfieldsbookname,author,publisher,publishing\\_date.UsepydantictodefineaBookmodelwiththefields:bookname(string,description:"Name of the book"),author(string,description:"Name of the author"),publisher(string,description:"Name of the publisher"),andpublishing\\_date(string,description:"Date of publishing").Uselangchainandgeminimodeltogeneratebookdata.theoutputshouldfollowtheformatdefinedinBookmodel.ThelogicshoulduseJsonOutputParserfromlangchaintoenforceoutputformatdefinedinBookModel.Haveafunctionget\\_recommended\\_books(category)thatinternallyuseslangchainandgeminitoreturnasinglebookobject.Themainfunction,exposedastheCloudFunction,shouldcallget\\_recommended\\_books()multipletimes(basedonnumber\\_of\\_book)andreturnaJSONlistofthegeneratedbookobjects.Handlethecasewherecategoryornumber\\_of\\_bookaremissingbyreturninganerrorJSONresponsewitha400statuscode.returnaJSONstringrepresentingtherecommendedbooks.useoslibrarytoretrieveGOOGLE\\_CLOUD\\_PROJECTenvvar.UseChatVertexAIfromlangchainfortheLLMcall`\n```\n然后，Code Assist 会生成一个潜在的解决方案，同时提供源代码和requirements.txt 依赖项文件。（请勿使用此代码）建议您将Code Assist 生成的代码与下面提供的经过测试的正确解决方案进行比较。这样，您就可以评估该工具的效果并发现任何潜在的差异。虽然绝不应盲目信任LLM，但 Code Assist 是一款出色的工具，可用于快速原型设计和生成初始代码结构，应使用它来获得良好的开端。由于这是一个研讨会，我们将继续使用下方提供的已验证代码。不过，您可以自行尝试使用Code Assist 生成的代码，以便更深入地了解其功能和局限性。👉返回到Cloud Run 函数的源代码编辑器（在另一个浏览器标签页中）。请仔细将`main.py`的现有内容替换为以下代码：\n```\n`importfunctions\\_frameworkimportjsonfromflaskimportFlask,jsonify,requestfromlangchain\\_google\\_vertexaiimportChatVertexAIfromlangchain\\_core.output\\_parsersimportJsonOutputParserfromlangchain\\_core.promptsimportPromptTemplatefrompydanticimportBaseModel,FieldimportosclassBook(BaseModel):bookname:str=Field(description="Name of the book")author:str=Field(description="Name of the author")publisher:str=Field(description="Name of the publisher")publishing\\_date:str=Field(description="Date of publishing")project\\_id=os.environ.get("GOOGLE\\_CLOUD\\_PROJECT")llm=ChatVertexAI(model\\_name="gemini-2.0-flash-lite-001")defget\\_recommended\\_books(category):"""A simple book recommendation function.Args:category (str): categoryReturns:str: A JSON string representing the recommended books."""parser=JsonOutputParser(pydantic\\_object=Book)question=f"Generate a random made up book on{category}with bookname, author and publisher and publishing\\_date"prompt=PromptTemplate(template="Answer the user query.\\\\n{format\\_instructions}\\\\n{query}\\\\n",input\\_variables=["query"],partial\\_variables={"format\\_instructions":parser.get\\_format\\_instructions()},)chain=prompt|llm|parserresponse=chain.invoke({"query":question})returnjson.dumps(response)@functions\\_framework.httpdefrecommended(request):request\\_json=request.get\\_json(silent=True)# Get JSON dataifrequest\\_jsonand\'category\'inrequest\\_jsonand\'number\\_of\\_book\'inrequest\\_json:category=request\\_json[\'category\']number\\_of\\_book=int(request\\_json[\'number\\_of\\_book\'])elifrequest.argsand\'category\'inrequest.argsand\'number\\_of\\_book\'inrequest.args:category=request.args.get(\'category\')number\\_of\\_book=int(request.args.get(\'number\\_of\\_book\'))else:returnjsonify({\'error\':\'Missing category or number\\_of\\_book parameters\'}),400recommendations\\_list=[]foriinrange(number\\_of\\_book)', 'doi': '', 'published_date': '2025-12-22T00:00:00+00:00', 'pdf_url': '', 'url': 'https://codelabs.developers.google.com/aidemy-multi-agent/instructions?hl=zh-cn', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}
2026-02-22 20:38:25,698 - __main__ - INFO - handle_search: returned=1
2026-02-22 20:38:25,698 - __main__ - INFO - call_tool payload: source_tool=tavily_search, result_type=papers, count=1
2026-02-22 20:38:25,698 - __main__ - INFO - call_tool: name=tavily_search, result_type=papers, count=1
2026-02-22 20:38:25,699 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': 'LangGraph 圣经：从0到1穿透multi-agent多智能体入门实战', 'authors': [], 'abstract': '* [博客园](https://www.cnblogs.com/)\n* [首页](https://www.cnblogs.com/crazymakercircle/)\n* [新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)\n* [联系](https://msg.cnblogs.com/send/%E6%8A%80%E6%9C%AF%E8%87%AA%E7%94%B1%E5%9C%88)\n* [订阅](javascript:void(0))\n* [管理](https://i.cnblogs.com/)\n\n# [LangGraph 圣经：从0到1穿透 multi-agent多智能体 入门实战](https://www.cnblogs.com/crazymakercircle/p/19412858 "发布于 2025-12-28 21:38")\n\n# 本文 的 原文 地址\n\n#### 原始的内容，请参考 本文 的 原文 地址\n\n[本文 的 原文 地址](https://mp.weixin.qq.com/s/3Unzc4dHuYDwQyBgBaRKtA)\n\n## 尼恩：LLM大模型学习圣经PDF的起源\n\n在40岁老架构师 尼恩的**读者交流群**(50+)中，经常性的指导小伙伴们改造简历。\n\n然而，其中一个成功案例，是一个9年经验 网易的小伙伴，当时拿到了一个年薪近**80W的大模型架构offer**，逆涨50%，那是在去年2023年的 5月。\n\n* [惊天大逆袭：8年小伙20天时间提75W年薪offer，逆涨50%，秘诀在这](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247491496&idx=1&sn=cb31f7510a7c2efb7daf6cad793860ad&scene=21#wechat_redirect)\n\n不到1年，小伙伴也在团队站稳了脚跟，成为了名副其实的大模型 应用 架构师。接下来，尼恩架构团队，通过 梳理一个《LLM大模型学习圣经》 帮助更多的人做LLM架构，拿到年薪100W, 这个内容体系包括下面的内容：\n\n* [《**Python学习圣经：从0到1精通Python，打好AI基础**》](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247504585&idx=1&sn=dcc5f35e6733d36d5bb74113833d6263&scene=21#wechat_redirect)\n* [《**LLM大模型学习圣经：从0到1吃透Transformer技术底座**》](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247502020&idx=1&sn=a446e3f018639b85e49748d393e7620c&scene=21#wechat_redirect)\n\n* [**《SpringCloud + Python 混合微服务架构，打造AI分布式业务应用的技术底层》**](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247502436&idx=1&sn=cfbf4aec8188cf6025f9a7d09965b809&scene=21#wechat_redirect)\n* 《LLM 智能体 学习圣经：从0到1吃透 LLM 智能体 的架构 与实操》\n* 《LLM 智能体 学习圣经：从0到1吃透 LLM 智能体 的 中台 架构 与实操》\n* 《[**Spring 集成 DeepSeek 的 3大方法，史上最全**](https://mp.weixin.qq.com/s/wWJ8-Py3c6FyjkpiKAKihQ)》\n* 《[基于Dify +Ollama+ Qwen2 完成本地 LLM 大模型应用实战](https://mp.weixin.qq.com/s/lruBtgNRouvoJVqsmRDfIg)》\n* 《**Spring AI 学习圣经 和配套视频** 》\n* 《[AI部署架构：A100、H100、A800、H800、H20的差异以及如何选型？开发、测试、生产环境如何进行部署架构？](https://mp.weixin.qq.com/s/m8_2my55QWhzfnDs70TTUQ)》\n* [**生产环境 K8S + Deepseek 实现大模型部署 和 容器调度（图解+史上最全）**](https://mp.weixin.qq.com/s/xagt2GYz_pKHkdePBZ6Y0Q)\n* [**《最近大火的 MCP 协议，看这篇文章就够了》**](https://mp.weixin.qq.com/s/jwzEFeHuB_k9BA7go8bNVg)\n* [《美团面试：LLM 大模型会有 什么问题？说说进行 RAG 优化的方法？》](https://mp.weixin.qq.com/s/ny7Y9-tIzepfSCR-RSuwbA)\n\n## LangGraph 圣经 介绍\n\n接下来，尼恩团队开始给大家写 LangGraph 学习圣经 ，包括：\n\n* LangGraph学习圣经 （1）： 从0到1穿透 multi-agent多智能体 入门实战\n* LangGraph学习圣经 （2）： 从0到1穿透LangGraph架构与源码\n* LangGraph学习圣经（3） : 基于langgraph的java 运维多智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent 等\n\n现本文是第一篇：从0到1穿透 multi-agent多智能体 入门实战\n\n## 第一章. LangGraph介绍，从单智能体 到多智能体 的演进\n\n### 1.1 、从“一个人独干(单智能体)”到“团队协作(多智能体)”的转变\n\n而现在的趋势是：**让专业的人干专业的事**。\n\n* 研究员智能体：专门负责找资料\n* 事实核查员：专挑毛病，验证真假\n* 写作助手：专注输出流畅内容\n* 编辑：最后润色把关\n\n研究数据显示，这种分工协作的方式，在处理复杂任务时性能提升 **40%-60%**，而且更容易调试、维护和扩展。\n\n### 1.2 一个 AI研究助手 多智能体例子\n\n**(1) 拆解问题**\n\n**(2) 查资料**\n\n**(3) 验证信息真伪**\n\n**(4) 输出一份结构清晰的研究报告**\n\n整个流程由多个AI智能体接力完成，背后靠的是 **LangGraph** 这个框架来调度协调。\n\n### 1.3什么是 LangGraph？\n\n普通的 AI 智能体 应用记不住事、理不清步骤，像个没头苍蝇。\n\n**怎么办？ 用“图”来设计 AI 流程** ：\n\n\n\nLangGraph 就是 **AI 的导演**，指挥模块按剧本走。\n\n比如做个旅游助手： 问目的地 → 查天气 → 推荐穿搭 → 生成行程。\n\n\n\nLangGraph是LangChain团队推出的开源框架，专为构建**有状态、长时间运行**的AI工作流而生。\n\n它不像别的工具那样, 封装太多细节.\n\nLangGraph 给你底层控制权，像搭积木一样组装AI系统。\n\nLangGraph 核心思想很简单：用“图”来建模AI行为，节点是动作，边是跳转逻辑，状态是记忆。\n\n> LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\n\n### 图结构三要素\n\n* **节点（Nodes）**：每个节点, 是一个独立任务. 比如调用大模型、查数据库、执行工具函数。\n* **边（Edges）**：每 一 边是Python函数，根据当前状态决定下一步去哪，支持条件判断和循环。\n* **状态（State）**：全局共享的数据结构，记录所有关键信息，相当于AI的“短期记忆”。\n\n有了这图结构三要素 ， 就能画出一条条清晰的执行路径，不怕逻辑混乱，也不怕断电重启丢数据。\n\n```\n from langgraph.graph import StateGraph, END # 示例：定义一个简单图 graph = StateGraph(dict) def node_a(state): return {"value": "from A"} def node_b(state): return {"value": "from B"} graph.add_node("A", node_a) graph.add_node("B", node_b) graph.set_entry_point("A") graph.add_edge("A", "B") graph.add_edge("B", END) app = graph.compile() \n```\n\n上面这段代码就是一个最简单的流程图：A → B → 结束。\n\n**(1) 开始节点 → 由 `graph.set_entry_point("A")`设定入口**\n\n**(2) 节点A → 执行 `node_a(state)`函数，返回 `{"value": "from A"}`**\n\n**(3) 节点B → 执行 `node_b(state)`函数，返回 `{"value": "from B"}`**\n\n**(4) 结束节点 → 由 `graph.add_edge("B", END)`设定终点**\n\n### 1.4 LangGraph 的核心优势\n\nLangGraph 本质是什么？ LangGraph 其实是一个 **“带记忆的、AI 任务图 、执行器”**。\n\n更技术一点说：\n\n它是一个**有状态、可持久化、基于图的工作流引擎**，灵感来自两种老但牛的技术：\n\n* **数据流编程**（Dataflow Programming）：数据推着计算走\n* **Actor 模型**：每个节点像个独立小演员，收到消息才行动\n\n这就让它特别适合做复杂AI流程，比如：\n\n* 自动规划任务\n* 多Agent协作\n* 出错自动重试\n* 用户中途修改需求也能接得上\n\n**LangGraph 的设计哲学很简单粗暴**：\n\n> **把AI应用当成一个永远在线的“活系统”，而不是一次性的函数调用。**\n\n你可以把它想象成一个**会记住上下文、能持续反应、还会自己做决定的智能机器人**，而不是一个“问一句答一句”的问答机。\n\n#### LangChain 和 LangChain 的区别？\n\n我们先来对比一下：\n\n* **LangChain**：像是流水线工人。\n\n  原料（输入）进来 → 经过几道工序（处理）→ 成品（输出）出去 → 完事，关机器。 下次再来？从头开始。\n* **LangGraph**：像是一个值班经理，24小时在岗。\n\n  他记得昨天客户说了啥，今天问题进展到哪了，还能根据情况叫同事帮忙、重试任务、甚至主动发消息提醒你。\n\n  LangGraph核心是：**状态一直在线，流程可以反复跳转。**\n\n### 1.6 LangGraph 的关键组件\n\n**问题根源：** 多个AI 任务，各模块各干各的，逻辑乱、状态丢、没法 debug。\n\n**解决思路：** 以“状态图”为核心，靠 **节点、边、状态** 三件套，构建可控可溯的工作流。\n\n三大支柱：\n\n* **节点（Nodes）**：执行单元，比如调大模型、查库；\n* **边（Edges）**：控制流程走向，可以固定跳转，也能条件判断；\n* **状态（State）**：共享内存，全程保存数据，上下文不断。\n\n支持循环：AI 生成内容后问用户“满意吗？” 不满意就回退修改，直到通过。\n\n支持人工干预：在流程中插入“等待确认”节点，处理完再继续。\n\n集成 RAG，先检索再回答，不说胡话；\n\n用 LangSmith 监控全过程，像 DevTools 一样看执行轨迹。\n\n### 1.5 LangGraph 五大能力：\n\n**1、状态保持、持久执行（Durable Execution）**\n\n跑一半断电了怎么办？重头再来？那用户不得疯了？\n\nLangGraph把每一步状态存下来，哪怕服务挂了，重启也能接着干，就像游戏存档一样靠谱。\n\n**2、人机协同、人机协作（Human-in-the-loop）**\n\nAI再聪明也有拿不准的时候。\n\n这时候可以让人类插一脚，看看状态、改改参数、点个确认，然后再继续。\n\n这种设计特别适合审批流、客服质检这类高风险场景。\n\n**3、持久化存储、全面记忆管理（Comprehensive Memory）**\n\n有的框架只记最近几句话，LangGraph不一样，它可以同时管：\n\n* 短期记忆：本次会话的状态\n* 长期记忆：跨会话的历史数据（结合向量库或数据库）\n\n这就让AI不仅能“接话”，还能“认人”，提供个性化体验。\n\n**4、调试能力（Debugging）**\n\n复杂的AI流程就像迷宫，光看日志根本找不到问题在哪。\n\nLangGraph配合LangSmith，能生成可视化轨迹图，每一步走到哪、状态变成啥样，清清楚楚。\n\n**5、工具集成 & 多智能体支持**\n\n想让AI查天气、订机票、写报告？没问题，接API就行。\n\n而且它天生支持多个AI协同工作，比如一个负责分析，一个负责决策，一个负责汇报。\n\n## 第二章. 使用一个LangGraph 构建基础多智能体 聊天机器人\n\n核心痛点：想搭聊天机器人，但不会把大模型塞进一个能扩展的流程里，代码乱、状态散，没法往复杂应用走。\n\n核心方案：用 **LangGraph** 搭个状态驱动的流水线，把大模型调用变成简单节点，实现“输入→处理→输出”的清晰控制流。\n\n## 2.1 环境准备\n\n做AI项目，第一步就是选个靠谱的大模型。\n\n别一上来就烧钱，咱们先试试免费的路子。\n\n百度千帆和硅基流动这两个平台，都有不错的国产模型可以白嫖。\n\n**百度千帆调用**\n\n这是目前demo里用的方式，靠 `QianfanChatEndpoint` 接入ERNIE-Speed-128K模型：\n\n```\n import os from langchain_community.chat_models import QianfanChatEndpoint llm = QianfanChatEndpoint( model="ERNIE-Speed-128K", streaming=True, # 启用流式输出 api_key=os.getenv(\'QIANFAN_AK\', \'\'), secret_key=os.getenv(\'QIANFAN_SK\', \'\') ) \n```\n\n注册个账号，拿AK/SK密钥就能跑起来，适合新手练手。\n\n**硅基流动调用方式**\n\n另一个选择是硅基流动，它支持GLM、Qwen这些热门开源模型：\n\n```\n from langchain_openai import ChatOpenAI llm = ChatOpenAI( model="THUDM/glm-4-9b-chat", streaming=False, api_key=os.getenv(\'SILICONFLOW_API_KEY\', \'\'), base_url=os.getenv(\'SILICONFLOW_BASE_URL\', \'\'), temperature=0.1, ) \n```\n\n换个base\\_url，就能当OpenAI用，兼容性好得很，迁移成本低。\n\n**Deepseek 调用方式**\n\nDeepseek 支持多款开源大模型（如 Deepseek-R1、Deepseek-Coder 等），且提供兼容 OpenAI API 格式的调用方式，迁移成本低，直接复用 `ChatOpenAI` 即可快速接入：\n\n```\n from langchain_openai import ChatOpenAI import os # Deepseek 调用（兼容 OpenAI 接口格式） llm = ChatOpenAI( model="deepseek-chat", # 可选模型：deepseek-chat（通用）、deepseek-coder-v2（编程）等 streaming=True, # 支持流式输出，按需开启 api_key=os.getenv(\'DEEPSEEK_API_KEY\', \'\'), # 从环境变量读取密钥（推荐） base_url="https://api.deepseek.com/v1", # Deepseek 官方 API 基础地址 temperature=0.7, # 随机性调节：0~1，值越低输出越确定 max_tokens=4096 # 单次生成最大令牌数（按需调整，不同模型上限不同） ) \n```\n\n**前置准备**：注册 Deepseek 账号（官网：[https://www.deepseek.com/），进入「API](https://www.deepseek.com/%EF%BC%89%EF%BC%8C%E8%BF%9B%E5%85%A5%E3%80%8CAPI) 密钥管理」获取 `DEEPSEEK_API_KEY`，建议将密钥存入环境变量（避免硬编码），如：\n\n```\n # Linux/Mac export DEEPSEEK_API_KEY="你的密钥" # Windows（命令行） set DEEPSEEK_API_KEY="你的密钥" \n```\n\n痛点：API 密钥写死在代码里？不安全！依赖安装慢还冲突？新手直接卡住。\n\n解决方案：用 `uv` 快速装包，`.env` 文件管密钥，环境干净又安全，一次配好反复用。\n\n```\n # 安装 LangGraph 和周边依赖 uv pip install -U langgraph langchain python-dotenv typing-extensions \n```\n\n```\n # .env 文件存密钥 DEEPSEEK_API_KEY=your_deepseek_api_key_here \n```\n\n自动加载配置，密钥不进代码，不怕泄露。\n\n## 2.2 实现基础聊天机器人\n\n痛点：直接调大模型？**没流程、没记忆，对话像复读机**，体验差。\n\n解决方案：用 LangGraph 维护消息列表作为状态，串起“用户输入 → 模型回复” 的复杂路线，让对话有上下文、能连贯。\n\nLangGraph的核心，就是把业务逻辑画成一张“流程图”。\n\n每个节点干一件事，边来决定执行顺序。\n\n就像工厂流水线，零件从这头进，那头出成品。\n\n```\n from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START from langgraph.graph.message import add_messages import os from dotenv import load_dotenv # 加载.env文件中的环境变量 load_dotenv() class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model( "deepseek-chat", # 使用DeepSeek模型 api_key=os.environ.get("DEEPSEEK_API_KEY") ) def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except KeyboardInterrupt: print("\\nGoodbye!") break \n```\n\n> 一句话概括：定义状态 → 创建图 → 添加节点 → 编译运行 → 流式输出。\n\n核心的代码如下\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() \n```\n\n## 2.3 基础聊天机器人 代码解析\n\n痛点：LangGraph 组件太多，看不懂谁干啥，学得迷糊。\n\n解决方案：盯死两个核心：**状态累积** 和 **流程编排**，其他都是配角。\n\n### 1. 引入依赖\n\n导包是第一步，为状态、模型、流程打基础。\n\n### 2. 加载环境变量\n\n```\n load_dotenv() \n```\n\n从 `.env` 读密钥，绝不硬编码。\n\n### 3. 定义状态 结构\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n所有消息存在一个可累加的 list 里，新消息自动追加，上下文不断。\n\n这行代码是 **LangGraph 中定义「可自动累加对话状态」的核心语法**。专门解决之前提到的「对话历史持久化」问题 。\n\n这行代码， 让 `messages` 字段能自动追加新消息（用户提问、AI 回复），而不是被覆盖。\n\n在 LangGraph 流程中，`State` 是流转的数据载体，而 `messages` 是存储「对话历史」的关键字段（比如 `[{"role": "user", "content": "你好"}, {"role": "assistant", "content": "你好！"}]`）。\n\n这行代码的核心目的是：**让每次节点执行后，新生成的消息（如 AI 回复）自动追加到 `messages` 列表中，而非替换原有历史**，确保流程能基于完整上下文推进。\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n**1、`class State(TypedDict)`：定义强类型的状态字典**\n\n* `TypedDict`（类型字典）：是 Python 标准库 `typing` 中的工具，用于定义「键名固定、值类型明确」的字典。作用：约束 `State` 的结构（必须有 `messages` 字段，且类型是 `list`），避免写代码时出现键名错误（如把 `messages` 写成 `message`），同时让 IDE 提供类型提示。\n* 对比普通字典：如果直接用 `dict` 定义状态，无法约束字段，而 `TypedDict` 让状态结构「可预期、可校验」，是 LangGraph 推荐的状态定义方式。\n\n  示例：符合 `State` 类型的合法数据\n\n  ```\n   valid_state = { "messages": [ {"role": "user", "content": "介绍 LangGraph"}, {"role": "assistant", "content": "LangGraph 是流程编排工具"} ] } \n  ```\n\n2、messages: Annotated[list, add\\_messages] ：给 messages 字段加「自动追加规则」 add\\_messages。\n\n这是关键中的关键，`Annotated` + `add_messages` 共同实现「对话历史自动累加」。\n\n（1）`Annotated[类型, 元数据]`：Annotated给字段附加额外规则，这是 Python 3.9+ 引入的标准库工具（从typing导入），作用是「给类型添加元数据 / 规则」。\n\n格式：\n\n```\n Annotated[基础类型, 规则1, 规则2, ...] \n```\n\n这里的「规则」会被 LangGraph 识别并生效。\n\n这里的 `Annotated[list, add_messages]` 表示：`messages` 字段的基础类型是 `list`（存储对话消息列表），同时附加 `add_messages` 这个「特殊规则」。\n\n（2）`add_messages`：LangGraph 定义的一个函数，是一个自定义「消息追加处理器」\n\n* `add_messages` 是从 `langgraph.graph` 导入的核心工具（需显式导入：`from langgraph.graph import add_messages`），核心代码参考下面。\n* add\\_messages 核心作用：自动合并「节点返回的新消息」和「原有状态中的旧消息」，具体逻辑是：\n  1. 假设原有状态的 `messages` 是：`[用户提问1]`\n  2. 节点函数（如之前的 `chatbot`）返回：`{"messages": [AI 回复1]}`\n  3. `add_messages` 会自动将两者合并为：`[用户提问1, AI 回复1]`，并更新到新状态中\n* 对比没有 `add_messages` 的情况：如果直接定义 `messages: list`，节点返回的新消息会「覆盖」原有 `messages`（原有对话历史丢失）。\n\nLangGraph 的 `add_messages` 源码（简化后）如下 ：\n\n```\n from typing import Any, List, Union def add_messages( existing: Union[List[Any], None], # 状态中已有的旧消息（可能为 None） updates: Union[List[Any], Any, None] # 节点返回的新消息（可能是列表/单条/None） ) -> List[Any]: """ 合并新旧消息：旧消息列表 + 新消息（自动处理单条/列表格式） - 若旧消息不存在，直接返回新消息列表 - 若新消息是单条，自动转为列表后追加 - 若新消息为 None，返回原旧消息列表 """ # 初始化旧消息列表（避免 None 报错） current = existing.copy() if existing is not None else [] # 处理新消息：统一转为列表格式 if updates is None: new_messages = [] elif isinstance(updates, list): new_messages = updates else: new_messages = [updates] # 单条消息 → 列表 # 合并：旧消息 + 新消息（保证顺序，不覆盖） return current + new_messages \n```\n\n如果没有 `add_messages` 规则：\n\n* 初始状态：`{"messages": [用户提问]}`\n* 节点执行后返回：`{"messages": [AI 回复]}`\n* 最终状态：`{"messages": [AI 回复]}`（用户提问被覆盖，丢失历史）\n\n有了 `add_messages` 规则：\n\n* 初始状态：`{"messages": [用户提问]}`\n* 节点返回：`{"messages": [AI 回复]}`\n* `add_messages` 自动合并：`[用户提问] + [AI 回复] = [用户提问, AI 回复]`\n* 最终状态：`{"messages": [用户提问, AI 回复]}`（历史保留，后续节点可复用上下文）\n\n这行代码的本质是：**用 `TypedDict` 约束状态结构，用 `Annotated + add_messages` 给 `messages` 字段绑定「自动追加」规则**。\n\n最终实现「对话历史不丢失、上下文可复用」—— 这正是 LangGraph 能支持多轮对话、复杂流程的基础， 这就是提到的「不怕断电重启丢数据」（状态可持久化，历史都在 `messages` 中）。\n\n### 状态结构的本质\n\n传统流程像函数链：前一步输出传给下一步，**每步只能看到局部数据**。\n\n结果？上下文断裂、逻辑难控、调试抓狂。\n\n#### 核心解法\n\nLangGraph 用一个**全局共享状态**，所有节点操作同一份数据。\n\n就像工厂流水线：半成品从一站传到下一站，每一站都加工它、更新它。\n\n```\n # 错误理解：以为是函数链 def workflow(): result1 = step1() result2 = step2(result1) return result3(result2) # 正确理解：是状态图 state = {"data": None} state = node1(state) # 每个节点都接收并返回完整状态 state = node2(state) state = node3(state) \n```\n\n> 状态 = 公共记事本，所有人一起看、一起改。\n\n> 不是每人一张草稿纸，最后对不上账。\n\n#### 关键点\n\n* **节点不是函数，是状态转换器**：读当前状态 → 加工 → 返回新状态。\n* **每一步都在提交快照**：像 Git commit，留下完整的系统瞬间。\n* **状态即上下文中枢**：所有节点共用一份数据源，不丢信息，不错乱序。\n\nLangGraph 的本质：**带记忆的自动化流水线**。 节点干活，状态流动，全程不断电。\n\n### 4. 创建图构建器\n\n```\n graph_builder = StateGraph(State) \n```\n\n初始化一个共享状态的流程图，所有节点都能看到最新消息。\n\n这行代码是 **LangGraph 流程图的 “地基构建”**： 创建一个「绑定了状态结构」的流程图构建器。\n\n核心作用是：明确流程图中流转的「数据格式（State）」，让后续节点、边的定义都遵循这个格式，避免数据混乱。\n\n先回顾代码上下文（结合之前的 `State` 定义）：\n\n```\n # 1. 先定义状态结构（约束数据格式） class State(TypedDict): messages: Annotated[list, add_messages] # 2. 创建流程图构建器（绑定状态） graph_builder = StateGraph(State) \n```\n\n逐部分解释：\n\n**（1）、`StateGraph`**：\n\n是 LangGraph 的核心类（从 `langgraph.graph` 导入），本质是「流程图构建工具」，负责管理「节点（处理步骤）」和「边（流转规则）」。\n\n可以把它理解为「一张空白的画布」，后续的 `add_node`（加节点）、`add_edge`（加流转）都是在这张画布上画画。\n\n**（2）、`State`**：\n\n是我们之前定义的「强类型状态结构」（基于 `TypedDict`），明确了流程图中流转的数据必须包含 `messages` 字段，且格式是「带 `add_messages` 规则的列表」。\n\n这里作为参数传入 `StateGraph`，表示「这张流程图的所有数据流转，都必须遵循 `State` 定义的格式」。\n\n**（3）、`graph_builder`**：\n\n是 `StateGraph` 类的实例（即 “构建好的空白画布”），后续所有流程图操作（加节点、加边、编译）都通过这个实例完成。\n\n### 5. 初始化大模型\n\n```\n llm = init_chat_model("deepseek-chat", api_key=os.environ.get("DEEPSEEK_API_KEY")) \n```\n\n接入 DeepSeek 当回答引擎，听你指挥。\n\n### 6. 编写节点逻辑\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} \n```\n\n输入当前消息，让模型生成回复，返回新消息对象。\n\n### 7. 搭建流程图\n\n```\n graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() \n```\n\n注册节点，连上起点，形成一条直线流程：问了就答。\n\n这些代码是 **LangGraph 构建对话流程图的核心逻辑**.\n\n核心作用是：定义一个「对话节点」，让流程从 “开始” 直接进入该节点，最终形成一个「 任务流程」（启动 → 对话 → 结束）。下面逐行拆解，结合 LangGraph 核心概念和实际作用讲清楚：\n\n先明确两个前提\n\n**(1) State（状态）：LangGraph 中流转的数据载体（类似 “流水线的物料”），这里的 `State` 是一个自定义数据结构（通常是字典或 Pydantic 模型），核心字段 `messages` 存储对话历史（比如用户提问、AI 回复）。**\n\n**(2) llm：之前定义的大语言模型实例（百度千帆 / 硅基流动 / Deepseek 等），负责接收对话历史并生成 AI 回复。**\n\n逐行代码解释\n\n#### (1). 定义对话节点函数 `chatbot`\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} \n```\n\n这是 LangGraph 的「节点函数」（节点是流程中的 “处理步骤”）.\n\n作用是：**接收当前对话状态，调用 LLM 生成回复，更新状态并返回**。\n\n函数参数 `state: State`：\n\n* 接收流程中传递过来的「当前状态」，里面包含 `state["messages"]`（对话历史列表，比如 `[{"role": "user", "content": "你好"}]`）。\n* 类型注解 `State` 是自定义的状态结构（确保代码规范，避免键名错误）。\n\n核心逻辑 `llm.invoke(state["messages"])`：\n\n* 调用 LLM 模型，传入完整的对话历史 `state["messages"]`（模型需要基于上下文生成回复，而不只是单轮提问）。\n* `invoke` 是同步调用方法（如果之前开启了 `streaming=True`，这里会返回流式迭代器，需配合处理）。\n* 模型返回结果是一个「AI 回复消息对象」（比如 `{"role": "assistant", "content": "你好！有什么可以帮你？"}`）。\n\n返回值 `{"messages": [llm.invoke(...)]}`：\n\n* LangGraph 要求节点函数返回「状态更新数据」（只需要返回要修改的字段，不需要全量状态）。\n* 这里表示：将 LLM 生成的 AI 回复，添加到 `messages` 字段中（覆盖旧的 `messages`？不 ——LangGraph 会自动合并：旧 `messages` + 新 AI 回复 = 新 `messages`，具体看状态定义是否允许列表追加，默认字典结构会覆盖，实际工程中会用 `list` 追加，比如 `{"messages": state["messages"] + [llm.invoke(...)]}`，这里 demo 简化了）。\n\n#### （2）. 向流程图添加「chatbot 节点」\n\n```\n graph_builder.add_node("chatbot", chatbot) \n```\n\n* `graph_builder` 是 `StateGraph` 的实例（流程图构建器），负责管理节点和节点间的流转。\n* add\\_node方法：注册一个节点到流程图中。\n  + 第一个参数 `"chatbot"`：节点的「唯一标识名」（后续用这个名字指定流转关系）。\n  + 第二个参数 `chatbot`：节点对应的「处理函数」（即上面定义的 `chatbot` 函数，节点被触发时会执行该函数）。\n\n#### （3）. 定义流程的「入口节点」和「流转关系」\n\n`START` 是 LangGraph 内置的「起始节点」（流程的入口，类似流程图的 “开始” 符号）。\n\n* `add_edge(START, "chatbot")`：定义一条流转规则：**流程从 START 开始后，直接进入名为 "chatbot" 的节点**。\n* 这是最简单的流转关系（无分支、无条件），是线性流程的核心。\n\n#### （4）. 编译流程图为可执行应用\n\n```\n graph = graph_builder.compile() \n```\n\n* `compile()` 是构建器的核心方法，作用是：将前面定义的「节点」和「流转规则」，编译成一个「可执行的流程图应用」（`graph` 是编译后的实例）。\n* 编译后，graph具备了「运行流程」的能力，后续可以通过graph.invoke(初始状态)启动流程，比如：\n\n  ```\n   # 启动流程：传入初始对话状态（用户的第一个提问） result = graph.invoke({"messages": [{"role": "user", "content": "介绍一下 LangGraph"}]}) # 输出最终状态中的 messages（包含用户提问 + AI 回复） print(result["messages"]) \n  ```\n\n整个流程的执行逻辑（一句话总结）\n\n**(1) 调用 `graph.invoke(初始状态)` 启动流程；**\n\n**(2) 流程从 `START` 节点出发，根据 `add_edge` 规则，进入 `chatbot` 节点；**\n\n**(3) 执行 `chatbot` 函数：读取初始状态中的用户提问 → 调用 LLM 生成回复 → 更新 `messages` 字段；**\n\n**(4) 由于没有定义 `chatbot` 节点之后的流转关系（比如 `add_edge("chatbot", END)`），demo 中流程会在 `chatbot` 节点执行完成后自动结束。实际工程中，会显式指定 `add_edge("chatbot", END)` 或其他节点 ；**\n\n**(5) 返回最终的状态（包含用户提问和 AI 回复的完整对话历史）。**\n\n核心特点\n\n* 「清晰执行路径」：START → chatbot → 结束，无分支、无逻辑混乱；\n* 「状态可追溯」：所有对话数据都存在 `state["messages"]` 中，即使中断 / 重启，只要恢复 `state` 就能继续流程（不会丢数据）；\n* 「可扩展」：后续可以添加更多节点（比如 “意图识别”“工具调用”“记忆管理”），通过 `add_edge` 定义复杂流转（比如条件分支、循环），而不需要重构核心逻辑。\n\n### 8. 流式输出响应\n\n```\n def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) \n```\n\n回复边生成边打印，像真人打字，体验丝滑。\n\n这行代码是 **LangGraph 流式输出对话结果的核心函数**，核心作用是：接收用户输入，启动流程图的「流式执行」，实时捕获 AI 回复并逐段打印（而非等待完整回复生成后再输出）。\n\n先明确两个前提\n\n**(1) 流式输出（streaming）：LLM 生成回复时，不是一次性返回完整内容，而是「逐句 / 逐段推送」（类似 ChatGPT 的打字机效果），减少用户等待感。**\n\n**(2) graph.stream()：LangGraph 编译后的 `graph` 实例的流式执行方法，与 `graph.invoke()`（同步阻塞，等待完整结果）相对，返回的是「事件迭代器」，实时输出流程执行中的状态更新。**\n\n非流式版本的等效代码（对比参考）：\n\n```\n def non_stream_graph_updates(user_input: str): # 同步阻塞，等待完整结果 result = graph.invoke({"messages": [{"role": "user", "content": user_input}]}) # 打印完整回复 print("Assistant:", result["messages"][-1].content) \n```\n\n再回到代码\n\n```\n def stream_graph_updates(user_input: str): # 1. 启动流程图的流式执行，传入初始状态 for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): # 2. 遍历事件中的所有节点输出（当前流程只有 chatbot 节点，所以只有一个 value） for value in event.values(): # 3. 提取并打印 AI 最新回复 print("Assistant:", value["messages"][-1].content) \n```\n\n**（1） 函数定义：`def stream_graph_updates(user_input: str):`**\n\n函数名：`stream_graph_updates` → 语义明确：流式获取流程图的更新（即 AI 回复）。\n\n参数 `user_input: str`：接收用户的提问（字符串类型），比如用户输入 `"介绍一下 LangGraph"`。\n\n**（2\\_) 核心：`graph.stream(初始状态)` → 启动流式执行**\n\n```\n for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): \n```\n\n`graph.stream(...)`：\n\n* 作用：启动流程图的「流式执行」，返回一个「事件迭代器」（`event` 是迭代器的每一个元素）。\n* 传入的初始状态：`{"messages": [{"role": "user", "content": user_input}]}` → 把用户输入包装成 LangChain/LLM 通用的「消息格式」（`role` 表示角色，`content` 表示内容），作为流程的起始数据。\n* 流式特性：执行后不会阻塞等待完整结果，而是每生成一段 AI 回复，就产生一个 `event`，迭代器会实时返回该事件。\n\n`for event in ...`：循环迭代流式事件.\n\n每一个 `event` 对应「流程图中某个节点的一次状态更新」（当前流程只有 `chatbot` 节点，所以所有 `event` 都来自 `chatbot` 节点的流式输出）。\n\n**(3) 解析事件：`for value in event.values()`**\n\nfor value in event.values():event的结构：LangGraph 的流式事件是一个「字典」，键是「节点名称」，值是「该节点的最新状态」。\n\n* 比如当前流程中，`event` 的结构是：`{"chatbot": {"messages": [用户输入, AI 回复片段1]}}`（后续事件会是 `{"chatbot": {"messages": [用户输入, AI 回复片段1, 回复片段2]}}` 等）。\n* `event.values()`：获取所有节点的最新状态（当前只有 `chatbot` 节点，所以 `values()` 只有一个元素）。\n* 循环的目的：兼容多节点流程（比如后续加了「工具调用节点」，`event` 可能包含多个节点的状态，这里统一遍历解析）。\n\n**(4) 提取并打印最新回复：`value["messages"][-1].content`**\n\n```\n print("Assistant:", value["messages"][-1].content) \n```\n\n* `value`：即 `chatbot` 节点的最新状态（符合之前定义的 `State` 格式，包含 `messages` 字段）。\n* `value["messages"]`：当前完整的对话历史（用户输入 + 已生成的 AI 回复片段）。\n* `[-1]`：取列表的「最后一个元素」→ 也就是 LLM 刚刚流式推送的「最新回复片段」（因为 `add_messages` 规则，`messages` 是不断追加的，最后一个元素永远是最新的）。\n* `.content`：提取消息的内容（消息对象的结构是 `{"role": "assistant", "content": "回复内容"}`，`.content` 直接获取文本）。\n* 最终效果：每收到一个 AI 回复片段，就打印一次，呈现「打字机式」的实时输出。\n\n**关键细节：为什么能实现 “实时输出”？**\n\n**(1) 依赖 LLM 的 `streaming=True`：之前定义 LLM 时开启了 `streaming=True`（比如 Deepseek / 百度千帆的调用代码），LLM 才会支持流式推送回复片段。**\n\n**(2) 依赖 `graph.stream()`：LangGraph 会把 LLM 的流式输出「封装成事件迭代器」，确保每一个回复片段都能被实时捕获。**\n\n**(3) 依赖 `messages` 的追加规则：`add_messages` 确保每一个新的回复片段都追加到 `messages` 列表末尾，通过 `[-1]` 能精准取到最新片段。**\n\n对比：流式输出 vs 非流式输出\n\n为了更直观，对比 `graph.stream()`（流式）和 `graph.invoke()`（非流式）的差异：\n\n| 方式 | 核心函数 | 执行效果 | 适用场景 |\n| --- | --- | --- | --- |\n| 流式输出 | `graph.stream()` | 逐段实时打印，类似打字机 | 交互类场景（如聊天机器人），提升用户体验 |\n| 非流式输出 | `graph.invoke()` | 等待完整回复生成后一次性打印 | 批量处理（如批量生成报告），无需实时交互 |\n\n### 9. 主循环交互\n\n```\n while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except KeyboardInterrupt: print("\\nGoodbye!") break \n```\n\n> 命令行入口，支持持续对话，按 q 或 Ctrl+C 优雅退出。\n\n这行代码是 **对话机器人的「交互循环入口」**，核心作用是：创建一个「持续运行的命令行交互环境」，让用户能反复输入提问、接收 AI 流式回复，同时支持正常退出和异常中断，是连接用户输入与 LangGraph 流程的 “桥梁”。下面逐行拆解逻辑、循环原理和边界处理：\n\n这行代码的本质是：**搭建一个「持续运行、支持多轮交互、优雅退出」的命令行对话入口**，将用户的手动输入转化为 LangGraph 流程的触发信号，最终实现 “用户提问 → AI 实时回复” 的完整交互闭环，是对话机器人从 “代码片段” 变成 “可使用工具” 的关键一步。\n\n## 2.4 运行聊天机器人\n\n痛点：跑完不知道成没成功？一脸懵。\n\n解决方案：运行脚本，输入问题，看到流式回复——就成了！\n\n```\n uv run 1-build-basic-chatbot.py \n```\n\n示例输出：\n\n```\n User: who are you? Assistant: I’m DeepSeek Chat, your AI assistant created by **DeepSeek**! \n```\n\n大模型已通，基本对话能力到手。\n\n### 小结\n\n痛点：教程太碎，学完还是不会搭真正的 AI 应用。\n\n解决方案：记住三个词：**状态、节点、边** —— 这就是 LangGraph 的骨架。\n\n这个机器人虽小，五脏俱全：\n\n* **状态**：存上下文（比如聊天记录）\n* **节点**：干活的（比如调模型）\n* **边**：定流程（下一步去哪）\n\n往后要加记忆、分支、工具调用？直接往上堆就行。\n\n## 第三章. 使用 Langgraph 完成工具调用\n\n我们之前用 LangGraph 做了个聊天机器人，但它只能“空想”，没法查天气、搜资料、算数学。\n\n现在要让它**能思考，也能动手**——比如你问“明天天气咋样？”，它会主动去“查一下”再告诉你。\n\n想让大 模型 查新鲜事、找实时数据？得给它接个“外挂大脑”。这外挂，就是工具（Tools）。 接上了，大模型就能上网搜、调接口、拿最新信息，不再靠死记硬背过日子。\n\n这就叫 **工具调用（Tool Calling）**。\n\n先给出完整可运行代码\n\n```\n # 1. 基础依赖导入 import asyncio from typing import Literal, List from langchain_core.tools import tool from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage from langchain_core.utils.function_calling import convert_to_openai_function from langgraph.graph import StateGraph, END, MessagesState # MessagesState是LangGraph内置的消息状态类 from langgraph.prebuilt import ToolNode # 导入LLM（以Deepseek为例，可替换为百度千帆/硅基流动等，用法一致） from langchain_openai import ChatOpenAI import os # 2. 初始化LLM（需提前配置环境变量 DEEPSEEK_API_KEY） llm = ChatOpenAI( model="deepseek-chat", api_key=os.getenv("DEEPSEEK_API_KEY", ""), base_url="https://api.deepseek.com/v1", streaming=True, # 支持流式输出 temperature=0.3 # 降低随机性，让工具调用更稳定 ) # 3. 定义工具：用@tool装饰器将普通函数转为AI可识别的工具 @tool # 核心装饰器：自动生成工具描述，供LLM判断是否调用 def get_weather(query: str) -> List[str]: """ 用于获取指定地区、指定时间的天气信息（工具描述很重要！LLM靠这个判断是否调用） :param query: 查询条件，格式示例："北京 今明两天天气"、"上海 明天是否下雨" :return: 天气信息列表，包含每天的天气描述 """ # 这里是模拟工具返回（真实场景替换为调用天气API，如高德/百度天气接口） if "今明两天" in query or "今天" in query or "明天" in query: return ["今天天气晴朗，温度20~28℃，微风", "明天天气多云转晴，温度22~30℃，南风3级"] elif "后天" in query: return ["后天天气小雨，温度18~25℃，东北风2级"] else: return [f"已查询到：{query} 的天气为晴朗，温度20~28℃（模拟数据）"] # 工具列表：可添加多个工具（如搜索、计算等） tools = [get_weather] # 4. 绑定工具到LLM：告诉模型"你现在拥有这些工具" # bind_tools：LangChain的工具绑定方法，让LLM能识别工具并生成工具调用指令 llm_with_tools = llm.bind_tools(tools) # convert_to_openai_function：将工具转为OpenAI格式的函数描述（兼容多数LLM） functions = [convert_to_openai_function(tool) for tool in tools] # 5. 定义对话节点（chat_bot）：让模型判断"要不要调工具" async def chat_bot(state: MessagesState): """ 核心思考节点：接收对话状态，让LLM判断是否调用工具，或直接生成回复 :param state: 流程状态，包含messages（对话历史） :return: 更新后的状态（添加LLM的思考结果） """ # 从状态中获取完整对话历史 messages = state["messages"] # 调用绑定工具的LLM，关键参数说明： # ainvoke：异步调用（适配流式和异步流程） # functions：工具的OpenAI格式描述，供LLM参考 # function_call="auto"：让LLM自主决定：直接回复 或 调用工具 response = await llm_with_tools.ainvoke( messages, functions=functions, function_call="auto" ) # 返回更新后的状态：将LLM的响应（思考结果）添加到对话历史 return {"messages": [response]} # 6. 定义路由节点（tool_router）：判断"下一步去哪"（分流逻辑） def tool_router(state: MessagesState) -> Literal["tools", "__end__"]: """ 流程路由（类似交通交警）：根据LLM的输出，决定下一个节点 :param state: 流程状态 :return: 下一个节点名称（"tools" 或 END） """ # 获取最新一条消息（LLM的思考结果） messages = state["messages"] last_message = messages[-1] # 关键判断：如果最新消息包含tool_calls（工具调用指令），则跳转到工具节点 if last_message.tool_calls: return "tools" # 去工具节点执行工具调用 return END # 没有工具调用，直接结束流程 # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) # 8. 编排完整流程（状态图）：将节点和路由串成流水线 # StateGraph(MessagesState)：创建状态图，指定状态格式为MessagesState（内置的消息列表结构） workflow = StateGraph(MessagesState) # 8.1 添加节点 workflow.add_node("chat_bot", chat_bot) # 思考节点：判断是否调工具 workflow.add_node("tools", tool_node) # 工具节点：执行工具调用 # 8.2 设置流程入口：流程从chat_bot节点开始（用户提问后先让模型思考） workflow.set_entry_point("chat_bot") # 8.3 定义节点流转规则 # 工具节点执行完成后，返回chat_bot节点：让模型基于工具结果生成最终回复 workflow.add_edge("tools", "chat_bot") # 条件流转：从chat_bot节点出发，由tool_router决定下一步 workflow.add_conditional_edges( source="chat_bot", # 起点：chat_bot节点 condition=tool_router, # 条件判断函数：tool_router # 无需手动映射目标节点（tool_router直接返回节点名） ) # 8.4 编译流程：生成可执行的流程图应用 app_graph = workflow.compile() # 9. 测试：流式运行流程（实时查看每一步输出） async def run_streaming_demo(): """流式运行工具调用流程，模拟用户交互""" # 初始对话状态：系统提示 + 用户提问 initial_messages = [ SystemMessage(content="你是一个智能助手，能回答问题和调用工具。" "如果需要查询天气，请调用get_weather工具，不要瞎编数据。"), HumanMessage(content="帮我查一下深圳今明两天的天气，谢谢～") ] initial_state = {"messages": initial_messages} print("=== 工具调用流程启动（流式输出）===") print(f"User: {initial_messages[-1].content}") print("Assistant: ", end="", flush=True) # 异步流式迭代流程输出（stream_mode=\'messages\'：按消息粒度输出，便于调试） async for event in app_graph.astream(initial_state, stream_mode=\'messages\'): # 解析事件：LangGraph的流式事件是元组，第一个元素是消息块 if isinstance(event, tuple): chunk = event[0] # 只打印AI的回复内容（过滤工具调用、系统消息等中间过程） if isinstance(chunk, HumanMessage): continue # 跳过用户消息 elif isinstance(chunk, ToolMessage): # 工具调用结果（用户不可见，可选打印用于调试） print(f"\\n[调试] 工具返回结果：{chunk.content}", flush=True) elif chunk.type == \'AIMessageChunk\': # 流式输出AI的最终回复（打字机效果） print(chunk.content, end="", flush=True) print("\\n=== 流程结束 ===") # 10. 运行测试（异步函数需用asyncio.run启动） if __name__ == "__main__": asyncio.run(run_streaming_demo()) \n```\n\n接下来，开始代码核心组件详解（按执行顺序）\n\n## 3.1、先搞个“假”天气工具：让AI知道它能干啥\n\n**痛点：**模型不知道自己有啥技能，就像助理没说明书，啥也不会干。\n\n**方案：**用 `@tool` 给函数打标签，变成 AI 能识别的“工具”。\n\n```\n # 3. 定义工具：用@tool装饰器将普通函数转为AI可识别的工具 @tool # 核心装饰器：自动生成工具描述，供LLM判断是否调用 def get_weather(query: str) -> List[str]: """ 用于获取指定地区、指定时间的天气信息（工具描述很重要！LLM靠这个判断是否调用） :param query: 查询条件，格式示例："北京 今明两天天气"、"上海 明天是否下雨" :return: 天气信息列表，包含每天的天气描述 """ # 这里是模拟工具返回（真实场景替换为调用天气API，如高德/百度天气接口） if "今明两天" in query or "今天" in query or "明天" in query: return ["今天天气晴朗，温度20~28℃，微风", "明天天气多云转晴，温度22~30℃，南风3级"] elif "后天" in query: return ["后天天气小雨，温度18~25℃，东北风2级"] else: return [f"已查询到：{query} 的天气为晴朗，温度20~28℃（模拟数据）"] # 工具列表：可添加多个工具（如搜索、计算等） tools = [get_weather] # 4. 绑定工具到LLM：告诉模型"你现在拥有这些工具" # bind_tools：LangChain的工具绑定方法，让LLM能识别工具并生成工具调用指令 llm_with_tools = llm.bind_tools(tools) # convert_to_openai_function：将工具转为OpenAI格式的函数描述（兼容多数LLM） functions = [convert_to_openai_function(tool) for tool in tools] # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) \n```\n\n* `@tool`：把普通函数包装成 AI 工具\n* `bind_tools`：告诉模型“你现在会这些技能”\n* `ToolNode`：负责真正执行工具调用，相当于给机器人配了“工具箱 + 操作员”\n\n**LLM 工具绑定（`bind_tools`）**\n\n* **`llm.bind_tools(tools)`**：给 LLM “挂载” 工具，让模型知道自己拥有这些能力，能生成符合工具调用格式的指令（如`tool_calls: [{"name": "get_weather", "args": {"query": "深圳今明两天天气"}}]`）\n* **`convert_to_openai_function`**：将工具转为 OpenAI 标准的函数描述格式，兼容绝大多数支持工具调用的 LLM（避免因模型格式差异导致调用失败）\n\n**创建 工具执行节点 ToolNode**\n\nToolNode 角色 是 流程的 “手脚”，负责执行工具调用的全流程（无需手动写解析逻辑）\n\nToolNode 内置逻辑：\n\n**(1) 解析 LLM 的`tool_calls`指令（提取工具名、参数）**\n\n**(2) 调用对应的工具函数（如`get_weather("深圳今明两天天气")`）**\n\n**(3) 将工具返回结果包装为`ToolMessage`（LangChain 的工具消息类型）**\n\n**(4) 自动将`ToolMessage`添加到对话历史，供后续节点使用**\n\n**ToolNode 优势**：无需关心工具调用的细节（如参数解析、异常处理），LangGraph 已封装好\n\n## 3.2、改造 chat\\_bot：让模型学会“要不要动手”\n\n**痛点：**模型只会硬答，不会判断“这事我能不能办”，要么瞎编，要么说“我不知道”。\n\n**方案：**让它输出“我要调哪个工具”，而不是直接回复。\n\n```\n # 5. 定义对话节点（chat_bot）：让模型判断"要不要调工具" async def chat_bot(state: MessagesState): """ 核心思考节点：接收对话状态，让LLM判断是否调用工具，或直接生成回复 :param state: 流程状态，包含messages（对话历史） :return: 更新后的状态（添加LLM的思考结果） """ # 从状态中获取完整对话历史 messages = state["messages"] # 调用绑定工具的LLM，关键参数说明： # ainvoke：异步调用（适配流式和异步流程） # functions：工具的OpenAI格式描述，供LLM参考 # function_call="auto"：让LLM自主决定：直接回复 或 调用工具 response = await llm_with_tools.ainvoke( messages, functions=functions, function_call="auto" ) # 返回更新后的状态：将LLM的响应（思考结果）添加到对话历史 return {"messages": [response]} \n```\n\n关键是 `function_call="auto"`：让模型自己决定是否调工具\n\n它不再嘴硬，而是说：“我要调 `get_weather`，参数是‘今明两天天气’”\n\n## 3.3、加个 add\\_conditional\\_edges 条件边，实现节点路由\n\n**痛点：**模型有时直接回，有时要调工具，系统懵了：接下来该干啥？\n\n**方案：**加个判断器，看消息里有没有 `tool_calls`，有就调工具，没有就结束。\n\n```\n # 6. 定义路由节点（tool_router）：判断"下一步去哪"（分流逻辑） def tool_router(state: MessagesState) -> Literal["tools", "__end__"]: """ 流程路由（类似交通交警）：根据LLM的输出，决定下一个节点 :param state: 流程状态 :return: 下一个节点名称（"tools" 或 END） """ # 获取最新一条消息（LLM的思考结果） messages = state["messages"] last_message = messages[-1] # 关键判断：如果最新消息包含tool_calls（工具调用指令），则跳转到工具节点 if last_message.tool_calls: return "tools" # 去工具节点 tool_node 执行工具调用 return END # 没有工具调用，直接结束流程 # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) # 8. 编排完整流程（状态图）：将节点和路由串成流水线 # StateGraph(MessagesState)：创建状态图，指定状态格式为MessagesState（内置的消息列表结构） workflow = StateGraph(MessagesState) # 8.1 添加节点 workflow.add_node("chat_bot", chat_bot) # 思考节点：判断是否调工具 workflow.add_node("tools", tool_node) # 工具节点：执行工具调用 # 条件流转：从chat_bot节点出发，由tool_router决定下一步 workflow.add_conditional_edges( source="chat_bot", # 起点：chat_bot节点 condition=tool_router, # 条件判断函数：tool_router # 无需手动映射目标节点（tool_router直接返回节点名） ) \n```\n\n核心节点：`chat_bot`（思考节点）\n\n核心节点：`tool_node`（工具节点）\n\n路由条件：`tool_router`（分流逻辑）\n\n**路由条件角色**：流程的 “交通警察”，解决 “下一步去哪” 的问题\n\n**路由条件角色**：判断逻辑， 检查 LLM 的最新输出是否包含 tool\\_calls（工具调用指令）：\n\n* 有 → 跳转到`tools`节点执行工具调用\n* 无 → 直接结束流程（返回最终回复）\n\n**返回值**：严格指定为`Literal["tools", "__end__"]`，确保类型安全，避免流转错误\n\n流程编排（`StateGraph`）核心逻辑：构建 “思考→判断→执行→再思考” 的闭环：\n\n```\n 入口 → chat_bot（思考）→ tool_router（判断）→ ① 无工具调用 → 结束 ② 有工具调用 → tools（执行）→ chat_bot（基于工具结果生成回复）→ 结束 \n```\n\n回顾langgraph 关键 API：\n\n* `add_node`：添加节点（思考节点、工具节点）\n* `set_entry_point`：设置流程入口（从`chat_bot`开始）\n* `add_edge`：固定流转（工具执行后返回`chat_bot`）\n* `add_conditional_edges`：条件流转（由`tool_router`决定下一步）\n\n## 3.4、工具到底怎么被调的？拆开看流程\n\n**痛点：**中间环节太多，容易断链，结果丢了都不知道。\n\n**方案：**`ToolNode` 一把梭， 解析 → 执行 → 返回结果，全自动接回对话。\n\n当模型输出：\n\n```\n tool_calls=[{\'name\': \'get_weather\', \'args\': {\'query\': \'今明两天天气\'}}] \n```\n\n→ 路由发现 `tool_calls` → 跳转到 `tool_node`\n\n→ 自动调本地方法 `get_weather()`\n\n→ 结果作为 `ToolMessage` 写入历史\n\n→ 再回到 `chat_bot`，模型结合真实数据生成最终回复\n\n## 3.5 、串成完整工作流：大脑 + 手脚 合体\n\n**痛点：**逻辑散，不成环，难维护。\n\n**方案：**用状态图把“思考→判断→执行→再思考”串成自动流水线。\n\n```\n workflow = StateGraph(MessagesState) workflow.add_node("chat_bot", chat_bot) workflow.set_entry_point("chat_bot") workflow.add_node("tools", tool_node) workflow.add_edge("tools", "chat_bot") workflow.add_conditional_edges( "chat_bot", tool_router, ) app_graph = workflow.compile() \n```\n\n> 两条路径自动切换：\n\n* 直接回答 → 结束\n* 需查数据 → 调工具 → 回模型 → 出答案\n\n相当于给AI装上了“手脚”，从嘴炮王变实干家\n\n## 3.6 、测试运行：看看它是怎么一步步干活的\n\n**痛点：**看不到过程，出问题没法 debug。\n\n**方案：**开流式输出，实时监听每一步。\n\n```\n async def run_streaming_chain(): messages = [ SystemMessage(content="你是一个智能助手..."), HumanMessage(content="帮我查一下今明两天的天气") ] initial_state = {"messages": messages} async for event in app_graph.astream(initial_state, stream_mode=\'messages\'): if isinstance(event, tuple): chunk = event[0] if chunk.type == \'AIMessageChunk\': print(\'event里监听到的流式输出------>\', chunk.content) \n```\n\n`stream_mode=\'messages\'`：逐字打印输出，调试神器\n\n## 3.7、自验证：看看结果长啥样\n\n**痛点：**用户以为模型“天生就知道”，根本看不出用了工具。\n\n**方案：**通过日志看清全过程：指令 → 执行 → 回复。\n\n用户输入：\n\n> 帮我查一下今明两天的天气\n\n模型第一步不是回答，而是发指令：\n\n```\n last_message.tool_calls ------> [{\'name\': \'get_weather\', \'args\': {\'query\': \'今明两天天气\'}, ...}] \n```\n\n→ 触发工具 → 获取真实数据 → 模型整合输出：\n\n> “好的，我来帮您查询一下。经过查询，今天天气晴朗，温度20度，明天天气多云，温度25度……”\n\n> 用户无感，但背后已完成一次“AI + 工具”的协同作业\n\n### 核心流程图\n\n> 两条路自动走：能干就干，不能就调工具，全程无人干预\n\n### 总结：工具调用核心五步\n\n| 步骤 | 组件 | 干啥用 |\n| --- | --- | --- |\n| 1. 定义工具 | `@tool` | 把函数变成 AI 能调的“能力” |\n| 2. 绑定工具 | `bind_tools()` | 告诉模型：“你现在有这技能” |\n| 3. 判断是否调 | `tool_router` | 看有没有 `tool_calls`，决定走哪条路 |\n| 4. 执行工具 | `ToolNode` | 自动跑函数，拿结果回来 |\n| 5. 整合回复 | 回到 `chat_bot` | 模型基于真实数据生成人话 |\n\n> 从此，机器人从“只会说”升级为“又能说又能做”\n\n可接入数据库、搜索、计算器……想连啥连啥。\n\n## 第四章： 为langgraph应用 添加记忆功能\n\n在写聊天机器人时，没有记忆就像金鱼，游两下就忘了自己是谁。\n\n咱们上一节给机器人装了工具，能查资料、能干活，但它还是记不住聊过啥。\n\n这不行啊，用户说一遍名字，下次还得重新介绍，体验直接掉地上。\n\n所以这一节，咱给它加上“脑子”，让它记住对话历史，真正实现多轮连贯交流。\n\n### 4.1. 添加记忆功能\n\n加记忆功能，不需要额外装包，LangGraph 自带这个本事。\n\n它用的是 `langgraph.checkpoint.memory` 模块里的 `MemorySaver`，说白了就是个内存快照工具。\n\n每次对话一结束，它就把当前状态拍个照存起来，下次接着用。\n\n开发阶段用它正合适，简单又省事，就跟手机临时存图一样。\n\n#### 解决方案：\n\n用 LangGraph 的 `MemorySaver` 当临时笔记本，自动存对话记录。 靠 `thread_id` 区分不同用户，互不串台。\n\nLangGraph 自带 `MemorySaver` 它就像一个会自动记笔记的助理：每次聊完记一笔，下次见你先翻本子再开口。\n\n#### 怎么加记忆？三步搞定：\n\n**（1）定义状态：消息列表支持累积**\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n> `add_messages` 是关键：新消息不是覆盖，而是追加到历史里，上下文不断档。\n\n**（2）创建 MemorySaver，当内存记事本**\n\n```\n memory = MemorySaver() \n```\n\n**（3）编译图时传 checkpointer，开启自动保存**\n\n```\n graph = graph_builder.compile(checkpointer=memory) \n```\n\n每次对话结束，系统自动拍个“状态快照”存进去，下次按 `thread_id` 找回来继续聊。\n\n#### 多人聊天不串台？靠 thread\\_id\n\n每个用户分配唯一 `thread_id`，相当于独立聊天室：\n\n```\n config = {"configurable": {"thread_id": "1"}} \n```\n\n* 用户A用 `"1"`，B用 `"2"` → 各聊各的，不干扰\n* 回到 `"1"` → 记忆还在，接着上次聊\n\n注意：开发阶段 用 `MemorySaver`（内存存储），重启就丢。生产建议换 `SqliteSaver` 或数据库持久化。：\n\n### 4.2 使用记忆增强聊天机器人\n\n新建一个文件叫 `3-add-memory.py`，开始搞有记忆的机器人。\n\n```\n """LangGraph 教程: 添加记忆功能的聊天机器人 本示例展示了如何使用 LangGraph 的检查点功能为聊天机器人添加记忆功能， 使其能够记住对话历史并在多轮对话中保持上下文。 """ from typing import Annotated from langchain.chat_models import init_chat_model from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict # 导入 MemorySaver 用于实现记忆功能 from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition import os from dotenv import load_dotenv # 加载.env文件中的环境变量 load_dotenv() # 定义状态类型，使用 add_messages 注解来自动合并消息列表 class State(TypedDict): messages: Annotated[list, add_messages] # 消息列表将使用 add_messages reducer 自动合并 # 初始化 DeepSeek 聊天模型 llm = init_chat_model( "deepseek-chat", # 使用DeepSeek模型 api_key=os.environ.get("DEEPSEEK_API_KEY") # 从环境变量中获取API密钥 ) # 创建状态图构建器 graph_builder = StateGraph(State) # 初始化Tavily搜索工具 print("\\n初始化Tavily搜索工具...") tool = TavilySearch(max_results=2) # 设置最多返回2个搜索结果 tools = [tool] # 将工具绑定到LLM llm_with_tools = llm.bind_tools(tools) # 定义聊天机器人节点函数 def chatbot(state: State): """LLM节点函数，处理用户输入并生成响应""" return {"messages": [llm_with_tools.invoke(state["messages"])]} # 添加聊天机器人节点 graph_builder.add_node("chatbot", chatbot) # 添加工具节点 tool_node = ToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) # 添加条件边 graph_builder.add_conditional_edges( "chatbot", tools_condition, ) # 工具调用完成后，返回到聊天机器人节点 graph_builder.add_edge("tools", "chatbot") graph_builder.set_entry_point("chatbot") print("\\n构建图并添加记忆功能...") # 创建内存保存器 print("\\n创建 MemorySaver 实例作为检查点保存器...") memory = MemorySaver() # 在内存中保存状态，适用于开发和测试 # 使用内存保存器编译图 print("使用检查点保存器编译图...") graph = graph_builder.compile(checkpointer=memory) # 将内存保存器传递给图 # 打印图结构 print("\\n图结构如下：") print(graph.get_graph().draw_mermaid()) # 定义对话线程ID print("\\n设置对话线程 ID = \'1\'...") config = {"configurable": {"thread_id": "1"}} # 使用线程ID来标识和区分不同的对话 # 示例 1: 第一次对话 print("\\n示例 1: 第一次对话 - 用户介绍自己") user_input = "Hi there! My name is Will." print(f"\\n用户输入: \'{user_input}\'") # 注意: config 是 stream() 函数的第二个参数! print("使用线程 ID \'1\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 传递包含 thread_id 的配置 stream_mode="values", ) print("\\n助理回应:") for event in events: event["messages"][-1].pretty_print() # 打印助理的回应 # 示例 2: 测试记忆功能 print("\\n\\n示例 2: 第二次对话 - 测试记忆功能") user_input = "Remember my name?" print(f"\\n用户输入: \'{user_input}\'") # 使用相同的线程ID再次调用图 print("使用相同的线程 ID \'1\' 再次调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 使用相同的配置，图将加载之前保存的状态 stream_mode="values", ) print("\\n助理回应 (应该记得用户名字):") for event in events: event["messages"][-1].pretty_print() # 示例 3: 新对话线程 print("\\n\\n示例 3: 新对话线程 - 测试线程隔离") print("创建新的线程 ID = \'2\'...") # 使用不同的线程ID print("使用新的线程 ID \'2\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, {"configurable": {"thread_id": "2"}}, # 使用新的线程ID stream_mode="values", ) print("\\n助理回应 (不应该记得用户名字):") for event in events: event["messages"][-1].pretty_print() # 示例 4: 返回第一个线程 print("\\n\\n示例 4: 返回第一个线程 - 验证记忆持久性") print(f"\\n用户输入: \'{user_input}\'") # 再次使用第一个线程ID print("再次使用线程 ID \'1\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 使用原始线程ID stream_mode="values", ) print("\\n助理回应 (应该仍然记得用户名字):") for event in events: event["messages"][-1].pretty_print() \n```\n\n### 4.3 代码解析\n\n#### 关键改动点\n\n```\n from langgraph.checkpoint.memory import MemorySaver \n```\n\n这是新加的核心依赖，相当于给机器人配了个“小本本”。\n\n以前每次对话都像第一次见面，现在它会翻本本看看你们之前聊了啥。\n\n```\n memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) \n```\n\n这两行是关键操作：\n\n第一行，创建一个内存记录员；\n\n第二行，把这个记录员塞进图里，让它自动拍照存档每一步状态。\n\n只要你不关服务，它就能一直记住。\n\n> 生产环境别用这个，得换成数据库版的 `SqliteSaver` 或 `PostgresSaver`，不然重启就全丢了。\n\n```\n config = {"configurable": {"thread_id": "1"}} \n```\n\n这个 `thread_id` 就像是对话身份证。 同一个 ID，读的是同一份记忆；换个 ID，就是全新对话。\n\n多用户场景下，靠它实现隔离，不会张冠李戴。\n\n```\n events = graph.stream(..., config, stream_mode="values") \n```\n\n注意啊，`config` 是第二个参数，位置不能错。\n\n传进去之后，图就知道该去哪找之前的快照，而不是从头开始。\n\n### 4.4 运行聊天机器人\n\n运行命令：\n\n```\n uv run 3-add-memory.py \n```\n\n你会看到输出长这样：\n\n```\n 设置对话线程 ID = \'1\'... 示例 1: 第一次对话 - 用户介绍自己 用户输入: \'Hi there! My name is Will.\' ... Assistant: Hi Will! It\'s great to meet you. How can I assist you today? 示例 2: 第二次对话 - 测试记忆功能 用户输入: \'Remember my name?\' ... Assistant: Of course, Will! I’ll remember your name for the rest of our conversation. 示例 3: 新对话线程 - 测试线程隔离 用户输入: \'Remember my name?\' ... Assistant: I don’t have the ability to remember personal details like names between interactions. 示例 4: 返回第一个线程 - 验证记忆持久性 用户输入: \'Remember my name?\' ... Assistant: Got it, Will! I’ll keep your name in mind while we chat. \n```\n\n你看：\n\n* 同一个线程 ID，第二次还能叫出“Will”；\n* 换个 ID，立马变脸不认人；\n* 回到原来的 ID，记忆还在，没丢。\n\n这就是线程级记忆的威力。\n\n### 4.5 LangGraph 中 thread\\_id（线程 ID）\n\n在 LangGraph 中，**thread\\_id（线程 ID）的隔离能力核心依赖「检查点（Checkpointer）」的状态分区机制**——\n\n简单说：`thread_id` 是状态的「唯一索引」，不同 `thread_id` 对应独立的对话状态存储，彼此互不干扰。\n\n简单记：**LangGraph 的 thread\\_id 管 “对话记忆”，理论上叫做memory\\_id，和 Python “并发干活”的线程 ID ，八竿子打不着**。\n\n#### （1）先明确memory\\_id核心前提\n\n要理解隔离，先搞懂两个关键概念：\n\n**(1) 检查点（Checkpoint）：LangGraph 中用于「持久化存储对话状态」的核心机制，本质是「状态快照」—— 每次流程执行后，会把最新的 `state`（比如 `messages` 对话历史）保存起来，下次执行时可恢复。**\n\n**(2) thread\\_id（理论的memory\\_id）：给「每一组独立对话」分配的唯一标识，相当于对话的「身份证」，用于区分不同用户 / 不同会话的状态。**\n\n#### （2）隔离的核心原理：`thread_id` 作为状态的「隔离键」\n\nLangGraph 的 `Checkpointer`（这里用的是 `MemorySaver`）会把所有对话状态，按照 `thread_id` 进行「分区存储」—— 不同 `thread_id` 的状态存在完全独立的「命名空间」里，彼此不会覆盖或混淆。\n\n可以把它想象成「文件柜」：\n\n* `Checkpointer` = 一个文件柜\n* `thread_id` = 文件柜里的「抽屉编号」\n* 每个抽屉（`thread_id`）里存放对应对话的状态（`messages` 等）\n* 打开抽屉时，只能看到当前 `thread_id` 下的文件（状态），看不到其他抽屉的内容\n\n关键逻辑：状态的「存储」与「读取」都绑定 `thread_id`\n\n**(1) 存储时：流程执行后，`Checkpointer` 会把更新后的 `state` 与传入的 `thread_id` 绑定，存入对应分区；**\n\n**(2) 读取时：下次调用流程传入相同 `thread_id`，`Checkpointer` 会自动加载该分区下的历史状态，继续推进对话；**\n\n**(3) 隔离时：不同 `thread_id` 对应不同分区，加载状态时只会读取自己分区的内容，自然实现隔离。**\n\n#### （3）核心组件：`MemorySaver` 如何实现分区存储？\n\n代码中用的 `MemorySaver` 是 LangGraph 提供的「内存级检查点实现」（适用于开发测试），其内部维护了一个「字典结构」，专门用于按 `thread_id` 分区存储状态。\n\n`MemorySaver` 的简化内部逻辑（伪代码）\n\n```\n class MemorySaver: def __init__(self): # 核心存储结构：key=thread_id，value=该线程的最新状态（checkpoint） self.storage = {} # 保存状态：绑定 thread_id def save_checkpoint(self, thread_id, state): self.storage[thread_id] = state # 按 thread_id 存入对应键值对 # 读取状态：按 thread_id 提取 def get_checkpoint(self, thread_id): return self.storage.get(thread_id, None) # 只返回当前 thread_id 的状态 \n```\n\n结合 的代码看：\n\n* 当 传入 `config = {"configurable": {"thread_id": "1"}}` 时，`MemorySaver` 会把对话状态存入 `self.storage["1"]`；\n* 后续传入相同 `thread_id="1"`，会读取 `self.storage["1"]` 中的历史状态（比如第一次对话的 `Hi there! My name is Will.`）；\n* 当传入 `thread_id="2"` 时，会读取 `self.storage["2"]`（初始为空，所以不记得名字）。\n\n#### (4)、完整执行流程：用你的代码示例验证隔离逻辑\n\n以你代码中的 4 个示例为例，一步步看 `thread_id` 如何隔离：\n\n示例 1：第一次对话（thread\\_id="1"）\n\n**(1) 传入 `config={"configurable": {"thread_id": "1"}}` 和用户输入 `Hi there! My name is Will.`；**\n\n**(2) `graph.stream()` 启动流程，`Checkpointer` 检查 `thread_id="1"` 的存储：无历史状态；**\n\n**(3) 执行 `chatbot` 节点，生成回复（比如 `Hello Will! Nice to meet you!`）；**\n\n**(4) `Checkpointer` 把更新后的 `state`（包含用户输入 + AI 回复）存入 `storage["1"]`；**\n\n**(5) 输出回复，流程结束。**\n\n示例 2：测试记忆（thread\\_id="1"）\n\n**(1) 传入相同 `config`（`thread_id="1"`）和用户输入 `Remember my name?`；**\n\n**(2) `Checkpointer` 读取 `storage["1"]` 中的历史状态（包含第一次的对话）；**\n\n**(3) `chatbot` 节点基于历史状态生成回复（`Yes! Your name is Will.`）；**\n\n**(4) 更新后的状态（追加本次对话）再次存入 `storage["1"]`；**\n\n**(5) 输出回复，记忆生效。**\n\n示例 3：新线程（thread\\_id="2"）\n\n**(1) 传入 `config={"configurable": {"thread_id": "2"}}` 和用户输入 `Remember my name?`；**\n\n**(2) `Checkpointer` 检查 `storage["2"]`：无历史状态（新线程）；**\n\n**(3) `chatbot` 节点没有历史上下文，生成回复（`I don\'t think we\'ve met before. Could you tell me your name?`）；**\n\n**(4) 状态存入 `storage["2"]`；**\n\n**(5) 输出回复，实现隔离（不记得名字）。**\n\n示例 4：返回旧线程（thread\\_id="1"）\n\n**(1) 再次传入 `thread_id="1"`，`Checkpointer` 读取 `storage["1"]` 中的历史状态（包含前两次对话）；**\n\n**(2) `chatbot` 节点基于历史状态，仍然记得名字，生成正确回复；**\n\n**(3) 验证记忆持久性和隔离性。**\n\n#### (4)、关键细节：为什么必须通过 `config` 传入 `thread_id`？\n\nLangGraph 中，`config` 是「流程配置的统一入口」，`configurable` 字段用于传递「可配置的全局参数」，其中 `thread_id` 是 LangGraph 约定的「状态隔离关键字」—— 只有通过 `config={"configurable": {"thread_id": "xxx"}}` 传入，`Checkpointer` 才能识别并用于状态分区。\n\n如果不传入 `thread_id`：\n\n* `Checkpointer` 会使用默认的 `thread_id`（通常是一个随机值或固定值）；\n* 所有对话会共享同一个状态，无法实现隔离（比如新用户会看到上一个用户的对话历史）。\n\n#### (6)、生产环境扩展：除了 MemorySaver，还有哪些 Checkpointer？\n\n`MemorySaver` 是内存级实现，程序重启后状态会丢失，适用于开发测试。\n\n生产环境中，为了实现「持久化隔离」，可以使用以下 `Checkpointer`：\n\n**(1) `RedisSaver`：基于 Redis 存储状态，支持分布式部署，`thread_id` 作为 Redis 的 key；**\n\n**(2) `SQLiteSaver`：基于 SQLite 数据库存储，适合轻量生产环境；**\n\n**(3) `PostgresSaver`：基于 PostgreSQL 数据库存储，适合大规模生产环境。**\n\n它们的隔离原理完全一致 —— 都是以 `thread_id` 作为「隔离键」，只是存储介质不同，确保不同对话的状态独立存储。\n\n#### (7)、核心总结\n\n`thread_id` 的隔离能力，本质是「Checkpointer 按 thread\\_id 分区存储对话状态」：\n\n**(1) `thread_id` 是对话的唯一标识，作为状态存储的「分区键」；**\n\n**(2) `Checkpointer`（如 `MemorySaver`）内部维护按 `thread_id` 划分的存储结构；**\n\n**(3) 相同 `thread_id` 复用历史状态，不同 `thread_id` 从零开始，实现完全隔离；**\n\n**(4) 隔离的核心价值：支持多用户 / 多会话同时使用，彼此不干扰，且能保持各自的对话上下文。**\n\n这种设计让 LangGraph 能够轻松支持「多用户聊天机器人」「会话级记忆」等场景，是生产级对话系统的核心能力之一。\n\n#### (8)、这个thread\\_id（线程 ID） 和 python的线程id 有关系吗\n\n**完全没有关系**！这是两个名字碰巧都带 “线程”，但语义、作用、实现逻辑完全独立的概念 —— 可以理解为 “同名不同姓”，毫无关联。\n\n| 对比维度 | LangGraph 的 `thread_id`（对话线程 ID） | Python 的 `threading.Thread.ident`（系统线程 ID） |\n| --- | --- | --- |\n| **核心作用** | 区分「不同的对话会话」，实现对话状态隔离（记忆隔离） | 区分「操作系统中的执行线程」，实现并发任务调度 |\n| **本质** | 对话的「唯一标识 / 身份证」（字符串 / 数字均可） | 操作系统分配给线程的「执行单元编号」（整数） |\n| **使用场景** | 多用户 / 多会话聊天机器人（比如用户 A 和用户 B 的对话互不干扰） | Python 程序中并发执行多个任务（比如同时下载文件 + 处理数据） |\n| **存储关联** | 与 LangGraph 的 `Checkpointer`（状态存储）绑定，用于分区存储对话历史 | 与操作系统的线程调度器绑定，用于标识执行上下文 |\n| **生命周期** | 随对话存在（可手动指定，比如 `thread_id="user_123"` 可长期有效） | 随 Python 线程启动而创建，线程结束而销毁 |\n| **示例取值** | `"1"`、`"user_888"`、`"session_xyz"`（自定义字符串 / 数字） | `140703324567360`（操作系统分配的整数，不可自定义） |\n\n通俗解释：两个 “线程” 的本质区别\n\n**(1) LangGraph 的 `thread_id`：对话的 “专属文件夹”**\n\n* 它不是 “执行任务的线程”，而是「对话会话的唯一标识」。\n* 类比：你在聊天软件上和 A 朋友、B 朋友的对话，会存在两个独立的 “聊天窗口”（文件夹），`thread_id` 就是这两个窗口的编号 —— 确保你和 A 的聊天记录不会出现在和 B 的窗口里。\n* 核心：**用于 “数据隔离”（对话状态 / 记忆隔离）**。\n\n**(2) Python 的线程 ID：程序的 “干活工人编号”**\n\n* 它是操作系统层面的「执行单元标识」，对应一个 “干活的工人”。\n* 类比：你让程序同时 “下载文件” 和 “处理数据”，操作系统会分配两个 “工人”（线程），每个工人有唯一编号（线程 ID），负责独立完成任务，互不干扰。\n* 核心：**用于 “并发执行”（任务调度隔离）**。\n\n（1）LangGraph 的 `thread_id`（对话隔离）\n\n```\n # 两个不同的对话会话，用 thread_id 隔离 config_user_a = {"configurable": {"thread_id": "user_a"}} # 用户A的对话标识 config_user_b = {"configurable": {"thread_id": "user_b"}} # 用户B的对话标识 # 两个会话的状态独立存储，互不干扰 graph.stream({"messages": [{"role": "user", "content": "我是A"}]}, config_user_a) graph.stream({"messages": [{"role": "user", "content": "我是B"}]}, config_user_b) \n```\n\n（2）Python 的线程 ID（并发执行）\n\n```\n import threading def task(name): # 获取当前 Python 线程的 ID（操作系统分配） print(f"任务 {name} 运行在 Python 线程 ID: {threading.get_ident()}") # 启动两个并发线程，各自有独立的线程 ID thread1 = threading.Thread(target=task, args=("下载文件",)) thread2 = threading.Thread(target=task, args=("处理数据",)) thread1.start() # 输出：任务 下载文件 运行在 Python 线程 ID: 140703324567360 thread2.start() # 输出：任务 处理数据 运行在 Python 线程 ID: 140703324567361 \n```\n\n**(1) 两者无任何技术关联：LangGraph 的 `thread_id` 是 LangGraph 框架自定义的「对话标识」，Python 的线程 ID 是操作系统分配的「执行单元标识」；**\n\n**(2) 作用完全不同：一个管「对话数据隔离」（记忆不混淆），一个管「程序并发执行」（任务不干扰）；**\n\n**(3) 可以叠加使用：比如用 Python 多线程同时处理 100 个用户的对话，每个用户的对话用独立的 `thread_id` 隔离状态 —— 此时 Python 线程 ID 是 “工人编号”，LangGraph 的 `thread_id` 是 “每个工人处理的对话文件夹编号”，互不冲突。**\n\n简单记：**LangGraph 的 thread\\_id 管 “对话记忆”，Python 的线程 ID 管 “并发干活”，八竿子打不着**。\n\n### 4.6 小结\n\n加了 `MemorySaver`，它就开始记事了，能维持上下文，像个正常人聊天。\n\n一是 `MemorySaver`，内存存状态，开发够用；\n\n二是 `thread_id`，区分不同用户的对话流；\n\n三是 `checkpointer`，让图自动保存和恢复。\n\n> 流程：读历史 → 推理 → 工具调用 → 保存状态。一次完整带记忆的交互。\n\n### 第四章小节：LangGraph总结与展望\n\n整个过程像搭积木，一步接一步，核心靠的就是 **LangGraph** —— 它像个智能调度员，把各个功能串起来，让机器人真正“活”了起来。\n\n### LangGraph 五件事 小节\n\n**(1) LangGraph 是怎么工作的？**\n\n**(2) 上下文怎么记住？**\n\n**(3) 怎么让它会用工具？**\n\n**(4) 跨会话记忆怎么实现？**\n\n**(5) 多人聊天不串台？**\n\n> 图解：识别身份 → 加载记忆 → 判断是否需查资料 → 更新状态并回复。颜色代表：初始化（绿）、线程处理（蓝）、工具执行（橙）、状态操作（紫），数据流向一目了然。\n\n### 4.6 LangGraph 展望\n\n以 LangGraph 状态机为核心，扩展持久化、协同和人工干预能力，升级成可落地的AI系统。\n\n**(1) 长期记忆落地**\n\n当前状态存在内存里，重启全丢。应该接入 Redis 或 PostgreSQL，实现持久存储——把便签本换成保险柜。\n\n**(2) 关键操作让人把关**\n\n**(3) 多个AI组队干活**\n\n一个干不过来？那就组团。比如一个接待、一个查资料、一个写报告。用 LangGraph 编排多个智能体协作，打造“AI员工团队”。\n\n**(4) 对话支持“撤销”**\n\n**(5) 上生产环境扛高并发**\n\n本地跑得好，线上可能崩。要结合检查点 + 容器化部署，加上监控、容错、负载均衡，才能撑住真实流量。\n\nLangGraph 正在飞速进化，功能越来越多。但万变不离其宗：**用状态机管理AI流程**。掌握了这一点，你就拿到了构建高级AI系统的钥匙。\n\n## 第五章：langgraph 常见陷阱和解决方案\n\n> ......... 略5000字+\n>\n> ...................由于平台篇幅限制， 剩下的内容(5000字+)，请参参见原文地址\n\n#### 原始的内容，请参考 本文 的 原文 地址\n\n[本文 的 原文 地址](https://mp.weixin.qq.com/s/3Unzc4dHuYDwQyBgBaRKtA)\n\n## 第七章 大实操 预告 ：使用LangGraph从零构建多智能体AI系统：实现智能协作的完整指南\n\n尼恩团队即将推出， 基于langgraph的java 运维智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent\n\n在当今快速发展的AI领域，单一的大语言模型（LLM）已经难以满足复杂、多步骤的业务需求。多智能体（Multi-Agent）系统应运而生，它通过将多个专用的子智能体（sub-agent）组合起来，协同完成一项复杂任务，从而显著提升了系统的鲁棒性、可扩展性和问题解决能力。  \n 本文将以一个**智能 运维助手**的实际场景为例，深入探讨如何使用强大的[图计算框架， 从零开始构建一个功能完备的多Agent系统。\n\n我们将覆盖从基础架构设计、状态与记忆管理、人机回环 ），到最终使用开源可观测性平台 **LangFuse** 进行系统评测的全过程。\n\n* **指标查询Agent (metric Status Agent)** ：负责连接 Prometheus 数据库 ，查询的实时运行参数，如cpu、内存、jvm gc等。\n* **日志查询Agent (log search Agent)** ：负责连接 Elasticsearch\'日志数据库 ，查询的error 日志。\n* **问题定位Agent (question alanasys Agent)** ：负责根据日志和指标，分析的error 日志的根因。\n* **工单任务调度Agent (Maintenance Scheduler Agent)** ：负责与工单系统（Ticketing System）交互，根据故障信息创建、查询和更新维保任务。\n* **主管Agent (Supervisor)** ：作为总协调者，负责理解用户意图，并将任务路由给合适的子Agent。它不直接执行工具，而是“指挥”其他Agent工作。\n\n这个第七章 大实操 ，即将结合 尼恩团队后续 的langgraph **学习圣经推出，敬请期待。**\n\n* langgraph 学习圣经 （1）： 从0到1穿透 multi-agent多智能体 入门实战\n* langgraph 学习圣经 （2）： 从0到1穿透LangGraph架构与源码\n* langgraph 学习圣经（3） : 基于langgraph的java 运维多智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent 等\n\nposted @ 2025-12-28 21:38\xa0 [技术自由圈](https://www.cnblogs.com/crazymakercircle)\xa0 阅读(292)\xa0 评论(0)\xa0 \xa0 [收藏](javascript:void(0))\xa0 [举报](javascript:void(0))\n\n[刷新页面](#)[返回顶部](#top)\n\n[博客园](https://www.cnblogs.com/)  \xa0©\xa0 2004-2026   \n [浙公网安备 33010602011771号](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33010602011771) [浙ICP备2021040463号-3](https://beian.miit.gov.cn)\n\n ', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://www.cnblogs.com/crazymakercircle/p/19412858', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.99986124, 'saved_path': None}}
2026-02-22 20:38:45,299 - __main__ - INFO - call_tool: name=exa_context_download, args={'papers': [{'paper_id': '', 'title': '在Google Cloud 上使用LangGraph、EDA 和生成式AI 构建 ...', 'authors': [], 'abstract': 'Aidemy：在 Google Cloud 上使用LangGraph、EDA 和生成式AI 构建多代理系统| Google Codelabs[跳至主要内容] \n[![Google Codelabs]] \n/\n* English\n* Deutsch\n* Español\n* Español –América Latina\n* Français\n* Indonesia\n* Italiano\n* Polski\n* Português –Brasil\n* Tiếng Việt\n* Türkçe\n* Русский* עברית* العربيّة* فارسی* हिंदी* বাংলা* ภาษาไทย* 中文–简体* 中文–繁體* 日本語* 한국어登录# Aidemy：在 Google Cloud 上使用LangGraph、EDA 和生成式AI 构建多代理系统## 1. 简介您好！您对智能体的概念很感兴趣，对吧？智能体是一种小助手，无需您动手即可帮您处理各种事务。太棒了！但实际上，一个代理并不总是能满足需求，尤其是在处理更大、更复杂的项目时。您可能需要一整个团队的他们！这时，多智能体系统就能派上用场了。与旧式的硬编码相比，由LLM 提供支持的代理可为您提供出色的灵活性。不过，总会有一些问题，这些模型也面临着一系列棘手的挑战。这正是我们将在本次研讨会中深入探讨的内容！![title] \n以下是您将学到的内容，可帮助您提升代理技能：**使用 LangGraph 构建您的第一个代理**：我们将使用热门框架 LangGraph 亲手构建您自己的代理。您将学习如何创建可连接到数据库的工具，如何利用最新的Gemini 2 API 进行一些互联网搜索，以及如何优化提示和回答，以便您的代理不仅可以与LLM 互动，还可以与现有服务互动。我们还将向您展示函数调用的运作方式。**以您的方式编排智能体**：我们将探索编排智能体的不同方式，从简单的直线路径到更复杂的多路径场景。您可以将其视为指导代理团队的运作。\n**多智能体系统**：您将了解如何设置一个智能体可以协作并共同完成任务的系统，而这一切都归功于事件驱动型架构。\n**LLM 自由**：选择最适合的 LLM：我们不必只使用一个 LLM！您将了解如何使用多个 LLM，为它们分配不同的角色，从而利用出色的“思维模型”来提升问题解决能力。\n**什么是动态内容？没问题！**：想象一下，您的代理可以实时创建专门为每位用户量身定制的动态内容。我们将向您展示如何操作！\n**使用 Google Cloud 将数据分析提升到新高度**：不要再局限于在笔记本中进行简单的数据分析。我们将向您展示如何在 Google Cloud 上设计和部署多代理系统，以便该系统能够投入实际应用！此项目将很好地展示如何使用我们讨论的所有技巧。## 2. 架构当老师或从事教育工作非常有意义，但我们必须承认，工作量（尤其是所有准备工作）可能非常具有挑战性！此外，学校往往没有足够的员工，辅导费用可能很高。因此，我们建议使用AI 赋能的教学助理。此工具可减轻教育工作者的负担，并帮助弥合因人员短缺和缺乏经济实惠的辅导而造成的差距。我们的AI 教学助理可以快速生成详细的课程计划、有趣的测验、易于理解的音频总结和个性化的作业。这样一来，教师就可以专注于自己最擅长的事情：与学生建立联系，帮助他们爱上学习。该系统包含两个网站：一个供教师创建未来几周的课程计划，![规划本] \n另一个供学生访问测验、音频总结和作业。![门户] \n好的，我们来了解一下为Aidemy 教学助理提供支持的架构。如您所见，我们已将其分解为几个关键组件，这些组件协同工作以实现此目的。![架构] \n**关键架构元素和技术**：\n**Google Cloud Platform (GCP)**：整个系统的核心：\n* Vertex AI：访问 Google 的Gemini LLM。\n* Cloud Run：用于部署容器化代理和函数的无服务器平台。\n* Cloud SQL：用于存储课程数据的 PostgreSQL 数据库。* Pub/Sub 和Eventarc：事件驱动型架构的基础，可实现组件之间的异步通信。\n* Cloud Storage：存储音频总结和作业文件。\n* Secret Manager：安全地管理数据库凭据。\n* Artifact Registry：存储代理的 Docker 映像。* Compute Engine：部署自托管 LLM，而不是依赖于供应商解决方案\n**LLM**：系统的“大脑”：\n* Google 的Gemini 模型：（Gemini x Pro、Gemini x Flash、Gemini x Flash Thinking）用于课程规划、内容生成、动态 HTML 创建、测验解释和作业合并。* DeepSeek：用于生成自学作业的专业任务\n**LangChain 和LangGraph**：用于 LLM 应用开发的框架* 有助于创建复杂的多智能体工作流。* 支持对工具（API 调用、数据库查询、网页搜索）进行智能编排。* 实现事件驱动型架构，以提高系统可伸缩性和灵活性。从本质上讲，我们的架构将LLM 的强大功能与结构化数据和事件驱动型通信相结合，所有这些都在Google Cloud 上运行。这使我们能够构建可扩缩、可靠且有效的教学助理。## 3. 准备工作**对于 Google Cloud 赠金**：为帮助您入门，请使用此[链接] 兑换免费 Google Cloud 赠金。您可以按照[此处的说明] 激活抵扣金额并创建新项目，也可以按照以下说明操作。\n在[Google Cloud 控制台] 的项目选择器页面上，选择或创建一个 Google Cloud[项目] 。确保您的 Cloud 项目已启用结算功能。[了解如何检查项目是否已启用结算功能] 。\n**在 Cloud Shell IDE 中启用Gemini Code Assist**\n👉在Google Cloud 控制台中，前往Gemini Code Assist 工具，同意相关条款及条件，即可免费启用Gemini Code Assist。\n![01-04-code-assist-enable.png] \n忽略权限设置，离开此页面。**在 Cloud Shell 编辑器中工作**\n👉点击Google Cloud 控制台顶部的**激活 Cloud Shell**（这是 Cloud Shell 窗格顶部的终端形状图标），然后点击“打开**编辑器**”按钮（看起来像一个带有铅笔的打开的文件夹）。此操作会在窗口中打开 Cloud Shell 代码编辑器。您会在左侧看到文件资源管理器。![Cloud Shell] \n👉如图所示，点击底部状态栏中的**Cloud Code 登录**按钮。按照说明对插件进行授权。如果您在状态栏中看到**Cloud Code - no project**，请选择该选项，然后在下拉菜单中选择“Select a Google Cloud Project”（选择 Google Cloud 项目），然后从您创建的项目列表中选择特定的Google Cloud 项目。![登录项目] \n👉在云IDE 中打开终端，![新终端] 或![新终端] \n👉在终端中，使用以下命令验证您是否已通过身份验证，以及项目是否已设置为您的项目ID：\n```\n`gcloudauthlist`\n```\n👉并运行，确保将*&lt;&lt;YOUR\\_PROJECT\\_ID\\>*替换为您的项目 ID：\n```\n`echo&lt;YOUR\\_PROJECT\\_ID&gt; &gt;\\~/project\\_id.txtgcloudconfigsetproject$(cat\\~/project\\_id.txt)`\n```\n👉运行以下命令以启用必要的Google Cloud API：\n```\n`gcloudservicesenablecompute.googleapis.com\\\\storage.googleapis.com\\\\run.googleapis.com\\\\artifactregistry.googleapis.com\\\\aiplatform.googleapis.com\\\\eventarc.googleapis.com\\\\sqladmin.googleapis.com\\\\secretmanager.googleapis.com\\\\cloudbuild.googleapis.com\\\\cloudresourcemanager.googleapis.com\\\\cloudfunctions.googleapis.com\\\\cloudaicompanion.googleapis.com`\n```\n这可能需要几分钟的时间。**设置权限**\n👉设置服务账号权限。在终端中，运行以下命令：```\n`gcloudconfigsetproject$(cat\\~/project\\_id.txt)exportPROJECT\\_ID=$(gcloudconfiggetproject)exportSERVICE\\_ACCOUNT\\_NAME=$(gcloudcomputeproject-infodescribe--format="value(defaultServiceAccount)")echo"Here\'s your SERVICE\\_ACCOUNT\\_NAME $SERVICE\\_ACCOUNT\\_NAME"`\n```\n👉授予权限。在终端中，运行以下命令：```\n`#Cloud Storage (Read/Write):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/storage.objectAdmin"#Pub/Sub (Publish/Receive):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/pubsub.publisher"gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/pubsub.subscriber"#Cloud SQL (Read/Write):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/cloudsql.editor"#Eventarc (Receive Events):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/iam.serviceAccountTokenCreator"gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/eventarc.eventReceiver"#Vertex AI (User):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/aiplatform.user"#Secret Manager (Read):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/secretmanager.secretAccessor"`\n```\n👉在[IAM 控制台] 中验证结果![IAM 控制台] \n👉在终端中运行以下命令，以创建名为**aidemy**的 Cloud SQL 实例。我们稍后会用到此功能，但由于此过程可能需要一些时间，因此我们现在就来完成。```\n`gcloudsqlinstancescreateaidemy\\\\--database-version=POSTGRES\\_14\\\\--cpu=2\\\\--memory=4GB\\\\--region=us-central1\\\\--root-password=1234qwer\\\\--storage-size=10GB\\\\--storage-auto-increase`\n```\n预配实例需要一段时间，请让此命令运行，然后继续执行下一步。## 4. 构建第一个代理在深入探讨复杂的多智能体系统之前，我们需要先建立一个基本构建块：单个功能性智能体。在本部分中，我们将迈出第一步，创建一个简单的“图书提供商”代理。图书提供商代理会接受一个类别作为输入，并使用Gemini LLM 生成该类别中的图书的JSON 表示形式。然后，它会以REST API 端点的形式提供这些图书推荐。![图书提供商]<image_link>\n👉在另一个浏览器标签页中，在网络浏览器中打开[Google Cloud 控制台]<web_link>。在导航菜单 (☰) 中，前往“Cloud Run”。点击“+ ... 编写函数”按钮。![创建函数]<image_link>\n👉接下来，我们将配置Cloud Run 函数的基本设置：* 服务名称：`book-provider`\n* 区域：`us-central1`\n* 运行时：`Python 3.12`\n* 身份验证：从`Allow unauthenticated invocations`更改为“已启用”。\n将“允许未经身份验证的调用”设置为启用后，外部流量无需身份验证即可访问该函数。👉将其他设置保留为默认值，然后点击**创建**。系统会将您转到源代码编辑器。\n您会看到预先填充的`main.py`和`requirements.txt`文件。\n`main.py`将包含函数的业务逻辑，`requirements.txt`将包含所需的软件包。\n👉现在，我们准备好编写一些代码了！不过，在深入了解之前，我们先看看**Gemini Code Assist**能否帮助我们抢占先机。**返回到 Cloud Shell 编辑器**，点击顶部的 Gemini Code Assist 图标，系统应会打开Gemini Code Assist 对话。![Gemini Code Assist]<image_link>\n您可能会看到一个弹出式窗口，其中包含类似“您缺少有效的Gemini Code Assist 许可，因此可能无法再访问该服务。请与您的结算管理员联系，购买或分配许可。”请忽略此消息。👉将以下请求粘贴到提示框中：```\n`Usethefunctions\\_frameworklibrarytobedeployableasanHTTPfunction.Acceptarequestwithcategoryandnumber\\_of\\_bookparameters(eitherinJSONbodyorquerystring).Uselangchainandgeminitogeneratethedataforbookwithfieldsbookname,author,publisher,publishing\\_date.UsepydantictodefineaBookmodelwiththefields:bookname(string,description:"Name of the book"),author(string,description:"Name of the author"),publisher(string,description:"Name of the publisher"),andpublishing\\_date(string,description:"Date of publishing").Uselangchainandgeminimodeltogeneratebookdata.theoutputshouldfollowtheformatdefinedinBookmodel.ThelogicshoulduseJsonOutputParserfromlangchaintoenforceoutputformatdefinedinBookModel.Haveafunctionget\\_recommended\\_books(category)thatinternallyuseslangchainandgeminitoreturnasinglebookobject.Themainfunction,exposedastheCloudFunction,shouldcallget\\_recommended\\_books()multipletimes(basedonnumber\\_of\\_book)andreturnaJSONlistofthegeneratedbookobjects.Handlethecasewherecategoryornumber\\_of\\_bookaremissingbyreturninganerrorJSONresponsewitha400statuscode.returnaJSONstringrepresentingtherecommendedbooks.useoslibrarytoretrieveGOOGLE\\_CLOUD\\_PROJECTenvvar.UseChatVertexAIfromlangchainfortheLLMcall`\n```\n然后，Code Assist 会生成一个潜在的解决方案，同时提供源代码和requirements.txt 依赖项文件。（请勿使用此代码）建议您将Code Assist 生成的代码与下面提供的经过测试的正确解决方案进行比较。这样，您就可以评估该工具的效果并发现任何潜在的差异。虽然绝不应盲目信任LLM，但 Code Assist 是一款出色的工具，可用于快速原型设计和生成初始代码结构，应使用它来获得良好的开端。由于这是一个研讨会，我们将继续使用下方提供的已验证代码。不过，您可以自行尝试使用Code Assist 生成的代码，以便更深入地了解其功能和局限性。👉返回到Cloud Run 函数的源代码编辑器（在另一个浏览器标签页中）。请仔细将`main.py`的现有内容替换为以下代码：\n```\n`importfunctions\\_frameworkimportjsonfromflaskimportFlask,jsonify,requestfromlangchain\\_google\\_vertexaiimportChatVertexAIfromlangchain\\_core.output\\_parsersimportJsonOutputParserfromlangchain\\_core.promptsimportPromptTemplatefrompydanticimportBaseModel,FieldimportosclassBook(BaseModel):bookname:str=Field(description="Name of the book")author:str=Field(description="Name of the author")publisher:str=Field(description="Name of the publisher")publishing\\_date:str=Field(description="Date of publishing")project\\_id=os.environ.get("GOOGLE\\_CLOUD\\_PROJECT")llm=ChatVertexAI(model\\_name="gemini-2.0-flash-lite-001")defget\\_recommended\\_books(category):"""A simple book recommendation function.Args:category (str): categoryReturns:str: A JSON string representing the recommended books."""parser=JsonOutputParser(pydantic\\_object=Book)question=f"Generate a random made up book on{category}with bookname, author and publisher and publishing\\_date"prompt=PromptTemplate(template="Answer the user query.\\\\n{format\\_instructions}\\\\n{query}\\\\n",input\\_variables=["query"],partial\\_variables={"format\\_instructions":parser.get\\_format\\_instructions()},)chain=prompt|llm|parserresponse=chain.invoke({"query":question})returnjson.dumps(response)@functions\\_framework.httpdefrecommended(request):request\\_json=request.get\\_json(silent=True)# Get JSON dataifrequest\\_jsonand\'category\'inrequest\\_jsonand\'number\\_of\\_book\'inrequest\\_json:category=request\\_json[\'category\']number\\_of\\_book=int(request\\_json[\'number\\_of\\_book\'])elifrequest.argsand\'category\'inrequest.argsand\'number\\_of\\_book\'inrequest.args:category=request.args.get(\'category\')number\\_of\\_book=int(request.args.get(\'number\\_of\\_book\'))else:returnjsonify({\'error\':\'Missing category or number\\_of\\_book parameters\'}),400recommendations\\_list=[]foriinrange(number\\_of\\_book)', 'doi': '', 'published_date': '2025-12-22T00:00:00+00:00', 'pdf_url': '', 'url': 'https://codelabs.developers.google.com/aidemy-multi-agent/instructions?hl=zh-cn', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:45,300 - __main__ - INFO - handle_download: searcher=ExaSearcherContext, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:45,300 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:45,300 - __main__ - INFO - call_tool payload: source_tool=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:45,300 - __main__ - INFO - call_tool: name=exa_context_download, result_type=papers, count=1
2026-02-22 20:38:45,300 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': '在Google Cloud 上使用LangGraph、EDA 和生成式AI 构建 ...', 'authors': [], 'abstract': 'Aidemy：在 Google Cloud 上使用LangGraph、EDA 和生成式AI 构建多代理系统| Google Codelabs[跳至主要内容] \n[![Google Codelabs]] \n/\n* English\n* Deutsch\n* Español\n* Español –América Latina\n* Français\n* Indonesia\n* Italiano\n* Polski\n* Português –Brasil\n* Tiếng Việt\n* Türkçe\n* Русский* עברית* العربيّة* فارسی* हिंदी* বাংলা* ภาษาไทย* 中文–简体* 中文–繁體* 日本語* 한국어登录# Aidemy：在 Google Cloud 上使用LangGraph、EDA 和生成式AI 构建多代理系统## 1. 简介您好！您对智能体的概念很感兴趣，对吧？智能体是一种小助手，无需您动手即可帮您处理各种事务。太棒了！但实际上，一个代理并不总是能满足需求，尤其是在处理更大、更复杂的项目时。您可能需要一整个团队的他们！这时，多智能体系统就能派上用场了。与旧式的硬编码相比，由LLM 提供支持的代理可为您提供出色的灵活性。不过，总会有一些问题，这些模型也面临着一系列棘手的挑战。这正是我们将在本次研讨会中深入探讨的内容！![title] \n以下是您将学到的内容，可帮助您提升代理技能：**使用 LangGraph 构建您的第一个代理**：我们将使用热门框架 LangGraph 亲手构建您自己的代理。您将学习如何创建可连接到数据库的工具，如何利用最新的Gemini 2 API 进行一些互联网搜索，以及如何优化提示和回答，以便您的代理不仅可以与LLM 互动，还可以与现有服务互动。我们还将向您展示函数调用的运作方式。**以您的方式编排智能体**：我们将探索编排智能体的不同方式，从简单的直线路径到更复杂的多路径场景。您可以将其视为指导代理团队的运作。\n**多智能体系统**：您将了解如何设置一个智能体可以协作并共同完成任务的系统，而这一切都归功于事件驱动型架构。\n**LLM 自由**：选择最适合的 LLM：我们不必只使用一个 LLM！您将了解如何使用多个 LLM，为它们分配不同的角色，从而利用出色的“思维模型”来提升问题解决能力。\n**什么是动态内容？没问题！**：想象一下，您的代理可以实时创建专门为每位用户量身定制的动态内容。我们将向您展示如何操作！\n**使用 Google Cloud 将数据分析提升到新高度**：不要再局限于在笔记本中进行简单的数据分析。我们将向您展示如何在 Google Cloud 上设计和部署多代理系统，以便该系统能够投入实际应用！此项目将很好地展示如何使用我们讨论的所有技巧。## 2. 架构当老师或从事教育工作非常有意义，但我们必须承认，工作量（尤其是所有准备工作）可能非常具有挑战性！此外，学校往往没有足够的员工，辅导费用可能很高。因此，我们建议使用AI 赋能的教学助理。此工具可减轻教育工作者的负担，并帮助弥合因人员短缺和缺乏经济实惠的辅导而造成的差距。我们的AI 教学助理可以快速生成详细的课程计划、有趣的测验、易于理解的音频总结和个性化的作业。这样一来，教师就可以专注于自己最擅长的事情：与学生建立联系，帮助他们爱上学习。该系统包含两个网站：一个供教师创建未来几周的课程计划，![规划本] \n另一个供学生访问测验、音频总结和作业。![门户] \n好的，我们来了解一下为Aidemy 教学助理提供支持的架构。如您所见，我们已将其分解为几个关键组件，这些组件协同工作以实现此目的。![架构] \n**关键架构元素和技术**：\n**Google Cloud Platform (GCP)**：整个系统的核心：\n* Vertex AI：访问 Google 的Gemini LLM。\n* Cloud Run：用于部署容器化代理和函数的无服务器平台。\n* Cloud SQL：用于存储课程数据的 PostgreSQL 数据库。* Pub/Sub 和Eventarc：事件驱动型架构的基础，可实现组件之间的异步通信。\n* Cloud Storage：存储音频总结和作业文件。\n* Secret Manager：安全地管理数据库凭据。\n* Artifact Registry：存储代理的 Docker 映像。* Compute Engine：部署自托管 LLM，而不是依赖于供应商解决方案\n**LLM**：系统的“大脑”：\n* Google 的Gemini 模型：（Gemini x Pro、Gemini x Flash、Gemini x Flash Thinking）用于课程规划、内容生成、动态 HTML 创建、测验解释和作业合并。* DeepSeek：用于生成自学作业的专业任务\n**LangChain 和LangGraph**：用于 LLM 应用开发的框架* 有助于创建复杂的多智能体工作流。* 支持对工具（API 调用、数据库查询、网页搜索）进行智能编排。* 实现事件驱动型架构，以提高系统可伸缩性和灵活性。从本质上讲，我们的架构将LLM 的强大功能与结构化数据和事件驱动型通信相结合，所有这些都在Google Cloud 上运行。这使我们能够构建可扩缩、可靠且有效的教学助理。## 3. 准备工作**对于 Google Cloud 赠金**：为帮助您入门，请使用此[链接] 兑换免费 Google Cloud 赠金。您可以按照[此处的说明] 激活抵扣金额并创建新项目，也可以按照以下说明操作。\n在[Google Cloud 控制台] 的项目选择器页面上，选择或创建一个 Google Cloud[项目] 。确保您的 Cloud 项目已启用结算功能。[了解如何检查项目是否已启用结算功能] 。\n**在 Cloud Shell IDE 中启用Gemini Code Assist**\n👉在Google Cloud 控制台中，前往Gemini Code Assist 工具，同意相关条款及条件，即可免费启用Gemini Code Assist。\n![01-04-code-assist-enable.png] \n忽略权限设置，离开此页面。**在 Cloud Shell 编辑器中工作**\n👉点击Google Cloud 控制台顶部的**激活 Cloud Shell**（这是 Cloud Shell 窗格顶部的终端形状图标），然后点击“打开**编辑器**”按钮（看起来像一个带有铅笔的打开的文件夹）。此操作会在窗口中打开 Cloud Shell 代码编辑器。您会在左侧看到文件资源管理器。![Cloud Shell] \n👉如图所示，点击底部状态栏中的**Cloud Code 登录**按钮。按照说明对插件进行授权。如果您在状态栏中看到**Cloud Code - no project**，请选择该选项，然后在下拉菜单中选择“Select a Google Cloud Project”（选择 Google Cloud 项目），然后从您创建的项目列表中选择特定的Google Cloud 项目。![登录项目] \n👉在云IDE 中打开终端，![新终端] 或![新终端] \n👉在终端中，使用以下命令验证您是否已通过身份验证，以及项目是否已设置为您的项目ID：\n```\n`gcloudauthlist`\n```\n👉并运行，确保将*&lt;&lt;YOUR\\_PROJECT\\_ID\\>*替换为您的项目 ID：\n```\n`echo&lt;YOUR\\_PROJECT\\_ID&gt; &gt;\\~/project\\_id.txtgcloudconfigsetproject$(cat\\~/project\\_id.txt)`\n```\n👉运行以下命令以启用必要的Google Cloud API：\n```\n`gcloudservicesenablecompute.googleapis.com\\\\storage.googleapis.com\\\\run.googleapis.com\\\\artifactregistry.googleapis.com\\\\aiplatform.googleapis.com\\\\eventarc.googleapis.com\\\\sqladmin.googleapis.com\\\\secretmanager.googleapis.com\\\\cloudbuild.googleapis.com\\\\cloudresourcemanager.googleapis.com\\\\cloudfunctions.googleapis.com\\\\cloudaicompanion.googleapis.com`\n```\n这可能需要几分钟的时间。**设置权限**\n👉设置服务账号权限。在终端中，运行以下命令：```\n`gcloudconfigsetproject$(cat\\~/project\\_id.txt)exportPROJECT\\_ID=$(gcloudconfiggetproject)exportSERVICE\\_ACCOUNT\\_NAME=$(gcloudcomputeproject-infodescribe--format="value(defaultServiceAccount)")echo"Here\'s your SERVICE\\_ACCOUNT\\_NAME $SERVICE\\_ACCOUNT\\_NAME"`\n```\n👉授予权限。在终端中，运行以下命令：```\n`#Cloud Storage (Read/Write):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/storage.objectAdmin"#Pub/Sub (Publish/Receive):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/pubsub.publisher"gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/pubsub.subscriber"#Cloud SQL (Read/Write):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/cloudsql.editor"#Eventarc (Receive Events):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/iam.serviceAccountTokenCreator"gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/eventarc.eventReceiver"#Vertex AI (User):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/aiplatform.user"#Secret Manager (Read):gcloudprojectsadd-iam-policy-binding$PROJECT\\_ID\\\\--member="serviceAccount:$SERVICE\\_ACCOUNT\\_NAME"\\\\--role="roles/secretmanager.secretAccessor"`\n```\n👉在[IAM 控制台] 中验证结果![IAM 控制台] \n👉在终端中运行以下命令，以创建名为**aidemy**的 Cloud SQL 实例。我们稍后会用到此功能，但由于此过程可能需要一些时间，因此我们现在就来完成。```\n`gcloudsqlinstancescreateaidemy\\\\--database-version=POSTGRES\\_14\\\\--cpu=2\\\\--memory=4GB\\\\--region=us-central1\\\\--root-password=1234qwer\\\\--storage-size=10GB\\\\--storage-auto-increase`\n```\n预配实例需要一段时间，请让此命令运行，然后继续执行下一步。## 4. 构建第一个代理在深入探讨复杂的多智能体系统之前，我们需要先建立一个基本构建块：单个功能性智能体。在本部分中，我们将迈出第一步，创建一个简单的“图书提供商”代理。图书提供商代理会接受一个类别作为输入，并使用Gemini LLM 生成该类别中的图书的JSON 表示形式。然后，它会以REST API 端点的形式提供这些图书推荐。![图书提供商]<image_link>\n👉在另一个浏览器标签页中，在网络浏览器中打开[Google Cloud 控制台]<web_link>。在导航菜单 (☰) 中，前往“Cloud Run”。点击“+ ... 编写函数”按钮。![创建函数]<image_link>\n👉接下来，我们将配置Cloud Run 函数的基本设置：* 服务名称：`book-provider`\n* 区域：`us-central1`\n* 运行时：`Python 3.12`\n* 身份验证：从`Allow unauthenticated invocations`更改为“已启用”。\n将“允许未经身份验证的调用”设置为启用后，外部流量无需身份验证即可访问该函数。👉将其他设置保留为默认值，然后点击**创建**。系统会将您转到源代码编辑器。\n您会看到预先填充的`main.py`和`requirements.txt`文件。\n`main.py`将包含函数的业务逻辑，`requirements.txt`将包含所需的软件包。\n👉现在，我们准备好编写一些代码了！不过，在深入了解之前，我们先看看**Gemini Code Assist**能否帮助我们抢占先机。**返回到 Cloud Shell 编辑器**，点击顶部的 Gemini Code Assist 图标，系统应会打开Gemini Code Assist 对话。![Gemini Code Assist]<image_link>\n您可能会看到一个弹出式窗口，其中包含类似“您缺少有效的Gemini Code Assist 许可，因此可能无法再访问该服务。请与您的结算管理员联系，购买或分配许可。”请忽略此消息。👉将以下请求粘贴到提示框中：```\n`Usethefunctions\\_frameworklibrarytobedeployableasanHTTPfunction.Acceptarequestwithcategoryandnumber\\_of\\_bookparameters(eitherinJSONbodyorquerystring).Uselangchainandgeminitogeneratethedataforbookwithfieldsbookname,author,publisher,publishing\\_date.UsepydantictodefineaBookmodelwiththefields:bookname(string,description:"Name of the book"),author(string,description:"Name of the author"),publisher(string,description:"Name of the publisher"),andpublishing\\_date(string,description:"Date of publishing").Uselangchainandgeminimodeltogeneratebookdata.theoutputshouldfollowtheformatdefinedinBookmodel.ThelogicshoulduseJsonOutputParserfromlangchaintoenforceoutputformatdefinedinBookModel.Haveafunctionget\\_recommended\\_books(category)thatinternallyuseslangchainandgeminitoreturnasinglebookobject.Themainfunction,exposedastheCloudFunction,shouldcallget\\_recommended\\_books()multipletimes(basedonnumber\\_of\\_book)andreturnaJSONlistofthegeneratedbookobjects.Handlethecasewherecategoryornumber\\_of\\_bookaremissingbyreturninganerrorJSONresponsewitha400statuscode.returnaJSONstringrepresentingtherecommendedbooks.useoslibrarytoretrieveGOOGLE\\_CLOUD\\_PROJECTenvvar.UseChatVertexAIfromlangchainfortheLLMcall`\n```\n然后，Code Assist 会生成一个潜在的解决方案，同时提供源代码和requirements.txt 依赖项文件。（请勿使用此代码）建议您将Code Assist 生成的代码与下面提供的经过测试的正确解决方案进行比较。这样，您就可以评估该工具的效果并发现任何潜在的差异。虽然绝不应盲目信任LLM，但 Code Assist 是一款出色的工具，可用于快速原型设计和生成初始代码结构，应使用它来获得良好的开端。由于这是一个研讨会，我们将继续使用下方提供的已验证代码。不过，您可以自行尝试使用Code Assist 生成的代码，以便更深入地了解其功能和局限性。👉返回到Cloud Run 函数的源代码编辑器（在另一个浏览器标签页中）。请仔细将`main.py`的现有内容替换为以下代码：\n```\n`importfunctions\\_frameworkimportjsonfromflaskimportFlask,jsonify,requestfromlangchain\\_google\\_vertexaiimportChatVertexAIfromlangchain\\_core.output\\_parsersimportJsonOutputParserfromlangchain\\_core.promptsimportPromptTemplatefrompydanticimportBaseModel,FieldimportosclassBook(BaseModel):bookname:str=Field(description="Name of the book")author:str=Field(description="Name of the author")publisher:str=Field(description="Name of the publisher")publishing\\_date:str=Field(description="Date of publishing")project\\_id=os.environ.get("GOOGLE\\_CLOUD\\_PROJECT")llm=ChatVertexAI(model\\_name="gemini-2.0-flash-lite-001")defget\\_recommended\\_books(category):"""A simple book recommendation function.Args:category (str): categoryReturns:str: A JSON string representing the recommended books."""parser=JsonOutputParser(pydantic\\_object=Book)question=f"Generate a random made up book on{category}with bookname, author and publisher and publishing\\_date"prompt=PromptTemplate(template="Answer the user query.\\\\n{format\\_instructions}\\\\n{query}\\\\n",input\\_variables=["query"],partial\\_variables={"format\\_instructions":parser.get\\_format\\_instructions()},)chain=prompt|llm|parserresponse=chain.invoke({"query":question})returnjson.dumps(response)@functions\\_framework.httpdefrecommended(request):request\\_json=request.get\\_json(silent=True)# Get JSON dataifrequest\\_jsonand\'category\'inrequest\\_jsonand\'number\\_of\\_book\'inrequest\\_json:category=request\\_json[\'category\']number\\_of\\_book=int(request\\_json[\'number\\_of\\_book\'])elifrequest.argsand\'category\'inrequest.argsand\'number\\_of\\_book\'inrequest.args:category=request.args.get(\'category\')number\\_of\\_book=int(request.args.get(\'number\\_of\\_book\'))else:returnjsonify({\'error\':\'Missing category or number\\_of\\_book parameters\'}),400recommendations\\_list=[]foriinrange(number\\_of\\_book)', 'doi': '', 'published_date': '2025-12-22T00:00:00+00:00', 'pdf_url': '', 'url': 'https://codelabs.developers.google.com/aidemy-multi-agent/instructions?hl=zh-cn', 'source': 'exa_context', 'categories': [], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'saved_path': '/home/qinshan/widthresearch/data/downloads/exa_在Google Cl.md'}}
2026-02-22 20:38:45,342 - __main__ - INFO - call_tool: name=tavily_download, args={'papers': [{'paper_id': '', 'title': 'LangGraph 圣经：从0到1穿透multi-agent多智能体入门实战', 'authors': [], 'abstract': '* [博客园](https://www.cnblogs.com/)\n* [首页](https://www.cnblogs.com/crazymakercircle/)\n* [新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)\n* [联系](https://msg.cnblogs.com/send/%E6%8A%80%E6%9C%AF%E8%87%AA%E7%94%B1%E5%9C%88)\n* [订阅](javascript:void(0))\n* [管理](https://i.cnblogs.com/)\n\n# [LangGraph 圣经：从0到1穿透 multi-agent多智能体 入门实战](https://www.cnblogs.com/crazymakercircle/p/19412858 "发布于 2025-12-28 21:38")\n\n# 本文 的 原文 地址\n\n#### 原始的内容，请参考 本文 的 原文 地址\n\n[本文 的 原文 地址](https://mp.weixin.qq.com/s/3Unzc4dHuYDwQyBgBaRKtA)\n\n## 尼恩：LLM大模型学习圣经PDF的起源\n\n在40岁老架构师 尼恩的**读者交流群**(50+)中，经常性的指导小伙伴们改造简历。\n\n然而，其中一个成功案例，是一个9年经验 网易的小伙伴，当时拿到了一个年薪近**80W的大模型架构offer**，逆涨50%，那是在去年2023年的 5月。\n\n* [惊天大逆袭：8年小伙20天时间提75W年薪offer，逆涨50%，秘诀在这](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247491496&idx=1&sn=cb31f7510a7c2efb7daf6cad793860ad&scene=21#wechat_redirect)\n\n不到1年，小伙伴也在团队站稳了脚跟，成为了名副其实的大模型 应用 架构师。接下来，尼恩架构团队，通过 梳理一个《LLM大模型学习圣经》 帮助更多的人做LLM架构，拿到年薪100W, 这个内容体系包括下面的内容：\n\n* [《**Python学习圣经：从0到1精通Python，打好AI基础**》](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247504585&idx=1&sn=dcc5f35e6733d36d5bb74113833d6263&scene=21#wechat_redirect)\n* [《**LLM大模型学习圣经：从0到1吃透Transformer技术底座**》](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247502020&idx=1&sn=a446e3f018639b85e49748d393e7620c&scene=21#wechat_redirect)\n\n* [**《SpringCloud + Python 混合微服务架构，打造AI分布式业务应用的技术底层》**](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247502436&idx=1&sn=cfbf4aec8188cf6025f9a7d09965b809&scene=21#wechat_redirect)\n* 《LLM 智能体 学习圣经：从0到1吃透 LLM 智能体 的架构 与实操》\n* 《LLM 智能体 学习圣经：从0到1吃透 LLM 智能体 的 中台 架构 与实操》\n* 《[**Spring 集成 DeepSeek 的 3大方法，史上最全**](https://mp.weixin.qq.com/s/wWJ8-Py3c6FyjkpiKAKihQ)》\n* 《[基于Dify +Ollama+ Qwen2 完成本地 LLM 大模型应用实战](https://mp.weixin.qq.com/s/lruBtgNRouvoJVqsmRDfIg)》\n* 《**Spring AI 学习圣经 和配套视频** 》\n* 《[AI部署架构：A100、H100、A800、H800、H20的差异以及如何选型？开发、测试、生产环境如何进行部署架构？](https://mp.weixin.qq.com/s/m8_2my55QWhzfnDs70TTUQ)》\n* [**生产环境 K8S + Deepseek 实现大模型部署 和 容器调度（图解+史上最全）**](https://mp.weixin.qq.com/s/xagt2GYz_pKHkdePBZ6Y0Q)\n* [**《最近大火的 MCP 协议，看这篇文章就够了》**](https://mp.weixin.qq.com/s/jwzEFeHuB_k9BA7go8bNVg)\n* [《美团面试：LLM 大模型会有 什么问题？说说进行 RAG 优化的方法？》](https://mp.weixin.qq.com/s/ny7Y9-tIzepfSCR-RSuwbA)\n\n## LangGraph 圣经 介绍\n\n接下来，尼恩团队开始给大家写 LangGraph 学习圣经 ，包括：\n\n* LangGraph学习圣经 （1）： 从0到1穿透 multi-agent多智能体 入门实战\n* LangGraph学习圣经 （2）： 从0到1穿透LangGraph架构与源码\n* LangGraph学习圣经（3） : 基于langgraph的java 运维多智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent 等\n\n现本文是第一篇：从0到1穿透 multi-agent多智能体 入门实战\n\n## 第一章. LangGraph介绍，从单智能体 到多智能体 的演进\n\n### 1.1 、从“一个人独干(单智能体)”到“团队协作(多智能体)”的转变\n\n而现在的趋势是：**让专业的人干专业的事**。\n\n* 研究员智能体：专门负责找资料\n* 事实核查员：专挑毛病，验证真假\n* 写作助手：专注输出流畅内容\n* 编辑：最后润色把关\n\n研究数据显示，这种分工协作的方式，在处理复杂任务时性能提升 **40%-60%**，而且更容易调试、维护和扩展。\n\n### 1.2 一个 AI研究助手 多智能体例子\n\n**(1) 拆解问题**\n\n**(2) 查资料**\n\n**(3) 验证信息真伪**\n\n**(4) 输出一份结构清晰的研究报告**\n\n整个流程由多个AI智能体接力完成，背后靠的是 **LangGraph** 这个框架来调度协调。\n\n### 1.3什么是 LangGraph？\n\n普通的 AI 智能体 应用记不住事、理不清步骤，像个没头苍蝇。\n\n**怎么办？ 用“图”来设计 AI 流程** ：\n\n\n\nLangGraph 就是 **AI 的导演**，指挥模块按剧本走。\n\n比如做个旅游助手： 问目的地 → 查天气 → 推荐穿搭 → 生成行程。\n\n\n\nLangGraph是LangChain团队推出的开源框架，专为构建**有状态、长时间运行**的AI工作流而生。\n\n它不像别的工具那样, 封装太多细节.\n\nLangGraph 给你底层控制权，像搭积木一样组装AI系统。\n\nLangGraph 核心思想很简单：用“图”来建模AI行为，节点是动作，边是跳转逻辑，状态是记忆。\n\n> LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\n\n### 图结构三要素\n\n* **节点（Nodes）**：每个节点, 是一个独立任务. 比如调用大模型、查数据库、执行工具函数。\n* **边（Edges）**：每 一 边是Python函数，根据当前状态决定下一步去哪，支持条件判断和循环。\n* **状态（State）**：全局共享的数据结构，记录所有关键信息，相当于AI的“短期记忆”。\n\n有了这图结构三要素 ， 就能画出一条条清晰的执行路径，不怕逻辑混乱，也不怕断电重启丢数据。\n\n```\n from langgraph.graph import StateGraph, END # 示例：定义一个简单图 graph = StateGraph(dict) def node_a(state): return {"value": "from A"} def node_b(state): return {"value": "from B"} graph.add_node("A", node_a) graph.add_node("B", node_b) graph.set_entry_point("A") graph.add_edge("A", "B") graph.add_edge("B", END) app = graph.compile() \n```\n\n上面这段代码就是一个最简单的流程图：A → B → 结束。\n\n**(1) 开始节点 → 由 `graph.set_entry_point("A")`设定入口**\n\n**(2) 节点A → 执行 `node_a(state)`函数，返回 `{"value": "from A"}`**\n\n**(3) 节点B → 执行 `node_b(state)`函数，返回 `{"value": "from B"}`**\n\n**(4) 结束节点 → 由 `graph.add_edge("B", END)`设定终点**\n\n### 1.4 LangGraph 的核心优势\n\nLangGraph 本质是什么？ LangGraph 其实是一个 **“带记忆的、AI 任务图 、执行器”**。\n\n更技术一点说：\n\n它是一个**有状态、可持久化、基于图的工作流引擎**，灵感来自两种老但牛的技术：\n\n* **数据流编程**（Dataflow Programming）：数据推着计算走\n* **Actor 模型**：每个节点像个独立小演员，收到消息才行动\n\n这就让它特别适合做复杂AI流程，比如：\n\n* 自动规划任务\n* 多Agent协作\n* 出错自动重试\n* 用户中途修改需求也能接得上\n\n**LangGraph 的设计哲学很简单粗暴**：\n\n> **把AI应用当成一个永远在线的“活系统”，而不是一次性的函数调用。**\n\n你可以把它想象成一个**会记住上下文、能持续反应、还会自己做决定的智能机器人**，而不是一个“问一句答一句”的问答机。\n\n#### LangChain 和 LangChain 的区别？\n\n我们先来对比一下：\n\n* **LangChain**：像是流水线工人。\n\n  原料（输入）进来 → 经过几道工序（处理）→ 成品（输出）出去 → 完事，关机器。 下次再来？从头开始。\n* **LangGraph**：像是一个值班经理，24小时在岗。\n\n  他记得昨天客户说了啥，今天问题进展到哪了，还能根据情况叫同事帮忙、重试任务、甚至主动发消息提醒你。\n\n  LangGraph核心是：**状态一直在线，流程可以反复跳转。**\n\n### 1.6 LangGraph 的关键组件\n\n**问题根源：** 多个AI 任务，各模块各干各的，逻辑乱、状态丢、没法 debug。\n\n**解决思路：** 以“状态图”为核心，靠 **节点、边、状态** 三件套，构建可控可溯的工作流。\n\n三大支柱：\n\n* **节点（Nodes）**：执行单元，比如调大模型、查库；\n* **边（Edges）**：控制流程走向，可以固定跳转，也能条件判断；\n* **状态（State）**：共享内存，全程保存数据，上下文不断。\n\n支持循环：AI 生成内容后问用户“满意吗？” 不满意就回退修改，直到通过。\n\n支持人工干预：在流程中插入“等待确认”节点，处理完再继续。\n\n集成 RAG，先检索再回答，不说胡话；\n\n用 LangSmith 监控全过程，像 DevTools 一样看执行轨迹。\n\n### 1.5 LangGraph 五大能力：\n\n**1、状态保持、持久执行（Durable Execution）**\n\n跑一半断电了怎么办？重头再来？那用户不得疯了？\n\nLangGraph把每一步状态存下来，哪怕服务挂了，重启也能接着干，就像游戏存档一样靠谱。\n\n**2、人机协同、人机协作（Human-in-the-loop）**\n\nAI再聪明也有拿不准的时候。\n\n这时候可以让人类插一脚，看看状态、改改参数、点个确认，然后再继续。\n\n这种设计特别适合审批流、客服质检这类高风险场景。\n\n**3、持久化存储、全面记忆管理（Comprehensive Memory）**\n\n有的框架只记最近几句话，LangGraph不一样，它可以同时管：\n\n* 短期记忆：本次会话的状态\n* 长期记忆：跨会话的历史数据（结合向量库或数据库）\n\n这就让AI不仅能“接话”，还能“认人”，提供个性化体验。\n\n**4、调试能力（Debugging）**\n\n复杂的AI流程就像迷宫，光看日志根本找不到问题在哪。\n\nLangGraph配合LangSmith，能生成可视化轨迹图，每一步走到哪、状态变成啥样，清清楚楚。\n\n**5、工具集成 & 多智能体支持**\n\n想让AI查天气、订机票、写报告？没问题，接API就行。\n\n而且它天生支持多个AI协同工作，比如一个负责分析，一个负责决策，一个负责汇报。\n\n## 第二章. 使用一个LangGraph 构建基础多智能体 聊天机器人\n\n核心痛点：想搭聊天机器人，但不会把大模型塞进一个能扩展的流程里，代码乱、状态散，没法往复杂应用走。\n\n核心方案：用 **LangGraph** 搭个状态驱动的流水线，把大模型调用变成简单节点，实现“输入→处理→输出”的清晰控制流。\n\n## 2.1 环境准备\n\n做AI项目，第一步就是选个靠谱的大模型。\n\n别一上来就烧钱，咱们先试试免费的路子。\n\n百度千帆和硅基流动这两个平台，都有不错的国产模型可以白嫖。\n\n**百度千帆调用**\n\n这是目前demo里用的方式，靠 `QianfanChatEndpoint` 接入ERNIE-Speed-128K模型：\n\n```\n import os from langchain_community.chat_models import QianfanChatEndpoint llm = QianfanChatEndpoint( model="ERNIE-Speed-128K", streaming=True, # 启用流式输出 api_key=os.getenv(\'QIANFAN_AK\', \'\'), secret_key=os.getenv(\'QIANFAN_SK\', \'\') ) \n```\n\n注册个账号，拿AK/SK密钥就能跑起来，适合新手练手。\n\n**硅基流动调用方式**\n\n另一个选择是硅基流动，它支持GLM、Qwen这些热门开源模型：\n\n```\n from langchain_openai import ChatOpenAI llm = ChatOpenAI( model="THUDM/glm-4-9b-chat", streaming=False, api_key=os.getenv(\'SILICONFLOW_API_KEY\', \'\'), base_url=os.getenv(\'SILICONFLOW_BASE_URL\', \'\'), temperature=0.1, ) \n```\n\n换个base\\_url，就能当OpenAI用，兼容性好得很，迁移成本低。\n\n**Deepseek 调用方式**\n\nDeepseek 支持多款开源大模型（如 Deepseek-R1、Deepseek-Coder 等），且提供兼容 OpenAI API 格式的调用方式，迁移成本低，直接复用 `ChatOpenAI` 即可快速接入：\n\n```\n from langchain_openai import ChatOpenAI import os # Deepseek 调用（兼容 OpenAI 接口格式） llm = ChatOpenAI( model="deepseek-chat", # 可选模型：deepseek-chat（通用）、deepseek-coder-v2（编程）等 streaming=True, # 支持流式输出，按需开启 api_key=os.getenv(\'DEEPSEEK_API_KEY\', \'\'), # 从环境变量读取密钥（推荐） base_url="https://api.deepseek.com/v1", # Deepseek 官方 API 基础地址 temperature=0.7, # 随机性调节：0~1，值越低输出越确定 max_tokens=4096 # 单次生成最大令牌数（按需调整，不同模型上限不同） ) \n```\n\n**前置准备**：注册 Deepseek 账号（官网：[https://www.deepseek.com/），进入「API](https://www.deepseek.com/%EF%BC%89%EF%BC%8C%E8%BF%9B%E5%85%A5%E3%80%8CAPI) 密钥管理」获取 `DEEPSEEK_API_KEY`，建议将密钥存入环境变量（避免硬编码），如：\n\n```\n # Linux/Mac export DEEPSEEK_API_KEY="你的密钥" # Windows（命令行） set DEEPSEEK_API_KEY="你的密钥" \n```\n\n痛点：API 密钥写死在代码里？不安全！依赖安装慢还冲突？新手直接卡住。\n\n解决方案：用 `uv` 快速装包，`.env` 文件管密钥，环境干净又安全，一次配好反复用。\n\n```\n # 安装 LangGraph 和周边依赖 uv pip install -U langgraph langchain python-dotenv typing-extensions \n```\n\n```\n # .env 文件存密钥 DEEPSEEK_API_KEY=your_deepseek_api_key_here \n```\n\n自动加载配置，密钥不进代码，不怕泄露。\n\n## 2.2 实现基础聊天机器人\n\n痛点：直接调大模型？**没流程、没记忆，对话像复读机**，体验差。\n\n解决方案：用 LangGraph 维护消息列表作为状态，串起“用户输入 → 模型回复” 的复杂路线，让对话有上下文、能连贯。\n\nLangGraph的核心，就是把业务逻辑画成一张“流程图”。\n\n每个节点干一件事，边来决定执行顺序。\n\n就像工厂流水线，零件从这头进，那头出成品。\n\n```\n from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START from langgraph.graph.message import add_messages import os from dotenv import load_dotenv # 加载.env文件中的环境变量 load_dotenv() class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model( "deepseek-chat", # 使用DeepSeek模型 api_key=os.environ.get("DEEPSEEK_API_KEY") ) def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except KeyboardInterrupt: print("\\nGoodbye!") break \n```\n\n> 一句话概括：定义状态 → 创建图 → 添加节点 → 编译运行 → 流式输出。\n\n核心的代码如下\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() \n```\n\n## 2.3 基础聊天机器人 代码解析\n\n痛点：LangGraph 组件太多，看不懂谁干啥，学得迷糊。\n\n解决方案：盯死两个核心：**状态累积** 和 **流程编排**，其他都是配角。\n\n### 1. 引入依赖\n\n导包是第一步，为状态、模型、流程打基础。\n\n### 2. 加载环境变量\n\n```\n load_dotenv() \n```\n\n从 `.env` 读密钥，绝不硬编码。\n\n### 3. 定义状态 结构\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n所有消息存在一个可累加的 list 里，新消息自动追加，上下文不断。\n\n这行代码是 **LangGraph 中定义「可自动累加对话状态」的核心语法**。专门解决之前提到的「对话历史持久化」问题 。\n\n这行代码， 让 `messages` 字段能自动追加新消息（用户提问、AI 回复），而不是被覆盖。\n\n在 LangGraph 流程中，`State` 是流转的数据载体，而 `messages` 是存储「对话历史」的关键字段（比如 `[{"role": "user", "content": "你好"}, {"role": "assistant", "content": "你好！"}]`）。\n\n这行代码的核心目的是：**让每次节点执行后，新生成的消息（如 AI 回复）自动追加到 `messages` 列表中，而非替换原有历史**，确保流程能基于完整上下文推进。\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n**1、`class State(TypedDict)`：定义强类型的状态字典**\n\n* `TypedDict`（类型字典）：是 Python 标准库 `typing` 中的工具，用于定义「键名固定、值类型明确」的字典。作用：约束 `State` 的结构（必须有 `messages` 字段，且类型是 `list`），避免写代码时出现键名错误（如把 `messages` 写成 `message`），同时让 IDE 提供类型提示。\n* 对比普通字典：如果直接用 `dict` 定义状态，无法约束字段，而 `TypedDict` 让状态结构「可预期、可校验」，是 LangGraph 推荐的状态定义方式。\n\n  示例：符合 `State` 类型的合法数据\n\n  ```\n   valid_state = { "messages": [ {"role": "user", "content": "介绍 LangGraph"}, {"role": "assistant", "content": "LangGraph 是流程编排工具"} ] } \n  ```\n\n2、messages: Annotated[list, add\\_messages] ：给 messages 字段加「自动追加规则」 add\\_messages。\n\n这是关键中的关键，`Annotated` + `add_messages` 共同实现「对话历史自动累加」。\n\n（1）`Annotated[类型, 元数据]`：Annotated给字段附加额外规则，这是 Python 3.9+ 引入的标准库工具（从typing导入），作用是「给类型添加元数据 / 规则」。\n\n格式：\n\n```\n Annotated[基础类型, 规则1, 规则2, ...] \n```\n\n这里的「规则」会被 LangGraph 识别并生效。\n\n这里的 `Annotated[list, add_messages]` 表示：`messages` 字段的基础类型是 `list`（存储对话消息列表），同时附加 `add_messages` 这个「特殊规则」。\n\n（2）`add_messages`：LangGraph 定义的一个函数，是一个自定义「消息追加处理器」\n\n* `add_messages` 是从 `langgraph.graph` 导入的核心工具（需显式导入：`from langgraph.graph import add_messages`），核心代码参考下面。\n* add\\_messages 核心作用：自动合并「节点返回的新消息」和「原有状态中的旧消息」，具体逻辑是：\n  1. 假设原有状态的 `messages` 是：`[用户提问1]`\n  2. 节点函数（如之前的 `chatbot`）返回：`{"messages": [AI 回复1]}`\n  3. `add_messages` 会自动将两者合并为：`[用户提问1, AI 回复1]`，并更新到新状态中\n* 对比没有 `add_messages` 的情况：如果直接定义 `messages: list`，节点返回的新消息会「覆盖」原有 `messages`（原有对话历史丢失）。\n\nLangGraph 的 `add_messages` 源码（简化后）如下 ：\n\n```\n from typing import Any, List, Union def add_messages( existing: Union[List[Any], None], # 状态中已有的旧消息（可能为 None） updates: Union[List[Any], Any, None] # 节点返回的新消息（可能是列表/单条/None） ) -> List[Any]: """ 合并新旧消息：旧消息列表 + 新消息（自动处理单条/列表格式） - 若旧消息不存在，直接返回新消息列表 - 若新消息是单条，自动转为列表后追加 - 若新消息为 None，返回原旧消息列表 """ # 初始化旧消息列表（避免 None 报错） current = existing.copy() if existing is not None else [] # 处理新消息：统一转为列表格式 if updates is None: new_messages = [] elif isinstance(updates, list): new_messages = updates else: new_messages = [updates] # 单条消息 → 列表 # 合并：旧消息 + 新消息（保证顺序，不覆盖） return current + new_messages \n```\n\n如果没有 `add_messages` 规则：\n\n* 初始状态：`{"messages": [用户提问]}`\n* 节点执行后返回：`{"messages": [AI 回复]}`\n* 最终状态：`{"messages": [AI 回复]}`（用户提问被覆盖，丢失历史）\n\n有了 `add_messages` 规则：\n\n* 初始状态：`{"messages": [用户提问]}`\n* 节点返回：`{"messages": [AI 回复]}`\n* `add_messages` 自动合并：`[用户提问] + [AI 回复] = [用户提问, AI 回复]`\n* 最终状态：`{"messages": [用户提问, AI 回复]}`（历史保留，后续节点可复用上下文）\n\n这行代码的本质是：**用 `TypedDict` 约束状态结构，用 `Annotated + add_messages` 给 `messages` 字段绑定「自动追加」规则**。\n\n最终实现「对话历史不丢失、上下文可复用」—— 这正是 LangGraph 能支持多轮对话、复杂流程的基础， 这就是提到的「不怕断电重启丢数据」（状态可持久化，历史都在 `messages` 中）。\n\n### 状态结构的本质\n\n传统流程像函数链：前一步输出传给下一步，**每步只能看到局部数据**。\n\n结果？上下文断裂、逻辑难控、调试抓狂。\n\n#### 核心解法\n\nLangGraph 用一个**全局共享状态**，所有节点操作同一份数据。\n\n就像工厂流水线：半成品从一站传到下一站，每一站都加工它、更新它。\n\n```\n # 错误理解：以为是函数链 def workflow(): result1 = step1() result2 = step2(result1) return result3(result2) # 正确理解：是状态图 state = {"data": None} state = node1(state) # 每个节点都接收并返回完整状态 state = node2(state) state = node3(state) \n```\n\n> 状态 = 公共记事本，所有人一起看、一起改。\n\n> 不是每人一张草稿纸，最后对不上账。\n\n#### 关键点\n\n* **节点不是函数，是状态转换器**：读当前状态 → 加工 → 返回新状态。\n* **每一步都在提交快照**：像 Git commit，留下完整的系统瞬间。\n* **状态即上下文中枢**：所有节点共用一份数据源，不丢信息，不错乱序。\n\nLangGraph 的本质：**带记忆的自动化流水线**。 节点干活，状态流动，全程不断电。\n\n### 4. 创建图构建器\n\n```\n graph_builder = StateGraph(State) \n```\n\n初始化一个共享状态的流程图，所有节点都能看到最新消息。\n\n这行代码是 **LangGraph 流程图的 “地基构建”**： 创建一个「绑定了状态结构」的流程图构建器。\n\n核心作用是：明确流程图中流转的「数据格式（State）」，让后续节点、边的定义都遵循这个格式，避免数据混乱。\n\n先回顾代码上下文（结合之前的 `State` 定义）：\n\n```\n # 1. 先定义状态结构（约束数据格式） class State(TypedDict): messages: Annotated[list, add_messages] # 2. 创建流程图构建器（绑定状态） graph_builder = StateGraph(State) \n```\n\n逐部分解释：\n\n**（1）、`StateGraph`**：\n\n是 LangGraph 的核心类（从 `langgraph.graph` 导入），本质是「流程图构建工具」，负责管理「节点（处理步骤）」和「边（流转规则）」。\n\n可以把它理解为「一张空白的画布」，后续的 `add_node`（加节点）、`add_edge`（加流转）都是在这张画布上画画。\n\n**（2）、`State`**：\n\n是我们之前定义的「强类型状态结构」（基于 `TypedDict`），明确了流程图中流转的数据必须包含 `messages` 字段，且格式是「带 `add_messages` 规则的列表」。\n\n这里作为参数传入 `StateGraph`，表示「这张流程图的所有数据流转，都必须遵循 `State` 定义的格式」。\n\n**（3）、`graph_builder`**：\n\n是 `StateGraph` 类的实例（即 “构建好的空白画布”），后续所有流程图操作（加节点、加边、编译）都通过这个实例完成。\n\n### 5. 初始化大模型\n\n```\n llm = init_chat_model("deepseek-chat", api_key=os.environ.get("DEEPSEEK_API_KEY")) \n```\n\n接入 DeepSeek 当回答引擎，听你指挥。\n\n### 6. 编写节点逻辑\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} \n```\n\n输入当前消息，让模型生成回复，返回新消息对象。\n\n### 7. 搭建流程图\n\n```\n graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() \n```\n\n注册节点，连上起点，形成一条直线流程：问了就答。\n\n这些代码是 **LangGraph 构建对话流程图的核心逻辑**.\n\n核心作用是：定义一个「对话节点」，让流程从 “开始” 直接进入该节点，最终形成一个「 任务流程」（启动 → 对话 → 结束）。下面逐行拆解，结合 LangGraph 核心概念和实际作用讲清楚：\n\n先明确两个前提\n\n**(1) State（状态）：LangGraph 中流转的数据载体（类似 “流水线的物料”），这里的 `State` 是一个自定义数据结构（通常是字典或 Pydantic 模型），核心字段 `messages` 存储对话历史（比如用户提问、AI 回复）。**\n\n**(2) llm：之前定义的大语言模型实例（百度千帆 / 硅基流动 / Deepseek 等），负责接收对话历史并生成 AI 回复。**\n\n逐行代码解释\n\n#### (1). 定义对话节点函数 `chatbot`\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} \n```\n\n这是 LangGraph 的「节点函数」（节点是流程中的 “处理步骤”）.\n\n作用是：**接收当前对话状态，调用 LLM 生成回复，更新状态并返回**。\n\n函数参数 `state: State`：\n\n* 接收流程中传递过来的「当前状态」，里面包含 `state["messages"]`（对话历史列表，比如 `[{"role": "user", "content": "你好"}]`）。\n* 类型注解 `State` 是自定义的状态结构（确保代码规范，避免键名错误）。\n\n核心逻辑 `llm.invoke(state["messages"])`：\n\n* 调用 LLM 模型，传入完整的对话历史 `state["messages"]`（模型需要基于上下文生成回复，而不只是单轮提问）。\n* `invoke` 是同步调用方法（如果之前开启了 `streaming=True`，这里会返回流式迭代器，需配合处理）。\n* 模型返回结果是一个「AI 回复消息对象」（比如 `{"role": "assistant", "content": "你好！有什么可以帮你？"}`）。\n\n返回值 `{"messages": [llm.invoke(...)]}`：\n\n* LangGraph 要求节点函数返回「状态更新数据」（只需要返回要修改的字段，不需要全量状态）。\n* 这里表示：将 LLM 生成的 AI 回复，添加到 `messages` 字段中（覆盖旧的 `messages`？不 ——LangGraph 会自动合并：旧 `messages` + 新 AI 回复 = 新 `messages`，具体看状态定义是否允许列表追加，默认字典结构会覆盖，实际工程中会用 `list` 追加，比如 `{"messages": state["messages"] + [llm.invoke(...)]}`，这里 demo 简化了）。\n\n#### （2）. 向流程图添加「chatbot 节点」\n\n```\n graph_builder.add_node("chatbot", chatbot) \n```\n\n* `graph_builder` 是 `StateGraph` 的实例（流程图构建器），负责管理节点和节点间的流转。\n* add\\_node方法：注册一个节点到流程图中。\n  + 第一个参数 `"chatbot"`：节点的「唯一标识名」（后续用这个名字指定流转关系）。\n  + 第二个参数 `chatbot`：节点对应的「处理函数」（即上面定义的 `chatbot` 函数，节点被触发时会执行该函数）。\n\n#### （3）. 定义流程的「入口节点」和「流转关系」\n\n`START` 是 LangGraph 内置的「起始节点」（流程的入口，类似流程图的 “开始” 符号）。\n\n* `add_edge(START, "chatbot")`：定义一条流转规则：**流程从 START 开始后，直接进入名为 "chatbot" 的节点**。\n* 这是最简单的流转关系（无分支、无条件），是线性流程的核心。\n\n#### （4）. 编译流程图为可执行应用\n\n```\n graph = graph_builder.compile() \n```\n\n* `compile()` 是构建器的核心方法，作用是：将前面定义的「节点」和「流转规则」，编译成一个「可执行的流程图应用」（`graph` 是编译后的实例）。\n* 编译后，graph具备了「运行流程」的能力，后续可以通过graph.invoke(初始状态)启动流程，比如：\n\n  ```\n   # 启动流程：传入初始对话状态（用户的第一个提问） result = graph.invoke({"messages": [{"role": "user", "content": "介绍一下 LangGraph"}]}) # 输出最终状态中的 messages（包含用户提问 + AI 回复） print(result["messages"]) \n  ```\n\n整个流程的执行逻辑（一句话总结）\n\n**(1) 调用 `graph.invoke(初始状态)` 启动流程；**\n\n**(2) 流程从 `START` 节点出发，根据 `add_edge` 规则，进入 `chatbot` 节点；**\n\n**(3) 执行 `chatbot` 函数：读取初始状态中的用户提问 → 调用 LLM 生成回复 → 更新 `messages` 字段；**\n\n**(4) 由于没有定义 `chatbot` 节点之后的流转关系（比如 `add_edge("chatbot", END)`），demo 中流程会在 `chatbot` 节点执行完成后自动结束。实际工程中，会显式指定 `add_edge("chatbot", END)` 或其他节点 ；**\n\n**(5) 返回最终的状态（包含用户提问和 AI 回复的完整对话历史）。**\n\n核心特点\n\n* 「清晰执行路径」：START → chatbot → 结束，无分支、无逻辑混乱；\n* 「状态可追溯」：所有对话数据都存在 `state["messages"]` 中，即使中断 / 重启，只要恢复 `state` 就能继续流程（不会丢数据）；\n* 「可扩展」：后续可以添加更多节点（比如 “意图识别”“工具调用”“记忆管理”），通过 `add_edge` 定义复杂流转（比如条件分支、循环），而不需要重构核心逻辑。\n\n### 8. 流式输出响应\n\n```\n def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) \n```\n\n回复边生成边打印，像真人打字，体验丝滑。\n\n这行代码是 **LangGraph 流式输出对话结果的核心函数**，核心作用是：接收用户输入，启动流程图的「流式执行」，实时捕获 AI 回复并逐段打印（而非等待完整回复生成后再输出）。\n\n先明确两个前提\n\n**(1) 流式输出（streaming）：LLM 生成回复时，不是一次性返回完整内容，而是「逐句 / 逐段推送」（类似 ChatGPT 的打字机效果），减少用户等待感。**\n\n**(2) graph.stream()：LangGraph 编译后的 `graph` 实例的流式执行方法，与 `graph.invoke()`（同步阻塞，等待完整结果）相对，返回的是「事件迭代器」，实时输出流程执行中的状态更新。**\n\n非流式版本的等效代码（对比参考）：\n\n```\n def non_stream_graph_updates(user_input: str): # 同步阻塞，等待完整结果 result = graph.invoke({"messages": [{"role": "user", "content": user_input}]}) # 打印完整回复 print("Assistant:", result["messages"][-1].content) \n```\n\n再回到代码\n\n```\n def stream_graph_updates(user_input: str): # 1. 启动流程图的流式执行，传入初始状态 for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): # 2. 遍历事件中的所有节点输出（当前流程只有 chatbot 节点，所以只有一个 value） for value in event.values(): # 3. 提取并打印 AI 最新回复 print("Assistant:", value["messages"][-1].content) \n```\n\n**（1） 函数定义：`def stream_graph_updates(user_input: str):`**\n\n函数名：`stream_graph_updates` → 语义明确：流式获取流程图的更新（即 AI 回复）。\n\n参数 `user_input: str`：接收用户的提问（字符串类型），比如用户输入 `"介绍一下 LangGraph"`。\n\n**（2\\_) 核心：`graph.stream(初始状态)` → 启动流式执行**\n\n```\n for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): \n```\n\n`graph.stream(...)`：\n\n* 作用：启动流程图的「流式执行」，返回一个「事件迭代器」（`event` 是迭代器的每一个元素）。\n* 传入的初始状态：`{"messages": [{"role": "user", "content": user_input}]}` → 把用户输入包装成 LangChain/LLM 通用的「消息格式」（`role` 表示角色，`content` 表示内容），作为流程的起始数据。\n* 流式特性：执行后不会阻塞等待完整结果，而是每生成一段 AI 回复，就产生一个 `event`，迭代器会实时返回该事件。\n\n`for event in ...`：循环迭代流式事件.\n\n每一个 `event` 对应「流程图中某个节点的一次状态更新」（当前流程只有 `chatbot` 节点，所以所有 `event` 都来自 `chatbot` 节点的流式输出）。\n\n**(3) 解析事件：`for value in event.values()`**\n\nfor value in event.values():event的结构：LangGraph 的流式事件是一个「字典」，键是「节点名称」，值是「该节点的最新状态」。\n\n* 比如当前流程中，`event` 的结构是：`{"chatbot": {"messages": [用户输入, AI 回复片段1]}}`（后续事件会是 `{"chatbot": {"messages": [用户输入, AI 回复片段1, 回复片段2]}}` 等）。\n* `event.values()`：获取所有节点的最新状态（当前只有 `chatbot` 节点，所以 `values()` 只有一个元素）。\n* 循环的目的：兼容多节点流程（比如后续加了「工具调用节点」，`event` 可能包含多个节点的状态，这里统一遍历解析）。\n\n**(4) 提取并打印最新回复：`value["messages"][-1].content`**\n\n```\n print("Assistant:", value["messages"][-1].content) \n```\n\n* `value`：即 `chatbot` 节点的最新状态（符合之前定义的 `State` 格式，包含 `messages` 字段）。\n* `value["messages"]`：当前完整的对话历史（用户输入 + 已生成的 AI 回复片段）。\n* `[-1]`：取列表的「最后一个元素」→ 也就是 LLM 刚刚流式推送的「最新回复片段」（因为 `add_messages` 规则，`messages` 是不断追加的，最后一个元素永远是最新的）。\n* `.content`：提取消息的内容（消息对象的结构是 `{"role": "assistant", "content": "回复内容"}`，`.content` 直接获取文本）。\n* 最终效果：每收到一个 AI 回复片段，就打印一次，呈现「打字机式」的实时输出。\n\n**关键细节：为什么能实现 “实时输出”？**\n\n**(1) 依赖 LLM 的 `streaming=True`：之前定义 LLM 时开启了 `streaming=True`（比如 Deepseek / 百度千帆的调用代码），LLM 才会支持流式推送回复片段。**\n\n**(2) 依赖 `graph.stream()`：LangGraph 会把 LLM 的流式输出「封装成事件迭代器」，确保每一个回复片段都能被实时捕获。**\n\n**(3) 依赖 `messages` 的追加规则：`add_messages` 确保每一个新的回复片段都追加到 `messages` 列表末尾，通过 `[-1]` 能精准取到最新片段。**\n\n对比：流式输出 vs 非流式输出\n\n为了更直观，对比 `graph.stream()`（流式）和 `graph.invoke()`（非流式）的差异：\n\n| 方式 | 核心函数 | 执行效果 | 适用场景 |\n| --- | --- | --- | --- |\n| 流式输出 | `graph.stream()` | 逐段实时打印，类似打字机 | 交互类场景（如聊天机器人），提升用户体验 |\n| 非流式输出 | `graph.invoke()` | 等待完整回复生成后一次性打印 | 批量处理（如批量生成报告），无需实时交互 |\n\n### 9. 主循环交互\n\n```\n while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except KeyboardInterrupt: print("\\nGoodbye!") break \n```\n\n> 命令行入口，支持持续对话，按 q 或 Ctrl+C 优雅退出。\n\n这行代码是 **对话机器人的「交互循环入口」**，核心作用是：创建一个「持续运行的命令行交互环境」，让用户能反复输入提问、接收 AI 流式回复，同时支持正常退出和异常中断，是连接用户输入与 LangGraph 流程的 “桥梁”。下面逐行拆解逻辑、循环原理和边界处理：\n\n这行代码的本质是：**搭建一个「持续运行、支持多轮交互、优雅退出」的命令行对话入口**，将用户的手动输入转化为 LangGraph 流程的触发信号，最终实现 “用户提问 → AI 实时回复” 的完整交互闭环，是对话机器人从 “代码片段” 变成 “可使用工具” 的关键一步。\n\n## 2.4 运行聊天机器人\n\n痛点：跑完不知道成没成功？一脸懵。\n\n解决方案：运行脚本，输入问题，看到流式回复——就成了！\n\n```\n uv run 1-build-basic-chatbot.py \n```\n\n示例输出：\n\n```\n User: who are you? Assistant: I’m DeepSeek Chat, your AI assistant created by **DeepSeek**! \n```\n\n大模型已通，基本对话能力到手。\n\n### 小结\n\n痛点：教程太碎，学完还是不会搭真正的 AI 应用。\n\n解决方案：记住三个词：**状态、节点、边** —— 这就是 LangGraph 的骨架。\n\n这个机器人虽小，五脏俱全：\n\n* **状态**：存上下文（比如聊天记录）\n* **节点**：干活的（比如调模型）\n* **边**：定流程（下一步去哪）\n\n往后要加记忆、分支、工具调用？直接往上堆就行。\n\n## 第三章. 使用 Langgraph 完成工具调用\n\n我们之前用 LangGraph 做了个聊天机器人，但它只能“空想”，没法查天气、搜资料、算数学。\n\n现在要让它**能思考，也能动手**——比如你问“明天天气咋样？”，它会主动去“查一下”再告诉你。\n\n想让大 模型 查新鲜事、找实时数据？得给它接个“外挂大脑”。这外挂，就是工具（Tools）。 接上了，大模型就能上网搜、调接口、拿最新信息，不再靠死记硬背过日子。\n\n这就叫 **工具调用（Tool Calling）**。\n\n先给出完整可运行代码\n\n```\n # 1. 基础依赖导入 import asyncio from typing import Literal, List from langchain_core.tools import tool from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage from langchain_core.utils.function_calling import convert_to_openai_function from langgraph.graph import StateGraph, END, MessagesState # MessagesState是LangGraph内置的消息状态类 from langgraph.prebuilt import ToolNode # 导入LLM（以Deepseek为例，可替换为百度千帆/硅基流动等，用法一致） from langchain_openai import ChatOpenAI import os # 2. 初始化LLM（需提前配置环境变量 DEEPSEEK_API_KEY） llm = ChatOpenAI( model="deepseek-chat", api_key=os.getenv("DEEPSEEK_API_KEY", ""), base_url="https://api.deepseek.com/v1", streaming=True, # 支持流式输出 temperature=0.3 # 降低随机性，让工具调用更稳定 ) # 3. 定义工具：用@tool装饰器将普通函数转为AI可识别的工具 @tool # 核心装饰器：自动生成工具描述，供LLM判断是否调用 def get_weather(query: str) -> List[str]: """ 用于获取指定地区、指定时间的天气信息（工具描述很重要！LLM靠这个判断是否调用） :param query: 查询条件，格式示例："北京 今明两天天气"、"上海 明天是否下雨" :return: 天气信息列表，包含每天的天气描述 """ # 这里是模拟工具返回（真实场景替换为调用天气API，如高德/百度天气接口） if "今明两天" in query or "今天" in query or "明天" in query: return ["今天天气晴朗，温度20~28℃，微风", "明天天气多云转晴，温度22~30℃，南风3级"] elif "后天" in query: return ["后天天气小雨，温度18~25℃，东北风2级"] else: return [f"已查询到：{query} 的天气为晴朗，温度20~28℃（模拟数据）"] # 工具列表：可添加多个工具（如搜索、计算等） tools = [get_weather] # 4. 绑定工具到LLM：告诉模型"你现在拥有这些工具" # bind_tools：LangChain的工具绑定方法，让LLM能识别工具并生成工具调用指令 llm_with_tools = llm.bind_tools(tools) # convert_to_openai_function：将工具转为OpenAI格式的函数描述（兼容多数LLM） functions = [convert_to_openai_function(tool) for tool in tools] # 5. 定义对话节点（chat_bot）：让模型判断"要不要调工具" async def chat_bot(state: MessagesState): """ 核心思考节点：接收对话状态，让LLM判断是否调用工具，或直接生成回复 :param state: 流程状态，包含messages（对话历史） :return: 更新后的状态（添加LLM的思考结果） """ # 从状态中获取完整对话历史 messages = state["messages"] # 调用绑定工具的LLM，关键参数说明： # ainvoke：异步调用（适配流式和异步流程） # functions：工具的OpenAI格式描述，供LLM参考 # function_call="auto"：让LLM自主决定：直接回复 或 调用工具 response = await llm_with_tools.ainvoke( messages, functions=functions, function_call="auto" ) # 返回更新后的状态：将LLM的响应（思考结果）添加到对话历史 return {"messages": [response]} # 6. 定义路由节点（tool_router）：判断"下一步去哪"（分流逻辑） def tool_router(state: MessagesState) -> Literal["tools", "__end__"]: """ 流程路由（类似交通交警）：根据LLM的输出，决定下一个节点 :param state: 流程状态 :return: 下一个节点名称（"tools" 或 END） """ # 获取最新一条消息（LLM的思考结果） messages = state["messages"] last_message = messages[-1] # 关键判断：如果最新消息包含tool_calls（工具调用指令），则跳转到工具节点 if last_message.tool_calls: return "tools" # 去工具节点执行工具调用 return END # 没有工具调用，直接结束流程 # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) # 8. 编排完整流程（状态图）：将节点和路由串成流水线 # StateGraph(MessagesState)：创建状态图，指定状态格式为MessagesState（内置的消息列表结构） workflow = StateGraph(MessagesState) # 8.1 添加节点 workflow.add_node("chat_bot", chat_bot) # 思考节点：判断是否调工具 workflow.add_node("tools", tool_node) # 工具节点：执行工具调用 # 8.2 设置流程入口：流程从chat_bot节点开始（用户提问后先让模型思考） workflow.set_entry_point("chat_bot") # 8.3 定义节点流转规则 # 工具节点执行完成后，返回chat_bot节点：让模型基于工具结果生成最终回复 workflow.add_edge("tools", "chat_bot") # 条件流转：从chat_bot节点出发，由tool_router决定下一步 workflow.add_conditional_edges( source="chat_bot", # 起点：chat_bot节点 condition=tool_router, # 条件判断函数：tool_router # 无需手动映射目标节点（tool_router直接返回节点名） ) # 8.4 编译流程：生成可执行的流程图应用 app_graph = workflow.compile() # 9. 测试：流式运行流程（实时查看每一步输出） async def run_streaming_demo(): """流式运行工具调用流程，模拟用户交互""" # 初始对话状态：系统提示 + 用户提问 initial_messages = [ SystemMessage(content="你是一个智能助手，能回答问题和调用工具。" "如果需要查询天气，请调用get_weather工具，不要瞎编数据。"), HumanMessage(content="帮我查一下深圳今明两天的天气，谢谢～") ] initial_state = {"messages": initial_messages} print("=== 工具调用流程启动（流式输出）===") print(f"User: {initial_messages[-1].content}") print("Assistant: ", end="", flush=True) # 异步流式迭代流程输出（stream_mode=\'messages\'：按消息粒度输出，便于调试） async for event in app_graph.astream(initial_state, stream_mode=\'messages\'): # 解析事件：LangGraph的流式事件是元组，第一个元素是消息块 if isinstance(event, tuple): chunk = event[0] # 只打印AI的回复内容（过滤工具调用、系统消息等中间过程） if isinstance(chunk, HumanMessage): continue # 跳过用户消息 elif isinstance(chunk, ToolMessage): # 工具调用结果（用户不可见，可选打印用于调试） print(f"\\n[调试] 工具返回结果：{chunk.content}", flush=True) elif chunk.type == \'AIMessageChunk\': # 流式输出AI的最终回复（打字机效果） print(chunk.content, end="", flush=True) print("\\n=== 流程结束 ===") # 10. 运行测试（异步函数需用asyncio.run启动） if __name__ == "__main__": asyncio.run(run_streaming_demo()) \n```\n\n接下来，开始代码核心组件详解（按执行顺序）\n\n## 3.1、先搞个“假”天气工具：让AI知道它能干啥\n\n**痛点：**模型不知道自己有啥技能，就像助理没说明书，啥也不会干。\n\n**方案：**用 `@tool` 给函数打标签，变成 AI 能识别的“工具”。\n\n```\n # 3. 定义工具：用@tool装饰器将普通函数转为AI可识别的工具 @tool # 核心装饰器：自动生成工具描述，供LLM判断是否调用 def get_weather(query: str) -> List[str]: """ 用于获取指定地区、指定时间的天气信息（工具描述很重要！LLM靠这个判断是否调用） :param query: 查询条件，格式示例："北京 今明两天天气"、"上海 明天是否下雨" :return: 天气信息列表，包含每天的天气描述 """ # 这里是模拟工具返回（真实场景替换为调用天气API，如高德/百度天气接口） if "今明两天" in query or "今天" in query or "明天" in query: return ["今天天气晴朗，温度20~28℃，微风", "明天天气多云转晴，温度22~30℃，南风3级"] elif "后天" in query: return ["后天天气小雨，温度18~25℃，东北风2级"] else: return [f"已查询到：{query} 的天气为晴朗，温度20~28℃（模拟数据）"] # 工具列表：可添加多个工具（如搜索、计算等） tools = [get_weather] # 4. 绑定工具到LLM：告诉模型"你现在拥有这些工具" # bind_tools：LangChain的工具绑定方法，让LLM能识别工具并生成工具调用指令 llm_with_tools = llm.bind_tools(tools) # convert_to_openai_function：将工具转为OpenAI格式的函数描述（兼容多数LLM） functions = [convert_to_openai_function(tool) for tool in tools] # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) \n```\n\n* `@tool`：把普通函数包装成 AI 工具\n* `bind_tools`：告诉模型“你现在会这些技能”\n* `ToolNode`：负责真正执行工具调用，相当于给机器人配了“工具箱 + 操作员”\n\n**LLM 工具绑定（`bind_tools`）**\n\n* **`llm.bind_tools(tools)`**：给 LLM “挂载” 工具，让模型知道自己拥有这些能力，能生成符合工具调用格式的指令（如`tool_calls: [{"name": "get_weather", "args": {"query": "深圳今明两天天气"}}]`）\n* **`convert_to_openai_function`**：将工具转为 OpenAI 标准的函数描述格式，兼容绝大多数支持工具调用的 LLM（避免因模型格式差异导致调用失败）\n\n**创建 工具执行节点 ToolNode**\n\nToolNode 角色 是 流程的 “手脚”，负责执行工具调用的全流程（无需手动写解析逻辑）\n\nToolNode 内置逻辑：\n\n**(1) 解析 LLM 的`tool_calls`指令（提取工具名、参数）**\n\n**(2) 调用对应的工具函数（如`get_weather("深圳今明两天天气")`）**\n\n**(3) 将工具返回结果包装为`ToolMessage`（LangChain 的工具消息类型）**\n\n**(4) 自动将`ToolMessage`添加到对话历史，供后续节点使用**\n\n**ToolNode 优势**：无需关心工具调用的细节（如参数解析、异常处理），LangGraph 已封装好\n\n## 3.2、改造 chat\\_bot：让模型学会“要不要动手”\n\n**痛点：**模型只会硬答，不会判断“这事我能不能办”，要么瞎编，要么说“我不知道”。\n\n**方案：**让它输出“我要调哪个工具”，而不是直接回复。\n\n```\n # 5. 定义对话节点（chat_bot）：让模型判断"要不要调工具" async def chat_bot(state: MessagesState): """ 核心思考节点：接收对话状态，让LLM判断是否调用工具，或直接生成回复 :param state: 流程状态，包含messages（对话历史） :return: 更新后的状态（添加LLM的思考结果） """ # 从状态中获取完整对话历史 messages = state["messages"] # 调用绑定工具的LLM，关键参数说明： # ainvoke：异步调用（适配流式和异步流程） # functions：工具的OpenAI格式描述，供LLM参考 # function_call="auto"：让LLM自主决定：直接回复 或 调用工具 response = await llm_with_tools.ainvoke( messages, functions=functions, function_call="auto" ) # 返回更新后的状态：将LLM的响应（思考结果）添加到对话历史 return {"messages": [response]} \n```\n\n关键是 `function_call="auto"`：让模型自己决定是否调工具\n\n它不再嘴硬，而是说：“我要调 `get_weather`，参数是‘今明两天天气’”\n\n## 3.3、加个 add\\_conditional\\_edges 条件边，实现节点路由\n\n**痛点：**模型有时直接回，有时要调工具，系统懵了：接下来该干啥？\n\n**方案：**加个判断器，看消息里有没有 `tool_calls`，有就调工具，没有就结束。\n\n```\n # 6. 定义路由节点（tool_router）：判断"下一步去哪"（分流逻辑） def tool_router(state: MessagesState) -> Literal["tools", "__end__"]: """ 流程路由（类似交通交警）：根据LLM的输出，决定下一个节点 :param state: 流程状态 :return: 下一个节点名称（"tools" 或 END） """ # 获取最新一条消息（LLM的思考结果） messages = state["messages"] last_message = messages[-1] # 关键判断：如果最新消息包含tool_calls（工具调用指令），则跳转到工具节点 if last_message.tool_calls: return "tools" # 去工具节点 tool_node 执行工具调用 return END # 没有工具调用，直接结束流程 # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) # 8. 编排完整流程（状态图）：将节点和路由串成流水线 # StateGraph(MessagesState)：创建状态图，指定状态格式为MessagesState（内置的消息列表结构） workflow = StateGraph(MessagesState) # 8.1 添加节点 workflow.add_node("chat_bot", chat_bot) # 思考节点：判断是否调工具 workflow.add_node("tools", tool_node) # 工具节点：执行工具调用 # 条件流转：从chat_bot节点出发，由tool_router决定下一步 workflow.add_conditional_edges( source="chat_bot", # 起点：chat_bot节点 condition=tool_router, # 条件判断函数：tool_router # 无需手动映射目标节点（tool_router直接返回节点名） ) \n```\n\n核心节点：`chat_bot`（思考节点）\n\n核心节点：`tool_node`（工具节点）\n\n路由条件：`tool_router`（分流逻辑）\n\n**路由条件角色**：流程的 “交通警察”，解决 “下一步去哪” 的问题\n\n**路由条件角色**：判断逻辑， 检查 LLM 的最新输出是否包含 tool\\_calls（工具调用指令）：\n\n* 有 → 跳转到`tools`节点执行工具调用\n* 无 → 直接结束流程（返回最终回复）\n\n**返回值**：严格指定为`Literal["tools", "__end__"]`，确保类型安全，避免流转错误\n\n流程编排（`StateGraph`）核心逻辑：构建 “思考→判断→执行→再思考” 的闭环：\n\n```\n 入口 → chat_bot（思考）→ tool_router（判断）→ ① 无工具调用 → 结束 ② 有工具调用 → tools（执行）→ chat_bot（基于工具结果生成回复）→ 结束 \n```\n\n回顾langgraph 关键 API：\n\n* `add_node`：添加节点（思考节点、工具节点）\n* `set_entry_point`：设置流程入口（从`chat_bot`开始）\n* `add_edge`：固定流转（工具执行后返回`chat_bot`）\n* `add_conditional_edges`：条件流转（由`tool_router`决定下一步）\n\n## 3.4、工具到底怎么被调的？拆开看流程\n\n**痛点：**中间环节太多，容易断链，结果丢了都不知道。\n\n**方案：**`ToolNode` 一把梭， 解析 → 执行 → 返回结果，全自动接回对话。\n\n当模型输出：\n\n```\n tool_calls=[{\'name\': \'get_weather\', \'args\': {\'query\': \'今明两天天气\'}}] \n```\n\n→ 路由发现 `tool_calls` → 跳转到 `tool_node`\n\n→ 自动调本地方法 `get_weather()`\n\n→ 结果作为 `ToolMessage` 写入历史\n\n→ 再回到 `chat_bot`，模型结合真实数据生成最终回复\n\n## 3.5 、串成完整工作流：大脑 + 手脚 合体\n\n**痛点：**逻辑散，不成环，难维护。\n\n**方案：**用状态图把“思考→判断→执行→再思考”串成自动流水线。\n\n```\n workflow = StateGraph(MessagesState) workflow.add_node("chat_bot", chat_bot) workflow.set_entry_point("chat_bot") workflow.add_node("tools", tool_node) workflow.add_edge("tools", "chat_bot") workflow.add_conditional_edges( "chat_bot", tool_router, ) app_graph = workflow.compile() \n```\n\n> 两条路径自动切换：\n\n* 直接回答 → 结束\n* 需查数据 → 调工具 → 回模型 → 出答案\n\n相当于给AI装上了“手脚”，从嘴炮王变实干家\n\n## 3.6 、测试运行：看看它是怎么一步步干活的\n\n**痛点：**看不到过程，出问题没法 debug。\n\n**方案：**开流式输出，实时监听每一步。\n\n```\n async def run_streaming_chain(): messages = [ SystemMessage(content="你是一个智能助手..."), HumanMessage(content="帮我查一下今明两天的天气") ] initial_state = {"messages": messages} async for event in app_graph.astream(initial_state, stream_mode=\'messages\'): if isinstance(event, tuple): chunk = event[0] if chunk.type == \'AIMessageChunk\': print(\'event里监听到的流式输出------>\', chunk.content) \n```\n\n`stream_mode=\'messages\'`：逐字打印输出，调试神器\n\n## 3.7、自验证：看看结果长啥样\n\n**痛点：**用户以为模型“天生就知道”，根本看不出用了工具。\n\n**方案：**通过日志看清全过程：指令 → 执行 → 回复。\n\n用户输入：\n\n> 帮我查一下今明两天的天气\n\n模型第一步不是回答，而是发指令：\n\n```\n last_message.tool_calls ------> [{\'name\': \'get_weather\', \'args\': {\'query\': \'今明两天天气\'}, ...}] \n```\n\n→ 触发工具 → 获取真实数据 → 模型整合输出：\n\n> “好的，我来帮您查询一下。经过查询，今天天气晴朗，温度20度，明天天气多云，温度25度……”\n\n> 用户无感，但背后已完成一次“AI + 工具”的协同作业\n\n### 核心流程图\n\n> 两条路自动走：能干就干，不能就调工具，全程无人干预\n\n### 总结：工具调用核心五步\n\n| 步骤 | 组件 | 干啥用 |\n| --- | --- | --- |\n| 1. 定义工具 | `@tool` | 把函数变成 AI 能调的“能力” |\n| 2. 绑定工具 | `bind_tools()` | 告诉模型：“你现在有这技能” |\n| 3. 判断是否调 | `tool_router` | 看有没有 `tool_calls`，决定走哪条路 |\n| 4. 执行工具 | `ToolNode` | 自动跑函数，拿结果回来 |\n| 5. 整合回复 | 回到 `chat_bot` | 模型基于真实数据生成人话 |\n\n> 从此，机器人从“只会说”升级为“又能说又能做”\n\n可接入数据库、搜索、计算器……想连啥连啥。\n\n## 第四章： 为langgraph应用 添加记忆功能\n\n在写聊天机器人时，没有记忆就像金鱼，游两下就忘了自己是谁。\n\n咱们上一节给机器人装了工具，能查资料、能干活，但它还是记不住聊过啥。\n\n这不行啊，用户说一遍名字，下次还得重新介绍，体验直接掉地上。\n\n所以这一节，咱给它加上“脑子”，让它记住对话历史，真正实现多轮连贯交流。\n\n### 4.1. 添加记忆功能\n\n加记忆功能，不需要额外装包，LangGraph 自带这个本事。\n\n它用的是 `langgraph.checkpoint.memory` 模块里的 `MemorySaver`，说白了就是个内存快照工具。\n\n每次对话一结束，它就把当前状态拍个照存起来，下次接着用。\n\n开发阶段用它正合适，简单又省事，就跟手机临时存图一样。\n\n#### 解决方案：\n\n用 LangGraph 的 `MemorySaver` 当临时笔记本，自动存对话记录。 靠 `thread_id` 区分不同用户，互不串台。\n\nLangGraph 自带 `MemorySaver` 它就像一个会自动记笔记的助理：每次聊完记一笔，下次见你先翻本子再开口。\n\n#### 怎么加记忆？三步搞定：\n\n**（1）定义状态：消息列表支持累积**\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n> `add_messages` 是关键：新消息不是覆盖，而是追加到历史里，上下文不断档。\n\n**（2）创建 MemorySaver，当内存记事本**\n\n```\n memory = MemorySaver() \n```\n\n**（3）编译图时传 checkpointer，开启自动保存**\n\n```\n graph = graph_builder.compile(checkpointer=memory) \n```\n\n每次对话结束，系统自动拍个“状态快照”存进去，下次按 `thread_id` 找回来继续聊。\n\n#### 多人聊天不串台？靠 thread\\_id\n\n每个用户分配唯一 `thread_id`，相当于独立聊天室：\n\n```\n config = {"configurable": {"thread_id": "1"}} \n```\n\n* 用户A用 `"1"`，B用 `"2"` → 各聊各的，不干扰\n* 回到 `"1"` → 记忆还在，接着上次聊\n\n注意：开发阶段 用 `MemorySaver`（内存存储），重启就丢。生产建议换 `SqliteSaver` 或数据库持久化。：\n\n### 4.2 使用记忆增强聊天机器人\n\n新建一个文件叫 `3-add-memory.py`，开始搞有记忆的机器人。\n\n```\n """LangGraph 教程: 添加记忆功能的聊天机器人 本示例展示了如何使用 LangGraph 的检查点功能为聊天机器人添加记忆功能， 使其能够记住对话历史并在多轮对话中保持上下文。 """ from typing import Annotated from langchain.chat_models import init_chat_model from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict # 导入 MemorySaver 用于实现记忆功能 from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition import os from dotenv import load_dotenv # 加载.env文件中的环境变量 load_dotenv() # 定义状态类型，使用 add_messages 注解来自动合并消息列表 class State(TypedDict): messages: Annotated[list, add_messages] # 消息列表将使用 add_messages reducer 自动合并 # 初始化 DeepSeek 聊天模型 llm = init_chat_model( "deepseek-chat", # 使用DeepSeek模型 api_key=os.environ.get("DEEPSEEK_API_KEY") # 从环境变量中获取API密钥 ) # 创建状态图构建器 graph_builder = StateGraph(State) # 初始化Tavily搜索工具 print("\\n初始化Tavily搜索工具...") tool = TavilySearch(max_results=2) # 设置最多返回2个搜索结果 tools = [tool] # 将工具绑定到LLM llm_with_tools = llm.bind_tools(tools) # 定义聊天机器人节点函数 def chatbot(state: State): """LLM节点函数，处理用户输入并生成响应""" return {"messages": [llm_with_tools.invoke(state["messages"])]} # 添加聊天机器人节点 graph_builder.add_node("chatbot", chatbot) # 添加工具节点 tool_node = ToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) # 添加条件边 graph_builder.add_conditional_edges( "chatbot", tools_condition, ) # 工具调用完成后，返回到聊天机器人节点 graph_builder.add_edge("tools", "chatbot") graph_builder.set_entry_point("chatbot") print("\\n构建图并添加记忆功能...") # 创建内存保存器 print("\\n创建 MemorySaver 实例作为检查点保存器...") memory = MemorySaver() # 在内存中保存状态，适用于开发和测试 # 使用内存保存器编译图 print("使用检查点保存器编译图...") graph = graph_builder.compile(checkpointer=memory) # 将内存保存器传递给图 # 打印图结构 print("\\n图结构如下：") print(graph.get_graph().draw_mermaid()) # 定义对话线程ID print("\\n设置对话线程 ID = \'1\'...") config = {"configurable": {"thread_id": "1"}} # 使用线程ID来标识和区分不同的对话 # 示例 1: 第一次对话 print("\\n示例 1: 第一次对话 - 用户介绍自己") user_input = "Hi there! My name is Will." print(f"\\n用户输入: \'{user_input}\'") # 注意: config 是 stream() 函数的第二个参数! print("使用线程 ID \'1\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 传递包含 thread_id 的配置 stream_mode="values", ) print("\\n助理回应:") for event in events: event["messages"][-1].pretty_print() # 打印助理的回应 # 示例 2: 测试记忆功能 print("\\n\\n示例 2: 第二次对话 - 测试记忆功能") user_input = "Remember my name?" print(f"\\n用户输入: \'{user_input}\'") # 使用相同的线程ID再次调用图 print("使用相同的线程 ID \'1\' 再次调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 使用相同的配置，图将加载之前保存的状态 stream_mode="values", ) print("\\n助理回应 (应该记得用户名字):") for event in events: event["messages"][-1].pretty_print() # 示例 3: 新对话线程 print("\\n\\n示例 3: 新对话线程 - 测试线程隔离") print("创建新的线程 ID = \'2\'...") # 使用不同的线程ID print("使用新的线程 ID \'2\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, {"configurable": {"thread_id": "2"}}, # 使用新的线程ID stream_mode="values", ) print("\\n助理回应 (不应该记得用户名字):") for event in events: event["messages"][-1].pretty_print() # 示例 4: 返回第一个线程 print("\\n\\n示例 4: 返回第一个线程 - 验证记忆持久性") print(f"\\n用户输入: \'{user_input}\'") # 再次使用第一个线程ID print("再次使用线程 ID \'1\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 使用原始线程ID stream_mode="values", ) print("\\n助理回应 (应该仍然记得用户名字):") for event in events: event["messages"][-1].pretty_print() \n```\n\n### 4.3 代码解析\n\n#### 关键改动点\n\n```\n from langgraph.checkpoint.memory import MemorySaver \n```\n\n这是新加的核心依赖，相当于给机器人配了个“小本本”。\n\n以前每次对话都像第一次见面，现在它会翻本本看看你们之前聊了啥。\n\n```\n memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) \n```\n\n这两行是关键操作：\n\n第一行，创建一个内存记录员；\n\n第二行，把这个记录员塞进图里，让它自动拍照存档每一步状态。\n\n只要你不关服务，它就能一直记住。\n\n> 生产环境别用这个，得换成数据库版的 `SqliteSaver` 或 `PostgresSaver`，不然重启就全丢了。\n\n```\n config = {"configurable": {"thread_id": "1"}} \n```\n\n这个 `thread_id` 就像是对话身份证。 同一个 ID，读的是同一份记忆；换个 ID，就是全新对话。\n\n多用户场景下，靠它实现隔离，不会张冠李戴。\n\n```\n events = graph.stream(..., config, stream_mode="values") \n```\n\n注意啊，`config` 是第二个参数，位置不能错。\n\n传进去之后，图就知道该去哪找之前的快照，而不是从头开始。\n\n### 4.4 运行聊天机器人\n\n运行命令：\n\n```\n uv run 3-add-memory.py \n```\n\n你会看到输出长这样：\n\n```\n 设置对话线程 ID = \'1\'... 示例 1: 第一次对话 - 用户介绍自己 用户输入: \'Hi there! My name is Will.\' ... Assistant: Hi Will! It\'s great to meet you. How can I assist you today? 示例 2: 第二次对话 - 测试记忆功能 用户输入: \'Remember my name?\' ... Assistant: Of course, Will! I’ll remember your name for the rest of our conversation. 示例 3: 新对话线程 - 测试线程隔离 用户输入: \'Remember my name?\' ... Assistant: I don’t have the ability to remember personal details like names between interactions. 示例 4: 返回第一个线程 - 验证记忆持久性 用户输入: \'Remember my name?\' ... Assistant: Got it, Will! I’ll keep your name in mind while we chat. \n```\n\n你看：\n\n* 同一个线程 ID，第二次还能叫出“Will”；\n* 换个 ID，立马变脸不认人；\n* 回到原来的 ID，记忆还在，没丢。\n\n这就是线程级记忆的威力。\n\n### 4.5 LangGraph 中 thread\\_id（线程 ID）\n\n在 LangGraph 中，**thread\\_id（线程 ID）的隔离能力核心依赖「检查点（Checkpointer）」的状态分区机制**——\n\n简单说：`thread_id` 是状态的「唯一索引」，不同 `thread_id` 对应独立的对话状态存储，彼此互不干扰。\n\n简单记：**LangGraph 的 thread\\_id 管 “对话记忆”，理论上叫做memory\\_id，和 Python “并发干活”的线程 ID ，八竿子打不着**。\n\n#### （1）先明确memory\\_id核心前提\n\n要理解隔离，先搞懂两个关键概念：\n\n**(1) 检查点（Checkpoint）：LangGraph 中用于「持久化存储对话状态」的核心机制，本质是「状态快照」—— 每次流程执行后，会把最新的 `state`（比如 `messages` 对话历史）保存起来，下次执行时可恢复。**\n\n**(2) thread\\_id（理论的memory\\_id）：给「每一组独立对话」分配的唯一标识，相当于对话的「身份证」，用于区分不同用户 / 不同会话的状态。**\n\n#### （2）隔离的核心原理：`thread_id` 作为状态的「隔离键」\n\nLangGraph 的 `Checkpointer`（这里用的是 `MemorySaver`）会把所有对话状态，按照 `thread_id` 进行「分区存储」—— 不同 `thread_id` 的状态存在完全独立的「命名空间」里，彼此不会覆盖或混淆。\n\n可以把它想象成「文件柜」：\n\n* `Checkpointer` = 一个文件柜\n* `thread_id` = 文件柜里的「抽屉编号」\n* 每个抽屉（`thread_id`）里存放对应对话的状态（`messages` 等）\n* 打开抽屉时，只能看到当前 `thread_id` 下的文件（状态），看不到其他抽屉的内容\n\n关键逻辑：状态的「存储」与「读取」都绑定 `thread_id`\n\n**(1) 存储时：流程执行后，`Checkpointer` 会把更新后的 `state` 与传入的 `thread_id` 绑定，存入对应分区；**\n\n**(2) 读取时：下次调用流程传入相同 `thread_id`，`Checkpointer` 会自动加载该分区下的历史状态，继续推进对话；**\n\n**(3) 隔离时：不同 `thread_id` 对应不同分区，加载状态时只会读取自己分区的内容，自然实现隔离。**\n\n#### （3）核心组件：`MemorySaver` 如何实现分区存储？\n\n代码中用的 `MemorySaver` 是 LangGraph 提供的「内存级检查点实现」（适用于开发测试），其内部维护了一个「字典结构」，专门用于按 `thread_id` 分区存储状态。\n\n`MemorySaver` 的简化内部逻辑（伪代码）\n\n```\n class MemorySaver: def __init__(self): # 核心存储结构：key=thread_id，value=该线程的最新状态（checkpoint） self.storage = {} # 保存状态：绑定 thread_id def save_checkpoint(self, thread_id, state): self.storage[thread_id] = state # 按 thread_id 存入对应键值对 # 读取状态：按 thread_id 提取 def get_checkpoint(self, thread_id): return self.storage.get(thread_id, None) # 只返回当前 thread_id 的状态 \n```\n\n结合 的代码看：\n\n* 当 传入 `config = {"configurable": {"thread_id": "1"}}` 时，`MemorySaver` 会把对话状态存入 `self.storage["1"]`；\n* 后续传入相同 `thread_id="1"`，会读取 `self.storage["1"]` 中的历史状态（比如第一次对话的 `Hi there! My name is Will.`）；\n* 当传入 `thread_id="2"` 时，会读取 `self.storage["2"]`（初始为空，所以不记得名字）。\n\n#### (4)、完整执行流程：用你的代码示例验证隔离逻辑\n\n以你代码中的 4 个示例为例，一步步看 `thread_id` 如何隔离：\n\n示例 1：第一次对话（thread\\_id="1"）\n\n**(1) 传入 `config={"configurable": {"thread_id": "1"}}` 和用户输入 `Hi there! My name is Will.`；**\n\n**(2) `graph.stream()` 启动流程，`Checkpointer` 检查 `thread_id="1"` 的存储：无历史状态；**\n\n**(3) 执行 `chatbot` 节点，生成回复（比如 `Hello Will! Nice to meet you!`）；**\n\n**(4) `Checkpointer` 把更新后的 `state`（包含用户输入 + AI 回复）存入 `storage["1"]`；**\n\n**(5) 输出回复，流程结束。**\n\n示例 2：测试记忆（thread\\_id="1"）\n\n**(1) 传入相同 `config`（`thread_id="1"`）和用户输入 `Remember my name?`；**\n\n**(2) `Checkpointer` 读取 `storage["1"]` 中的历史状态（包含第一次的对话）；**\n\n**(3) `chatbot` 节点基于历史状态生成回复（`Yes! Your name is Will.`）；**\n\n**(4) 更新后的状态（追加本次对话）再次存入 `storage["1"]`；**\n\n**(5) 输出回复，记忆生效。**\n\n示例 3：新线程（thread\\_id="2"）\n\n**(1) 传入 `config={"configurable": {"thread_id": "2"}}` 和用户输入 `Remember my name?`；**\n\n**(2) `Checkpointer` 检查 `storage["2"]`：无历史状态（新线程）；**\n\n**(3) `chatbot` 节点没有历史上下文，生成回复（`I don\'t think we\'ve met before. Could you tell me your name?`）；**\n\n**(4) 状态存入 `storage["2"]`；**\n\n**(5) 输出回复，实现隔离（不记得名字）。**\n\n示例 4：返回旧线程（thread\\_id="1"）\n\n**(1) 再次传入 `thread_id="1"`，`Checkpointer` 读取 `storage["1"]` 中的历史状态（包含前两次对话）；**\n\n**(2) `chatbot` 节点基于历史状态，仍然记得名字，生成正确回复；**\n\n**(3) 验证记忆持久性和隔离性。**\n\n#### (4)、关键细节：为什么必须通过 `config` 传入 `thread_id`？\n\nLangGraph 中，`config` 是「流程配置的统一入口」，`configurable` 字段用于传递「可配置的全局参数」，其中 `thread_id` 是 LangGraph 约定的「状态隔离关键字」—— 只有通过 `config={"configurable": {"thread_id": "xxx"}}` 传入，`Checkpointer` 才能识别并用于状态分区。\n\n如果不传入 `thread_id`：\n\n* `Checkpointer` 会使用默认的 `thread_id`（通常是一个随机值或固定值）；\n* 所有对话会共享同一个状态，无法实现隔离（比如新用户会看到上一个用户的对话历史）。\n\n#### (6)、生产环境扩展：除了 MemorySaver，还有哪些 Checkpointer？\n\n`MemorySaver` 是内存级实现，程序重启后状态会丢失，适用于开发测试。\n\n生产环境中，为了实现「持久化隔离」，可以使用以下 `Checkpointer`：\n\n**(1) `RedisSaver`：基于 Redis 存储状态，支持分布式部署，`thread_id` 作为 Redis 的 key；**\n\n**(2) `SQLiteSaver`：基于 SQLite 数据库存储，适合轻量生产环境；**\n\n**(3) `PostgresSaver`：基于 PostgreSQL 数据库存储，适合大规模生产环境。**\n\n它们的隔离原理完全一致 —— 都是以 `thread_id` 作为「隔离键」，只是存储介质不同，确保不同对话的状态独立存储。\n\n#### (7)、核心总结\n\n`thread_id` 的隔离能力，本质是「Checkpointer 按 thread\\_id 分区存储对话状态」：\n\n**(1) `thread_id` 是对话的唯一标识，作为状态存储的「分区键」；**\n\n**(2) `Checkpointer`（如 `MemorySaver`）内部维护按 `thread_id` 划分的存储结构；**\n\n**(3) 相同 `thread_id` 复用历史状态，不同 `thread_id` 从零开始，实现完全隔离；**\n\n**(4) 隔离的核心价值：支持多用户 / 多会话同时使用，彼此不干扰，且能保持各自的对话上下文。**\n\n这种设计让 LangGraph 能够轻松支持「多用户聊天机器人」「会话级记忆」等场景，是生产级对话系统的核心能力之一。\n\n#### (8)、这个thread\\_id（线程 ID） 和 python的线程id 有关系吗\n\n**完全没有关系**！这是两个名字碰巧都带 “线程”，但语义、作用、实现逻辑完全独立的概念 —— 可以理解为 “同名不同姓”，毫无关联。\n\n| 对比维度 | LangGraph 的 `thread_id`（对话线程 ID） | Python 的 `threading.Thread.ident`（系统线程 ID） |\n| --- | --- | --- |\n| **核心作用** | 区分「不同的对话会话」，实现对话状态隔离（记忆隔离） | 区分「操作系统中的执行线程」，实现并发任务调度 |\n| **本质** | 对话的「唯一标识 / 身份证」（字符串 / 数字均可） | 操作系统分配给线程的「执行单元编号」（整数） |\n| **使用场景** | 多用户 / 多会话聊天机器人（比如用户 A 和用户 B 的对话互不干扰） | Python 程序中并发执行多个任务（比如同时下载文件 + 处理数据） |\n| **存储关联** | 与 LangGraph 的 `Checkpointer`（状态存储）绑定，用于分区存储对话历史 | 与操作系统的线程调度器绑定，用于标识执行上下文 |\n| **生命周期** | 随对话存在（可手动指定，比如 `thread_id="user_123"` 可长期有效） | 随 Python 线程启动而创建，线程结束而销毁 |\n| **示例取值** | `"1"`、`"user_888"`、`"session_xyz"`（自定义字符串 / 数字） | `140703324567360`（操作系统分配的整数，不可自定义） |\n\n通俗解释：两个 “线程” 的本质区别\n\n**(1) LangGraph 的 `thread_id`：对话的 “专属文件夹”**\n\n* 它不是 “执行任务的线程”，而是「对话会话的唯一标识」。\n* 类比：你在聊天软件上和 A 朋友、B 朋友的对话，会存在两个独立的 “聊天窗口”（文件夹），`thread_id` 就是这两个窗口的编号 —— 确保你和 A 的聊天记录不会出现在和 B 的窗口里。\n* 核心：**用于 “数据隔离”（对话状态 / 记忆隔离）**。\n\n**(2) Python 的线程 ID：程序的 “干活工人编号”**\n\n* 它是操作系统层面的「执行单元标识」，对应一个 “干活的工人”。\n* 类比：你让程序同时 “下载文件” 和 “处理数据”，操作系统会分配两个 “工人”（线程），每个工人有唯一编号（线程 ID），负责独立完成任务，互不干扰。\n* 核心：**用于 “并发执行”（任务调度隔离）**。\n\n（1）LangGraph 的 `thread_id`（对话隔离）\n\n```\n # 两个不同的对话会话，用 thread_id 隔离 config_user_a = {"configurable": {"thread_id": "user_a"}} # 用户A的对话标识 config_user_b = {"configurable": {"thread_id": "user_b"}} # 用户B的对话标识 # 两个会话的状态独立存储，互不干扰 graph.stream({"messages": [{"role": "user", "content": "我是A"}]}, config_user_a) graph.stream({"messages": [{"role": "user", "content": "我是B"}]}, config_user_b) \n```\n\n（2）Python 的线程 ID（并发执行）\n\n```\n import threading def task(name): # 获取当前 Python 线程的 ID（操作系统分配） print(f"任务 {name} 运行在 Python 线程 ID: {threading.get_ident()}") # 启动两个并发线程，各自有独立的线程 ID thread1 = threading.Thread(target=task, args=("下载文件",)) thread2 = threading.Thread(target=task, args=("处理数据",)) thread1.start() # 输出：任务 下载文件 运行在 Python 线程 ID: 140703324567360 thread2.start() # 输出：任务 处理数据 运行在 Python 线程 ID: 140703324567361 \n```\n\n**(1) 两者无任何技术关联：LangGraph 的 `thread_id` 是 LangGraph 框架自定义的「对话标识」，Python 的线程 ID 是操作系统分配的「执行单元标识」；**\n\n**(2) 作用完全不同：一个管「对话数据隔离」（记忆不混淆），一个管「程序并发执行」（任务不干扰）；**\n\n**(3) 可以叠加使用：比如用 Python 多线程同时处理 100 个用户的对话，每个用户的对话用独立的 `thread_id` 隔离状态 —— 此时 Python 线程 ID 是 “工人编号”，LangGraph 的 `thread_id` 是 “每个工人处理的对话文件夹编号”，互不冲突。**\n\n简单记：**LangGraph 的 thread\\_id 管 “对话记忆”，Python 的线程 ID 管 “并发干活”，八竿子打不着**。\n\n### 4.6 小结\n\n加了 `MemorySaver`，它就开始记事了，能维持上下文，像个正常人聊天。\n\n一是 `MemorySaver`，内存存状态，开发够用；\n\n二是 `thread_id`，区分不同用户的对话流；\n\n三是 `checkpointer`，让图自动保存和恢复。\n\n> 流程：读历史 → 推理 → 工具调用 → 保存状态。一次完整带记忆的交互。\n\n### 第四章小节：LangGraph总结与展望\n\n整个过程像搭积木，一步接一步，核心靠的就是 **LangGraph** —— 它像个智能调度员，把各个功能串起来，让机器人真正“活”了起来。\n\n### LangGraph 五件事 小节\n\n**(1) LangGraph 是怎么工作的？**\n\n**(2) 上下文怎么记住？**\n\n**(3) 怎么让它会用工具？**\n\n**(4) 跨会话记忆怎么实现？**\n\n**(5) 多人聊天不串台？**\n\n> 图解：识别身份 → 加载记忆 → 判断是否需查资料 → 更新状态并回复。颜色代表：初始化（绿）、线程处理（蓝）、工具执行（橙）、状态操作（紫），数据流向一目了然。\n\n### 4.6 LangGraph 展望\n\n以 LangGraph 状态机为核心，扩展持久化、协同和人工干预能力，升级成可落地的AI系统。\n\n**(1) 长期记忆落地**\n\n当前状态存在内存里，重启全丢。应该接入 Redis 或 PostgreSQL，实现持久存储——把便签本换成保险柜。\n\n**(2) 关键操作让人把关**\n\n**(3) 多个AI组队干活**\n\n一个干不过来？那就组团。比如一个接待、一个查资料、一个写报告。用 LangGraph 编排多个智能体协作，打造“AI员工团队”。\n\n**(4) 对话支持“撤销”**\n\n**(5) 上生产环境扛高并发**\n\n本地跑得好，线上可能崩。要结合检查点 + 容器化部署，加上监控、容错、负载均衡，才能撑住真实流量。\n\nLangGraph 正在飞速进化，功能越来越多。但万变不离其宗：**用状态机管理AI流程**。掌握了这一点，你就拿到了构建高级AI系统的钥匙。\n\n## 第五章：langgraph 常见陷阱和解决方案\n\n> ......... 略5000字+\n>\n> ...................由于平台篇幅限制， 剩下的内容(5000字+)，请参参见原文地址\n\n#### 原始的内容，请参考 本文 的 原文 地址\n\n[本文 的 原文 地址](https://mp.weixin.qq.com/s/3Unzc4dHuYDwQyBgBaRKtA)\n\n## 第七章 大实操 预告 ：使用LangGraph从零构建多智能体AI系统：实现智能协作的完整指南\n\n尼恩团队即将推出， 基于langgraph的java 运维智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent\n\n在当今快速发展的AI领域，单一的大语言模型（LLM）已经难以满足复杂、多步骤的业务需求。多智能体（Multi-Agent）系统应运而生，它通过将多个专用的子智能体（sub-agent）组合起来，协同完成一项复杂任务，从而显著提升了系统的鲁棒性、可扩展性和问题解决能力。  \n 本文将以一个**智能 运维助手**的实际场景为例，深入探讨如何使用强大的[图计算框架， 从零开始构建一个功能完备的多Agent系统。\n\n我们将覆盖从基础架构设计、状态与记忆管理、人机回环 ），到最终使用开源可观测性平台 **LangFuse** 进行系统评测的全过程。\n\n* **指标查询Agent (metric Status Agent)** ：负责连接 Prometheus 数据库 ，查询的实时运行参数，如cpu、内存、jvm gc等。\n* **日志查询Agent (log search Agent)** ：负责连接 Elasticsearch\'日志数据库 ，查询的error 日志。\n* **问题定位Agent (question alanasys Agent)** ：负责根据日志和指标，分析的error 日志的根因。\n* **工单任务调度Agent (Maintenance Scheduler Agent)** ：负责与工单系统（Ticketing System）交互，根据故障信息创建、查询和更新维保任务。\n* **主管Agent (Supervisor)** ：作为总协调者，负责理解用户意图，并将任务路由给合适的子Agent。它不直接执行工具，而是“指挥”其他Agent工作。\n\n这个第七章 大实操 ，即将结合 尼恩团队后续 的langgraph **学习圣经推出，敬请期待。**\n\n* langgraph 学习圣经 （1）： 从0到1穿透 multi-agent多智能体 入门实战\n* langgraph 学习圣经 （2）： 从0到1穿透LangGraph架构与源码\n* langgraph 学习圣经（3） : 基于langgraph的java 运维多智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent 等\n\nposted @ 2025-12-28 21:38\xa0 [技术自由圈](https://www.cnblogs.com/crazymakercircle)\xa0 阅读(292)\xa0 评论(0)\xa0 \xa0 [收藏](javascript:void(0))\xa0 [举报](javascript:void(0))\n\n[刷新页面](#)[返回顶部](#top)\n\n[博客园](https://www.cnblogs.com/)  \xa0©\xa0 2004-2026   \n [浙公网安备 33010602011771号](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33010602011771) [浙ICP备2021040463号-3](https://beian.miit.gov.cn)\n\n ', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://www.cnblogs.com/crazymakercircle/p/19412858', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.99986124, 'saved_path': None}}], 'save_path': '/home/qinshan/widthresearch/data/downloads'}
2026-02-22 20:38:45,343 - __main__ - INFO - handle_download: searcher=TavilySearch, input_papers=1, save_path=/home/qinshan/widthresearch/data/downloads
2026-02-22 20:38:45,343 - __main__ - INFO - handle_download: downloaded=1
2026-02-22 20:38:45,343 - __main__ - INFO - call_tool payload: source_tool=tavily_download, result_type=papers, count=1
2026-02-22 20:38:45,343 - __main__ - INFO - call_tool: name=tavily_download, result_type=papers, count=1
2026-02-22 20:38:45,344 - __main__ - DEBUG - call_tool payload sample: {'paper_id': '', 'title': 'LangGraph 圣经：从0到1穿透multi-agent多智能体入门实战', 'authors': [], 'abstract': '* [博客园](https://www.cnblogs.com/)\n* [首页](https://www.cnblogs.com/crazymakercircle/)\n* [新随笔](https://i.cnblogs.com/EditPosts.aspx?opt=1)\n* [联系](https://msg.cnblogs.com/send/%E6%8A%80%E6%9C%AF%E8%87%AA%E7%94%B1%E5%9C%88)\n* [订阅](javascript:void(0))\n* [管理](https://i.cnblogs.com/)\n\n# [LangGraph 圣经：从0到1穿透 multi-agent多智能体 入门实战](https://www.cnblogs.com/crazymakercircle/p/19412858 "发布于 2025-12-28 21:38")\n\n# 本文 的 原文 地址\n\n#### 原始的内容，请参考 本文 的 原文 地址\n\n[本文 的 原文 地址](https://mp.weixin.qq.com/s/3Unzc4dHuYDwQyBgBaRKtA)\n\n## 尼恩：LLM大模型学习圣经PDF的起源\n\n在40岁老架构师 尼恩的**读者交流群**(50+)中，经常性的指导小伙伴们改造简历。\n\n然而，其中一个成功案例，是一个9年经验 网易的小伙伴，当时拿到了一个年薪近**80W的大模型架构offer**，逆涨50%，那是在去年2023年的 5月。\n\n* [惊天大逆袭：8年小伙20天时间提75W年薪offer，逆涨50%，秘诀在这](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247491496&idx=1&sn=cb31f7510a7c2efb7daf6cad793860ad&scene=21#wechat_redirect)\n\n不到1年，小伙伴也在团队站稳了脚跟，成为了名副其实的大模型 应用 架构师。接下来，尼恩架构团队，通过 梳理一个《LLM大模型学习圣经》 帮助更多的人做LLM架构，拿到年薪100W, 这个内容体系包括下面的内容：\n\n* [《**Python学习圣经：从0到1精通Python，打好AI基础**》](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247504585&idx=1&sn=dcc5f35e6733d36d5bb74113833d6263&scene=21#wechat_redirect)\n* [《**LLM大模型学习圣经：从0到1吃透Transformer技术底座**》](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247502020&idx=1&sn=a446e3f018639b85e49748d393e7620c&scene=21#wechat_redirect)\n\n* [**《SpringCloud + Python 混合微服务架构，打造AI分布式业务应用的技术底层》**](https://mp.weixin.qq.com/s?__biz=MzkxNzIyMTM1NQ==&mid=2247502436&idx=1&sn=cfbf4aec8188cf6025f9a7d09965b809&scene=21#wechat_redirect)\n* 《LLM 智能体 学习圣经：从0到1吃透 LLM 智能体 的架构 与实操》\n* 《LLM 智能体 学习圣经：从0到1吃透 LLM 智能体 的 中台 架构 与实操》\n* 《[**Spring 集成 DeepSeek 的 3大方法，史上最全**](https://mp.weixin.qq.com/s/wWJ8-Py3c6FyjkpiKAKihQ)》\n* 《[基于Dify +Ollama+ Qwen2 完成本地 LLM 大模型应用实战](https://mp.weixin.qq.com/s/lruBtgNRouvoJVqsmRDfIg)》\n* 《**Spring AI 学习圣经 和配套视频** 》\n* 《[AI部署架构：A100、H100、A800、H800、H20的差异以及如何选型？开发、测试、生产环境如何进行部署架构？](https://mp.weixin.qq.com/s/m8_2my55QWhzfnDs70TTUQ)》\n* [**生产环境 K8S + Deepseek 实现大模型部署 和 容器调度（图解+史上最全）**](https://mp.weixin.qq.com/s/xagt2GYz_pKHkdePBZ6Y0Q)\n* [**《最近大火的 MCP 协议，看这篇文章就够了》**](https://mp.weixin.qq.com/s/jwzEFeHuB_k9BA7go8bNVg)\n* [《美团面试：LLM 大模型会有 什么问题？说说进行 RAG 优化的方法？》](https://mp.weixin.qq.com/s/ny7Y9-tIzepfSCR-RSuwbA)\n\n## LangGraph 圣经 介绍\n\n接下来，尼恩团队开始给大家写 LangGraph 学习圣经 ，包括：\n\n* LangGraph学习圣经 （1）： 从0到1穿透 multi-agent多智能体 入门实战\n* LangGraph学习圣经 （2）： 从0到1穿透LangGraph架构与源码\n* LangGraph学习圣经（3） : 基于langgraph的java 运维多智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent 等\n\n现本文是第一篇：从0到1穿透 multi-agent多智能体 入门实战\n\n## 第一章. LangGraph介绍，从单智能体 到多智能体 的演进\n\n### 1.1 、从“一个人独干(单智能体)”到“团队协作(多智能体)”的转变\n\n而现在的趋势是：**让专业的人干专业的事**。\n\n* 研究员智能体：专门负责找资料\n* 事实核查员：专挑毛病，验证真假\n* 写作助手：专注输出流畅内容\n* 编辑：最后润色把关\n\n研究数据显示，这种分工协作的方式，在处理复杂任务时性能提升 **40%-60%**，而且更容易调试、维护和扩展。\n\n### 1.2 一个 AI研究助手 多智能体例子\n\n**(1) 拆解问题**\n\n**(2) 查资料**\n\n**(3) 验证信息真伪**\n\n**(4) 输出一份结构清晰的研究报告**\n\n整个流程由多个AI智能体接力完成，背后靠的是 **LangGraph** 这个框架来调度协调。\n\n### 1.3什么是 LangGraph？\n\n普通的 AI 智能体 应用记不住事、理不清步骤，像个没头苍蝇。\n\n**怎么办？ 用“图”来设计 AI 流程** ：\n\n\n\nLangGraph 就是 **AI 的导演**，指挥模块按剧本走。\n\n比如做个旅游助手： 问目的地 → 查天气 → 推荐穿搭 → 生成行程。\n\n\n\nLangGraph是LangChain团队推出的开源框架，专为构建**有状态、长时间运行**的AI工作流而生。\n\n它不像别的工具那样, 封装太多细节.\n\nLangGraph 给你底层控制权，像搭积木一样组装AI系统。\n\nLangGraph 核心思想很简单：用“图”来建模AI行为，节点是动作，边是跳转逻辑，状态是记忆。\n\n> LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\n\n### 图结构三要素\n\n* **节点（Nodes）**：每个节点, 是一个独立任务. 比如调用大模型、查数据库、执行工具函数。\n* **边（Edges）**：每 一 边是Python函数，根据当前状态决定下一步去哪，支持条件判断和循环。\n* **状态（State）**：全局共享的数据结构，记录所有关键信息，相当于AI的“短期记忆”。\n\n有了这图结构三要素 ， 就能画出一条条清晰的执行路径，不怕逻辑混乱，也不怕断电重启丢数据。\n\n```\n from langgraph.graph import StateGraph, END # 示例：定义一个简单图 graph = StateGraph(dict) def node_a(state): return {"value": "from A"} def node_b(state): return {"value": "from B"} graph.add_node("A", node_a) graph.add_node("B", node_b) graph.set_entry_point("A") graph.add_edge("A", "B") graph.add_edge("B", END) app = graph.compile() \n```\n\n上面这段代码就是一个最简单的流程图：A → B → 结束。\n\n**(1) 开始节点 → 由 `graph.set_entry_point("A")`设定入口**\n\n**(2) 节点A → 执行 `node_a(state)`函数，返回 `{"value": "from A"}`**\n\n**(3) 节点B → 执行 `node_b(state)`函数，返回 `{"value": "from B"}`**\n\n**(4) 结束节点 → 由 `graph.add_edge("B", END)`设定终点**\n\n### 1.4 LangGraph 的核心优势\n\nLangGraph 本质是什么？ LangGraph 其实是一个 **“带记忆的、AI 任务图 、执行器”**。\n\n更技术一点说：\n\n它是一个**有状态、可持久化、基于图的工作流引擎**，灵感来自两种老但牛的技术：\n\n* **数据流编程**（Dataflow Programming）：数据推着计算走\n* **Actor 模型**：每个节点像个独立小演员，收到消息才行动\n\n这就让它特别适合做复杂AI流程，比如：\n\n* 自动规划任务\n* 多Agent协作\n* 出错自动重试\n* 用户中途修改需求也能接得上\n\n**LangGraph 的设计哲学很简单粗暴**：\n\n> **把AI应用当成一个永远在线的“活系统”，而不是一次性的函数调用。**\n\n你可以把它想象成一个**会记住上下文、能持续反应、还会自己做决定的智能机器人**，而不是一个“问一句答一句”的问答机。\n\n#### LangChain 和 LangChain 的区别？\n\n我们先来对比一下：\n\n* **LangChain**：像是流水线工人。\n\n  原料（输入）进来 → 经过几道工序（处理）→ 成品（输出）出去 → 完事，关机器。 下次再来？从头开始。\n* **LangGraph**：像是一个值班经理，24小时在岗。\n\n  他记得昨天客户说了啥，今天问题进展到哪了，还能根据情况叫同事帮忙、重试任务、甚至主动发消息提醒你。\n\n  LangGraph核心是：**状态一直在线，流程可以反复跳转。**\n\n### 1.6 LangGraph 的关键组件\n\n**问题根源：** 多个AI 任务，各模块各干各的，逻辑乱、状态丢、没法 debug。\n\n**解决思路：** 以“状态图”为核心，靠 **节点、边、状态** 三件套，构建可控可溯的工作流。\n\n三大支柱：\n\n* **节点（Nodes）**：执行单元，比如调大模型、查库；\n* **边（Edges）**：控制流程走向，可以固定跳转，也能条件判断；\n* **状态（State）**：共享内存，全程保存数据，上下文不断。\n\n支持循环：AI 生成内容后问用户“满意吗？” 不满意就回退修改，直到通过。\n\n支持人工干预：在流程中插入“等待确认”节点，处理完再继续。\n\n集成 RAG，先检索再回答，不说胡话；\n\n用 LangSmith 监控全过程，像 DevTools 一样看执行轨迹。\n\n### 1.5 LangGraph 五大能力：\n\n**1、状态保持、持久执行（Durable Execution）**\n\n跑一半断电了怎么办？重头再来？那用户不得疯了？\n\nLangGraph把每一步状态存下来，哪怕服务挂了，重启也能接着干，就像游戏存档一样靠谱。\n\n**2、人机协同、人机协作（Human-in-the-loop）**\n\nAI再聪明也有拿不准的时候。\n\n这时候可以让人类插一脚，看看状态、改改参数、点个确认，然后再继续。\n\n这种设计特别适合审批流、客服质检这类高风险场景。\n\n**3、持久化存储、全面记忆管理（Comprehensive Memory）**\n\n有的框架只记最近几句话，LangGraph不一样，它可以同时管：\n\n* 短期记忆：本次会话的状态\n* 长期记忆：跨会话的历史数据（结合向量库或数据库）\n\n这就让AI不仅能“接话”，还能“认人”，提供个性化体验。\n\n**4、调试能力（Debugging）**\n\n复杂的AI流程就像迷宫，光看日志根本找不到问题在哪。\n\nLangGraph配合LangSmith，能生成可视化轨迹图，每一步走到哪、状态变成啥样，清清楚楚。\n\n**5、工具集成 & 多智能体支持**\n\n想让AI查天气、订机票、写报告？没问题，接API就行。\n\n而且它天生支持多个AI协同工作，比如一个负责分析，一个负责决策，一个负责汇报。\n\n## 第二章. 使用一个LangGraph 构建基础多智能体 聊天机器人\n\n核心痛点：想搭聊天机器人，但不会把大模型塞进一个能扩展的流程里，代码乱、状态散，没法往复杂应用走。\n\n核心方案：用 **LangGraph** 搭个状态驱动的流水线，把大模型调用变成简单节点，实现“输入→处理→输出”的清晰控制流。\n\n## 2.1 环境准备\n\n做AI项目，第一步就是选个靠谱的大模型。\n\n别一上来就烧钱，咱们先试试免费的路子。\n\n百度千帆和硅基流动这两个平台，都有不错的国产模型可以白嫖。\n\n**百度千帆调用**\n\n这是目前demo里用的方式，靠 `QianfanChatEndpoint` 接入ERNIE-Speed-128K模型：\n\n```\n import os from langchain_community.chat_models import QianfanChatEndpoint llm = QianfanChatEndpoint( model="ERNIE-Speed-128K", streaming=True, # 启用流式输出 api_key=os.getenv(\'QIANFAN_AK\', \'\'), secret_key=os.getenv(\'QIANFAN_SK\', \'\') ) \n```\n\n注册个账号，拿AK/SK密钥就能跑起来，适合新手练手。\n\n**硅基流动调用方式**\n\n另一个选择是硅基流动，它支持GLM、Qwen这些热门开源模型：\n\n```\n from langchain_openai import ChatOpenAI llm = ChatOpenAI( model="THUDM/glm-4-9b-chat", streaming=False, api_key=os.getenv(\'SILICONFLOW_API_KEY\', \'\'), base_url=os.getenv(\'SILICONFLOW_BASE_URL\', \'\'), temperature=0.1, ) \n```\n\n换个base\\_url，就能当OpenAI用，兼容性好得很，迁移成本低。\n\n**Deepseek 调用方式**\n\nDeepseek 支持多款开源大模型（如 Deepseek-R1、Deepseek-Coder 等），且提供兼容 OpenAI API 格式的调用方式，迁移成本低，直接复用 `ChatOpenAI` 即可快速接入：\n\n```\n from langchain_openai import ChatOpenAI import os # Deepseek 调用（兼容 OpenAI 接口格式） llm = ChatOpenAI( model="deepseek-chat", # 可选模型：deepseek-chat（通用）、deepseek-coder-v2（编程）等 streaming=True, # 支持流式输出，按需开启 api_key=os.getenv(\'DEEPSEEK_API_KEY\', \'\'), # 从环境变量读取密钥（推荐） base_url="https://api.deepseek.com/v1", # Deepseek 官方 API 基础地址 temperature=0.7, # 随机性调节：0~1，值越低输出越确定 max_tokens=4096 # 单次生成最大令牌数（按需调整，不同模型上限不同） ) \n```\n\n**前置准备**：注册 Deepseek 账号（官网：[https://www.deepseek.com/），进入「API](https://www.deepseek.com/%EF%BC%89%EF%BC%8C%E8%BF%9B%E5%85%A5%E3%80%8CAPI) 密钥管理」获取 `DEEPSEEK_API_KEY`，建议将密钥存入环境变量（避免硬编码），如：\n\n```\n # Linux/Mac export DEEPSEEK_API_KEY="你的密钥" # Windows（命令行） set DEEPSEEK_API_KEY="你的密钥" \n```\n\n痛点：API 密钥写死在代码里？不安全！依赖安装慢还冲突？新手直接卡住。\n\n解决方案：用 `uv` 快速装包，`.env` 文件管密钥，环境干净又安全，一次配好反复用。\n\n```\n # 安装 LangGraph 和周边依赖 uv pip install -U langgraph langchain python-dotenv typing-extensions \n```\n\n```\n # .env 文件存密钥 DEEPSEEK_API_KEY=your_deepseek_api_key_here \n```\n\n自动加载配置，密钥不进代码，不怕泄露。\n\n## 2.2 实现基础聊天机器人\n\n痛点：直接调大模型？**没流程、没记忆，对话像复读机**，体验差。\n\n解决方案：用 LangGraph 维护消息列表作为状态，串起“用户输入 → 模型回复” 的复杂路线，让对话有上下文、能连贯。\n\nLangGraph的核心，就是把业务逻辑画成一张“流程图”。\n\n每个节点干一件事，边来决定执行顺序。\n\n就像工厂流水线，零件从这头进，那头出成品。\n\n```\n from typing import Annotated from langchain.chat_models import init_chat_model from typing_extensions import TypedDict from langgraph.graph import StateGraph, START from langgraph.graph.message import add_messages import os from dotenv import load_dotenv # 加载.env文件中的环境变量 load_dotenv() class State(TypedDict): messages: Annotated[list, add_messages] graph_builder = StateGraph(State) llm = init_chat_model( "deepseek-chat", # 使用DeepSeek模型 api_key=os.environ.get("DEEPSEEK_API_KEY") ) def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except KeyboardInterrupt: print("\\nGoodbye!") break \n```\n\n> 一句话概括：定义状态 → 创建图 → 添加节点 → 编译运行 → 流式输出。\n\n核心的代码如下\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() \n```\n\n## 2.3 基础聊天机器人 代码解析\n\n痛点：LangGraph 组件太多，看不懂谁干啥，学得迷糊。\n\n解决方案：盯死两个核心：**状态累积** 和 **流程编排**，其他都是配角。\n\n### 1. 引入依赖\n\n导包是第一步，为状态、模型、流程打基础。\n\n### 2. 加载环境变量\n\n```\n load_dotenv() \n```\n\n从 `.env` 读密钥，绝不硬编码。\n\n### 3. 定义状态 结构\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n所有消息存在一个可累加的 list 里，新消息自动追加，上下文不断。\n\n这行代码是 **LangGraph 中定义「可自动累加对话状态」的核心语法**。专门解决之前提到的「对话历史持久化」问题 。\n\n这行代码， 让 `messages` 字段能自动追加新消息（用户提问、AI 回复），而不是被覆盖。\n\n在 LangGraph 流程中，`State` 是流转的数据载体，而 `messages` 是存储「对话历史」的关键字段（比如 `[{"role": "user", "content": "你好"}, {"role": "assistant", "content": "你好！"}]`）。\n\n这行代码的核心目的是：**让每次节点执行后，新生成的消息（如 AI 回复）自动追加到 `messages` 列表中，而非替换原有历史**，确保流程能基于完整上下文推进。\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n**1、`class State(TypedDict)`：定义强类型的状态字典**\n\n* `TypedDict`（类型字典）：是 Python 标准库 `typing` 中的工具，用于定义「键名固定、值类型明确」的字典。作用：约束 `State` 的结构（必须有 `messages` 字段，且类型是 `list`），避免写代码时出现键名错误（如把 `messages` 写成 `message`），同时让 IDE 提供类型提示。\n* 对比普通字典：如果直接用 `dict` 定义状态，无法约束字段，而 `TypedDict` 让状态结构「可预期、可校验」，是 LangGraph 推荐的状态定义方式。\n\n  示例：符合 `State` 类型的合法数据\n\n  ```\n   valid_state = { "messages": [ {"role": "user", "content": "介绍 LangGraph"}, {"role": "assistant", "content": "LangGraph 是流程编排工具"} ] } \n  ```\n\n2、messages: Annotated[list, add\\_messages] ：给 messages 字段加「自动追加规则」 add\\_messages。\n\n这是关键中的关键，`Annotated` + `add_messages` 共同实现「对话历史自动累加」。\n\n（1）`Annotated[类型, 元数据]`：Annotated给字段附加额外规则，这是 Python 3.9+ 引入的标准库工具（从typing导入），作用是「给类型添加元数据 / 规则」。\n\n格式：\n\n```\n Annotated[基础类型, 规则1, 规则2, ...] \n```\n\n这里的「规则」会被 LangGraph 识别并生效。\n\n这里的 `Annotated[list, add_messages]` 表示：`messages` 字段的基础类型是 `list`（存储对话消息列表），同时附加 `add_messages` 这个「特殊规则」。\n\n（2）`add_messages`：LangGraph 定义的一个函数，是一个自定义「消息追加处理器」\n\n* `add_messages` 是从 `langgraph.graph` 导入的核心工具（需显式导入：`from langgraph.graph import add_messages`），核心代码参考下面。\n* add\\_messages 核心作用：自动合并「节点返回的新消息」和「原有状态中的旧消息」，具体逻辑是：\n  1. 假设原有状态的 `messages` 是：`[用户提问1]`\n  2. 节点函数（如之前的 `chatbot`）返回：`{"messages": [AI 回复1]}`\n  3. `add_messages` 会自动将两者合并为：`[用户提问1, AI 回复1]`，并更新到新状态中\n* 对比没有 `add_messages` 的情况：如果直接定义 `messages: list`，节点返回的新消息会「覆盖」原有 `messages`（原有对话历史丢失）。\n\nLangGraph 的 `add_messages` 源码（简化后）如下 ：\n\n```\n from typing import Any, List, Union def add_messages( existing: Union[List[Any], None], # 状态中已有的旧消息（可能为 None） updates: Union[List[Any], Any, None] # 节点返回的新消息（可能是列表/单条/None） ) -> List[Any]: """ 合并新旧消息：旧消息列表 + 新消息（自动处理单条/列表格式） - 若旧消息不存在，直接返回新消息列表 - 若新消息是单条，自动转为列表后追加 - 若新消息为 None，返回原旧消息列表 """ # 初始化旧消息列表（避免 None 报错） current = existing.copy() if existing is not None else [] # 处理新消息：统一转为列表格式 if updates is None: new_messages = [] elif isinstance(updates, list): new_messages = updates else: new_messages = [updates] # 单条消息 → 列表 # 合并：旧消息 + 新消息（保证顺序，不覆盖） return current + new_messages \n```\n\n如果没有 `add_messages` 规则：\n\n* 初始状态：`{"messages": [用户提问]}`\n* 节点执行后返回：`{"messages": [AI 回复]}`\n* 最终状态：`{"messages": [AI 回复]}`（用户提问被覆盖，丢失历史）\n\n有了 `add_messages` 规则：\n\n* 初始状态：`{"messages": [用户提问]}`\n* 节点返回：`{"messages": [AI 回复]}`\n* `add_messages` 自动合并：`[用户提问] + [AI 回复] = [用户提问, AI 回复]`\n* 最终状态：`{"messages": [用户提问, AI 回复]}`（历史保留，后续节点可复用上下文）\n\n这行代码的本质是：**用 `TypedDict` 约束状态结构，用 `Annotated + add_messages` 给 `messages` 字段绑定「自动追加」规则**。\n\n最终实现「对话历史不丢失、上下文可复用」—— 这正是 LangGraph 能支持多轮对话、复杂流程的基础， 这就是提到的「不怕断电重启丢数据」（状态可持久化，历史都在 `messages` 中）。\n\n### 状态结构的本质\n\n传统流程像函数链：前一步输出传给下一步，**每步只能看到局部数据**。\n\n结果？上下文断裂、逻辑难控、调试抓狂。\n\n#### 核心解法\n\nLangGraph 用一个**全局共享状态**，所有节点操作同一份数据。\n\n就像工厂流水线：半成品从一站传到下一站，每一站都加工它、更新它。\n\n```\n # 错误理解：以为是函数链 def workflow(): result1 = step1() result2 = step2(result1) return result3(result2) # 正确理解：是状态图 state = {"data": None} state = node1(state) # 每个节点都接收并返回完整状态 state = node2(state) state = node3(state) \n```\n\n> 状态 = 公共记事本，所有人一起看、一起改。\n\n> 不是每人一张草稿纸，最后对不上账。\n\n#### 关键点\n\n* **节点不是函数，是状态转换器**：读当前状态 → 加工 → 返回新状态。\n* **每一步都在提交快照**：像 Git commit，留下完整的系统瞬间。\n* **状态即上下文中枢**：所有节点共用一份数据源，不丢信息，不错乱序。\n\nLangGraph 的本质：**带记忆的自动化流水线**。 节点干活，状态流动，全程不断电。\n\n### 4. 创建图构建器\n\n```\n graph_builder = StateGraph(State) \n```\n\n初始化一个共享状态的流程图，所有节点都能看到最新消息。\n\n这行代码是 **LangGraph 流程图的 “地基构建”**： 创建一个「绑定了状态结构」的流程图构建器。\n\n核心作用是：明确流程图中流转的「数据格式（State）」，让后续节点、边的定义都遵循这个格式，避免数据混乱。\n\n先回顾代码上下文（结合之前的 `State` 定义）：\n\n```\n # 1. 先定义状态结构（约束数据格式） class State(TypedDict): messages: Annotated[list, add_messages] # 2. 创建流程图构建器（绑定状态） graph_builder = StateGraph(State) \n```\n\n逐部分解释：\n\n**（1）、`StateGraph`**：\n\n是 LangGraph 的核心类（从 `langgraph.graph` 导入），本质是「流程图构建工具」，负责管理「节点（处理步骤）」和「边（流转规则）」。\n\n可以把它理解为「一张空白的画布」，后续的 `add_node`（加节点）、`add_edge`（加流转）都是在这张画布上画画。\n\n**（2）、`State`**：\n\n是我们之前定义的「强类型状态结构」（基于 `TypedDict`），明确了流程图中流转的数据必须包含 `messages` 字段，且格式是「带 `add_messages` 规则的列表」。\n\n这里作为参数传入 `StateGraph`，表示「这张流程图的所有数据流转，都必须遵循 `State` 定义的格式」。\n\n**（3）、`graph_builder`**：\n\n是 `StateGraph` 类的实例（即 “构建好的空白画布”），后续所有流程图操作（加节点、加边、编译）都通过这个实例完成。\n\n### 5. 初始化大模型\n\n```\n llm = init_chat_model("deepseek-chat", api_key=os.environ.get("DEEPSEEK_API_KEY")) \n```\n\n接入 DeepSeek 当回答引擎，听你指挥。\n\n### 6. 编写节点逻辑\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} \n```\n\n输入当前消息，让模型生成回复，返回新消息对象。\n\n### 7. 搭建流程图\n\n```\n graph_builder.add_node("chatbot", chatbot) graph_builder.add_edge(START, "chatbot") graph = graph_builder.compile() \n```\n\n注册节点，连上起点，形成一条直线流程：问了就答。\n\n这些代码是 **LangGraph 构建对话流程图的核心逻辑**.\n\n核心作用是：定义一个「对话节点」，让流程从 “开始” 直接进入该节点，最终形成一个「 任务流程」（启动 → 对话 → 结束）。下面逐行拆解，结合 LangGraph 核心概念和实际作用讲清楚：\n\n先明确两个前提\n\n**(1) State（状态）：LangGraph 中流转的数据载体（类似 “流水线的物料”），这里的 `State` 是一个自定义数据结构（通常是字典或 Pydantic 模型），核心字段 `messages` 存储对话历史（比如用户提问、AI 回复）。**\n\n**(2) llm：之前定义的大语言模型实例（百度千帆 / 硅基流动 / Deepseek 等），负责接收对话历史并生成 AI 回复。**\n\n逐行代码解释\n\n#### (1). 定义对话节点函数 `chatbot`\n\n```\n def chatbot(state: State): return {"messages": [llm.invoke(state["messages"])]} \n```\n\n这是 LangGraph 的「节点函数」（节点是流程中的 “处理步骤”）.\n\n作用是：**接收当前对话状态，调用 LLM 生成回复，更新状态并返回**。\n\n函数参数 `state: State`：\n\n* 接收流程中传递过来的「当前状态」，里面包含 `state["messages"]`（对话历史列表，比如 `[{"role": "user", "content": "你好"}]`）。\n* 类型注解 `State` 是自定义的状态结构（确保代码规范，避免键名错误）。\n\n核心逻辑 `llm.invoke(state["messages"])`：\n\n* 调用 LLM 模型，传入完整的对话历史 `state["messages"]`（模型需要基于上下文生成回复，而不只是单轮提问）。\n* `invoke` 是同步调用方法（如果之前开启了 `streaming=True`，这里会返回流式迭代器，需配合处理）。\n* 模型返回结果是一个「AI 回复消息对象」（比如 `{"role": "assistant", "content": "你好！有什么可以帮你？"}`）。\n\n返回值 `{"messages": [llm.invoke(...)]}`：\n\n* LangGraph 要求节点函数返回「状态更新数据」（只需要返回要修改的字段，不需要全量状态）。\n* 这里表示：将 LLM 生成的 AI 回复，添加到 `messages` 字段中（覆盖旧的 `messages`？不 ——LangGraph 会自动合并：旧 `messages` + 新 AI 回复 = 新 `messages`，具体看状态定义是否允许列表追加，默认字典结构会覆盖，实际工程中会用 `list` 追加，比如 `{"messages": state["messages"] + [llm.invoke(...)]}`，这里 demo 简化了）。\n\n#### （2）. 向流程图添加「chatbot 节点」\n\n```\n graph_builder.add_node("chatbot", chatbot) \n```\n\n* `graph_builder` 是 `StateGraph` 的实例（流程图构建器），负责管理节点和节点间的流转。\n* add\\_node方法：注册一个节点到流程图中。\n  + 第一个参数 `"chatbot"`：节点的「唯一标识名」（后续用这个名字指定流转关系）。\n  + 第二个参数 `chatbot`：节点对应的「处理函数」（即上面定义的 `chatbot` 函数，节点被触发时会执行该函数）。\n\n#### （3）. 定义流程的「入口节点」和「流转关系」\n\n`START` 是 LangGraph 内置的「起始节点」（流程的入口，类似流程图的 “开始” 符号）。\n\n* `add_edge(START, "chatbot")`：定义一条流转规则：**流程从 START 开始后，直接进入名为 "chatbot" 的节点**。\n* 这是最简单的流转关系（无分支、无条件），是线性流程的核心。\n\n#### （4）. 编译流程图为可执行应用\n\n```\n graph = graph_builder.compile() \n```\n\n* `compile()` 是构建器的核心方法，作用是：将前面定义的「节点」和「流转规则」，编译成一个「可执行的流程图应用」（`graph` 是编译后的实例）。\n* 编译后，graph具备了「运行流程」的能力，后续可以通过graph.invoke(初始状态)启动流程，比如：\n\n  ```\n   # 启动流程：传入初始对话状态（用户的第一个提问） result = graph.invoke({"messages": [{"role": "user", "content": "介绍一下 LangGraph"}]}) # 输出最终状态中的 messages（包含用户提问 + AI 回复） print(result["messages"]) \n  ```\n\n整个流程的执行逻辑（一句话总结）\n\n**(1) 调用 `graph.invoke(初始状态)` 启动流程；**\n\n**(2) 流程从 `START` 节点出发，根据 `add_edge` 规则，进入 `chatbot` 节点；**\n\n**(3) 执行 `chatbot` 函数：读取初始状态中的用户提问 → 调用 LLM 生成回复 → 更新 `messages` 字段；**\n\n**(4) 由于没有定义 `chatbot` 节点之后的流转关系（比如 `add_edge("chatbot", END)`），demo 中流程会在 `chatbot` 节点执行完成后自动结束。实际工程中，会显式指定 `add_edge("chatbot", END)` 或其他节点 ；**\n\n**(5) 返回最终的状态（包含用户提问和 AI 回复的完整对话历史）。**\n\n核心特点\n\n* 「清晰执行路径」：START → chatbot → 结束，无分支、无逻辑混乱；\n* 「状态可追溯」：所有对话数据都存在 `state["messages"]` 中，即使中断 / 重启，只要恢复 `state` 就能继续流程（不会丢数据）；\n* 「可扩展」：后续可以添加更多节点（比如 “意图识别”“工具调用”“记忆管理”），通过 `add_edge` 定义复杂流转（比如条件分支、循环），而不需要重构核心逻辑。\n\n### 8. 流式输出响应\n\n```\n def stream_graph_updates(user_input: str): for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): for value in event.values(): print("Assistant:", value["messages"][-1].content) \n```\n\n回复边生成边打印，像真人打字，体验丝滑。\n\n这行代码是 **LangGraph 流式输出对话结果的核心函数**，核心作用是：接收用户输入，启动流程图的「流式执行」，实时捕获 AI 回复并逐段打印（而非等待完整回复生成后再输出）。\n\n先明确两个前提\n\n**(1) 流式输出（streaming）：LLM 生成回复时，不是一次性返回完整内容，而是「逐句 / 逐段推送」（类似 ChatGPT 的打字机效果），减少用户等待感。**\n\n**(2) graph.stream()：LangGraph 编译后的 `graph` 实例的流式执行方法，与 `graph.invoke()`（同步阻塞，等待完整结果）相对，返回的是「事件迭代器」，实时输出流程执行中的状态更新。**\n\n非流式版本的等效代码（对比参考）：\n\n```\n def non_stream_graph_updates(user_input: str): # 同步阻塞，等待完整结果 result = graph.invoke({"messages": [{"role": "user", "content": user_input}]}) # 打印完整回复 print("Assistant:", result["messages"][-1].content) \n```\n\n再回到代码\n\n```\n def stream_graph_updates(user_input: str): # 1. 启动流程图的流式执行，传入初始状态 for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): # 2. 遍历事件中的所有节点输出（当前流程只有 chatbot 节点，所以只有一个 value） for value in event.values(): # 3. 提取并打印 AI 最新回复 print("Assistant:", value["messages"][-1].content) \n```\n\n**（1） 函数定义：`def stream_graph_updates(user_input: str):`**\n\n函数名：`stream_graph_updates` → 语义明确：流式获取流程图的更新（即 AI 回复）。\n\n参数 `user_input: str`：接收用户的提问（字符串类型），比如用户输入 `"介绍一下 LangGraph"`。\n\n**（2\\_) 核心：`graph.stream(初始状态)` → 启动流式执行**\n\n```\n for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}): \n```\n\n`graph.stream(...)`：\n\n* 作用：启动流程图的「流式执行」，返回一个「事件迭代器」（`event` 是迭代器的每一个元素）。\n* 传入的初始状态：`{"messages": [{"role": "user", "content": user_input}]}` → 把用户输入包装成 LangChain/LLM 通用的「消息格式」（`role` 表示角色，`content` 表示内容），作为流程的起始数据。\n* 流式特性：执行后不会阻塞等待完整结果，而是每生成一段 AI 回复，就产生一个 `event`，迭代器会实时返回该事件。\n\n`for event in ...`：循环迭代流式事件.\n\n每一个 `event` 对应「流程图中某个节点的一次状态更新」（当前流程只有 `chatbot` 节点，所以所有 `event` 都来自 `chatbot` 节点的流式输出）。\n\n**(3) 解析事件：`for value in event.values()`**\n\nfor value in event.values():event的结构：LangGraph 的流式事件是一个「字典」，键是「节点名称」，值是「该节点的最新状态」。\n\n* 比如当前流程中，`event` 的结构是：`{"chatbot": {"messages": [用户输入, AI 回复片段1]}}`（后续事件会是 `{"chatbot": {"messages": [用户输入, AI 回复片段1, 回复片段2]}}` 等）。\n* `event.values()`：获取所有节点的最新状态（当前只有 `chatbot` 节点，所以 `values()` 只有一个元素）。\n* 循环的目的：兼容多节点流程（比如后续加了「工具调用节点」，`event` 可能包含多个节点的状态，这里统一遍历解析）。\n\n**(4) 提取并打印最新回复：`value["messages"][-1].content`**\n\n```\n print("Assistant:", value["messages"][-1].content) \n```\n\n* `value`：即 `chatbot` 节点的最新状态（符合之前定义的 `State` 格式，包含 `messages` 字段）。\n* `value["messages"]`：当前完整的对话历史（用户输入 + 已生成的 AI 回复片段）。\n* `[-1]`：取列表的「最后一个元素」→ 也就是 LLM 刚刚流式推送的「最新回复片段」（因为 `add_messages` 规则，`messages` 是不断追加的，最后一个元素永远是最新的）。\n* `.content`：提取消息的内容（消息对象的结构是 `{"role": "assistant", "content": "回复内容"}`，`.content` 直接获取文本）。\n* 最终效果：每收到一个 AI 回复片段，就打印一次，呈现「打字机式」的实时输出。\n\n**关键细节：为什么能实现 “实时输出”？**\n\n**(1) 依赖 LLM 的 `streaming=True`：之前定义 LLM 时开启了 `streaming=True`（比如 Deepseek / 百度千帆的调用代码），LLM 才会支持流式推送回复片段。**\n\n**(2) 依赖 `graph.stream()`：LangGraph 会把 LLM 的流式输出「封装成事件迭代器」，确保每一个回复片段都能被实时捕获。**\n\n**(3) 依赖 `messages` 的追加规则：`add_messages` 确保每一个新的回复片段都追加到 `messages` 列表末尾，通过 `[-1]` 能精准取到最新片段。**\n\n对比：流式输出 vs 非流式输出\n\n为了更直观，对比 `graph.stream()`（流式）和 `graph.invoke()`（非流式）的差异：\n\n| 方式 | 核心函数 | 执行效果 | 适用场景 |\n| --- | --- | --- | --- |\n| 流式输出 | `graph.stream()` | 逐段实时打印，类似打字机 | 交互类场景（如聊天机器人），提升用户体验 |\n| 非流式输出 | `graph.invoke()` | 等待完整回复生成后一次性打印 | 批量处理（如批量生成报告），无需实时交互 |\n\n### 9. 主循环交互\n\n```\n while True: try: user_input = input("User: ") if user_input.lower() in ["quit", "exit", "q"]: print("Goodbye!") break stream_graph_updates(user_input) except KeyboardInterrupt: print("\\nGoodbye!") break \n```\n\n> 命令行入口，支持持续对话，按 q 或 Ctrl+C 优雅退出。\n\n这行代码是 **对话机器人的「交互循环入口」**，核心作用是：创建一个「持续运行的命令行交互环境」，让用户能反复输入提问、接收 AI 流式回复，同时支持正常退出和异常中断，是连接用户输入与 LangGraph 流程的 “桥梁”。下面逐行拆解逻辑、循环原理和边界处理：\n\n这行代码的本质是：**搭建一个「持续运行、支持多轮交互、优雅退出」的命令行对话入口**，将用户的手动输入转化为 LangGraph 流程的触发信号，最终实现 “用户提问 → AI 实时回复” 的完整交互闭环，是对话机器人从 “代码片段” 变成 “可使用工具” 的关键一步。\n\n## 2.4 运行聊天机器人\n\n痛点：跑完不知道成没成功？一脸懵。\n\n解决方案：运行脚本，输入问题，看到流式回复——就成了！\n\n```\n uv run 1-build-basic-chatbot.py \n```\n\n示例输出：\n\n```\n User: who are you? Assistant: I’m DeepSeek Chat, your AI assistant created by **DeepSeek**! \n```\n\n大模型已通，基本对话能力到手。\n\n### 小结\n\n痛点：教程太碎，学完还是不会搭真正的 AI 应用。\n\n解决方案：记住三个词：**状态、节点、边** —— 这就是 LangGraph 的骨架。\n\n这个机器人虽小，五脏俱全：\n\n* **状态**：存上下文（比如聊天记录）\n* **节点**：干活的（比如调模型）\n* **边**：定流程（下一步去哪）\n\n往后要加记忆、分支、工具调用？直接往上堆就行。\n\n## 第三章. 使用 Langgraph 完成工具调用\n\n我们之前用 LangGraph 做了个聊天机器人，但它只能“空想”，没法查天气、搜资料、算数学。\n\n现在要让它**能思考，也能动手**——比如你问“明天天气咋样？”，它会主动去“查一下”再告诉你。\n\n想让大 模型 查新鲜事、找实时数据？得给它接个“外挂大脑”。这外挂，就是工具（Tools）。 接上了，大模型就能上网搜、调接口、拿最新信息，不再靠死记硬背过日子。\n\n这就叫 **工具调用（Tool Calling）**。\n\n先给出完整可运行代码\n\n```\n # 1. 基础依赖导入 import asyncio from typing import Literal, List from langchain_core.tools import tool from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage from langchain_core.utils.function_calling import convert_to_openai_function from langgraph.graph import StateGraph, END, MessagesState # MessagesState是LangGraph内置的消息状态类 from langgraph.prebuilt import ToolNode # 导入LLM（以Deepseek为例，可替换为百度千帆/硅基流动等，用法一致） from langchain_openai import ChatOpenAI import os # 2. 初始化LLM（需提前配置环境变量 DEEPSEEK_API_KEY） llm = ChatOpenAI( model="deepseek-chat", api_key=os.getenv("DEEPSEEK_API_KEY", ""), base_url="https://api.deepseek.com/v1", streaming=True, # 支持流式输出 temperature=0.3 # 降低随机性，让工具调用更稳定 ) # 3. 定义工具：用@tool装饰器将普通函数转为AI可识别的工具 @tool # 核心装饰器：自动生成工具描述，供LLM判断是否调用 def get_weather(query: str) -> List[str]: """ 用于获取指定地区、指定时间的天气信息（工具描述很重要！LLM靠这个判断是否调用） :param query: 查询条件，格式示例："北京 今明两天天气"、"上海 明天是否下雨" :return: 天气信息列表，包含每天的天气描述 """ # 这里是模拟工具返回（真实场景替换为调用天气API，如高德/百度天气接口） if "今明两天" in query or "今天" in query or "明天" in query: return ["今天天气晴朗，温度20~28℃，微风", "明天天气多云转晴，温度22~30℃，南风3级"] elif "后天" in query: return ["后天天气小雨，温度18~25℃，东北风2级"] else: return [f"已查询到：{query} 的天气为晴朗，温度20~28℃（模拟数据）"] # 工具列表：可添加多个工具（如搜索、计算等） tools = [get_weather] # 4. 绑定工具到LLM：告诉模型"你现在拥有这些工具" # bind_tools：LangChain的工具绑定方法，让LLM能识别工具并生成工具调用指令 llm_with_tools = llm.bind_tools(tools) # convert_to_openai_function：将工具转为OpenAI格式的函数描述（兼容多数LLM） functions = [convert_to_openai_function(tool) for tool in tools] # 5. 定义对话节点（chat_bot）：让模型判断"要不要调工具" async def chat_bot(state: MessagesState): """ 核心思考节点：接收对话状态，让LLM判断是否调用工具，或直接生成回复 :param state: 流程状态，包含messages（对话历史） :return: 更新后的状态（添加LLM的思考结果） """ # 从状态中获取完整对话历史 messages = state["messages"] # 调用绑定工具的LLM，关键参数说明： # ainvoke：异步调用（适配流式和异步流程） # functions：工具的OpenAI格式描述，供LLM参考 # function_call="auto"：让LLM自主决定：直接回复 或 调用工具 response = await llm_with_tools.ainvoke( messages, functions=functions, function_call="auto" ) # 返回更新后的状态：将LLM的响应（思考结果）添加到对话历史 return {"messages": [response]} # 6. 定义路由节点（tool_router）：判断"下一步去哪"（分流逻辑） def tool_router(state: MessagesState) -> Literal["tools", "__end__"]: """ 流程路由（类似交通交警）：根据LLM的输出，决定下一个节点 :param state: 流程状态 :return: 下一个节点名称（"tools" 或 END） """ # 获取最新一条消息（LLM的思考结果） messages = state["messages"] last_message = messages[-1] # 关键判断：如果最新消息包含tool_calls（工具调用指令），则跳转到工具节点 if last_message.tool_calls: return "tools" # 去工具节点执行工具调用 return END # 没有工具调用，直接结束流程 # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) # 8. 编排完整流程（状态图）：将节点和路由串成流水线 # StateGraph(MessagesState)：创建状态图，指定状态格式为MessagesState（内置的消息列表结构） workflow = StateGraph(MessagesState) # 8.1 添加节点 workflow.add_node("chat_bot", chat_bot) # 思考节点：判断是否调工具 workflow.add_node("tools", tool_node) # 工具节点：执行工具调用 # 8.2 设置流程入口：流程从chat_bot节点开始（用户提问后先让模型思考） workflow.set_entry_point("chat_bot") # 8.3 定义节点流转规则 # 工具节点执行完成后，返回chat_bot节点：让模型基于工具结果生成最终回复 workflow.add_edge("tools", "chat_bot") # 条件流转：从chat_bot节点出发，由tool_router决定下一步 workflow.add_conditional_edges( source="chat_bot", # 起点：chat_bot节点 condition=tool_router, # 条件判断函数：tool_router # 无需手动映射目标节点（tool_router直接返回节点名） ) # 8.4 编译流程：生成可执行的流程图应用 app_graph = workflow.compile() # 9. 测试：流式运行流程（实时查看每一步输出） async def run_streaming_demo(): """流式运行工具调用流程，模拟用户交互""" # 初始对话状态：系统提示 + 用户提问 initial_messages = [ SystemMessage(content="你是一个智能助手，能回答问题和调用工具。" "如果需要查询天气，请调用get_weather工具，不要瞎编数据。"), HumanMessage(content="帮我查一下深圳今明两天的天气，谢谢～") ] initial_state = {"messages": initial_messages} print("=== 工具调用流程启动（流式输出）===") print(f"User: {initial_messages[-1].content}") print("Assistant: ", end="", flush=True) # 异步流式迭代流程输出（stream_mode=\'messages\'：按消息粒度输出，便于调试） async for event in app_graph.astream(initial_state, stream_mode=\'messages\'): # 解析事件：LangGraph的流式事件是元组，第一个元素是消息块 if isinstance(event, tuple): chunk = event[0] # 只打印AI的回复内容（过滤工具调用、系统消息等中间过程） if isinstance(chunk, HumanMessage): continue # 跳过用户消息 elif isinstance(chunk, ToolMessage): # 工具调用结果（用户不可见，可选打印用于调试） print(f"\\n[调试] 工具返回结果：{chunk.content}", flush=True) elif chunk.type == \'AIMessageChunk\': # 流式输出AI的最终回复（打字机效果） print(chunk.content, end="", flush=True) print("\\n=== 流程结束 ===") # 10. 运行测试（异步函数需用asyncio.run启动） if __name__ == "__main__": asyncio.run(run_streaming_demo()) \n```\n\n接下来，开始代码核心组件详解（按执行顺序）\n\n## 3.1、先搞个“假”天气工具：让AI知道它能干啥\n\n**痛点：**模型不知道自己有啥技能，就像助理没说明书，啥也不会干。\n\n**方案：**用 `@tool` 给函数打标签，变成 AI 能识别的“工具”。\n\n```\n # 3. 定义工具：用@tool装饰器将普通函数转为AI可识别的工具 @tool # 核心装饰器：自动生成工具描述，供LLM判断是否调用 def get_weather(query: str) -> List[str]: """ 用于获取指定地区、指定时间的天气信息（工具描述很重要！LLM靠这个判断是否调用） :param query: 查询条件，格式示例："北京 今明两天天气"、"上海 明天是否下雨" :return: 天气信息列表，包含每天的天气描述 """ # 这里是模拟工具返回（真实场景替换为调用天气API，如高德/百度天气接口） if "今明两天" in query or "今天" in query or "明天" in query: return ["今天天气晴朗，温度20~28℃，微风", "明天天气多云转晴，温度22~30℃，南风3级"] elif "后天" in query: return ["后天天气小雨，温度18~25℃，东北风2级"] else: return [f"已查询到：{query} 的天气为晴朗，温度20~28℃（模拟数据）"] # 工具列表：可添加多个工具（如搜索、计算等） tools = [get_weather] # 4. 绑定工具到LLM：告诉模型"你现在拥有这些工具" # bind_tools：LangChain的工具绑定方法，让LLM能识别工具并生成工具调用指令 llm_with_tools = llm.bind_tools(tools) # convert_to_openai_function：将工具转为OpenAI格式的函数描述（兼容多数LLM） functions = [convert_to_openai_function(tool) for tool in tools] # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) \n```\n\n* `@tool`：把普通函数包装成 AI 工具\n* `bind_tools`：告诉模型“你现在会这些技能”\n* `ToolNode`：负责真正执行工具调用，相当于给机器人配了“工具箱 + 操作员”\n\n**LLM 工具绑定（`bind_tools`）**\n\n* **`llm.bind_tools(tools)`**：给 LLM “挂载” 工具，让模型知道自己拥有这些能力，能生成符合工具调用格式的指令（如`tool_calls: [{"name": "get_weather", "args": {"query": "深圳今明两天天气"}}]`）\n* **`convert_to_openai_function`**：将工具转为 OpenAI 标准的函数描述格式，兼容绝大多数支持工具调用的 LLM（避免因模型格式差异导致调用失败）\n\n**创建 工具执行节点 ToolNode**\n\nToolNode 角色 是 流程的 “手脚”，负责执行工具调用的全流程（无需手动写解析逻辑）\n\nToolNode 内置逻辑：\n\n**(1) 解析 LLM 的`tool_calls`指令（提取工具名、参数）**\n\n**(2) 调用对应的工具函数（如`get_weather("深圳今明两天天气")`）**\n\n**(3) 将工具返回结果包装为`ToolMessage`（LangChain 的工具消息类型）**\n\n**(4) 自动将`ToolMessage`添加到对话历史，供后续节点使用**\n\n**ToolNode 优势**：无需关心工具调用的细节（如参数解析、异常处理），LangGraph 已封装好\n\n## 3.2、改造 chat\\_bot：让模型学会“要不要动手”\n\n**痛点：**模型只会硬答，不会判断“这事我能不能办”，要么瞎编，要么说“我不知道”。\n\n**方案：**让它输出“我要调哪个工具”，而不是直接回复。\n\n```\n # 5. 定义对话节点（chat_bot）：让模型判断"要不要调工具" async def chat_bot(state: MessagesState): """ 核心思考节点：接收对话状态，让LLM判断是否调用工具，或直接生成回复 :param state: 流程状态，包含messages（对话历史） :return: 更新后的状态（添加LLM的思考结果） """ # 从状态中获取完整对话历史 messages = state["messages"] # 调用绑定工具的LLM，关键参数说明： # ainvoke：异步调用（适配流式和异步流程） # functions：工具的OpenAI格式描述，供LLM参考 # function_call="auto"：让LLM自主决定：直接回复 或 调用工具 response = await llm_with_tools.ainvoke( messages, functions=functions, function_call="auto" ) # 返回更新后的状态：将LLM的响应（思考结果）添加到对话历史 return {"messages": [response]} \n```\n\n关键是 `function_call="auto"`：让模型自己决定是否调工具\n\n它不再嘴硬，而是说：“我要调 `get_weather`，参数是‘今明两天天气’”\n\n## 3.3、加个 add\\_conditional\\_edges 条件边，实现节点路由\n\n**痛点：**模型有时直接回，有时要调工具，系统懵了：接下来该干啥？\n\n**方案：**加个判断器，看消息里有没有 `tool_calls`，有就调工具，没有就结束。\n\n```\n # 6. 定义路由节点（tool_router）：判断"下一步去哪"（分流逻辑） def tool_router(state: MessagesState) -> Literal["tools", "__end__"]: """ 流程路由（类似交通交警）：根据LLM的输出，决定下一个节点 :param state: 流程状态 :return: 下一个节点名称（"tools" 或 END） """ # 获取最新一条消息（LLM的思考结果） messages = state["messages"] last_message = messages[-1] # 关键判断：如果最新消息包含tool_calls（工具调用指令），则跳转到工具节点 if last_message.tool_calls: return "tools" # 去工具节点 tool_node 执行工具调用 return END # 没有工具调用，直接结束流程 # 7. 创建工具节点（ToolNode）：执行工具调用的"操作员" # ToolNode是LangGraph预构建节点，功能：解析tool_calls → 调用对应工具 → 生成ToolMessage tool_node = ToolNode(tools) # 8. 编排完整流程（状态图）：将节点和路由串成流水线 # StateGraph(MessagesState)：创建状态图，指定状态格式为MessagesState（内置的消息列表结构） workflow = StateGraph(MessagesState) # 8.1 添加节点 workflow.add_node("chat_bot", chat_bot) # 思考节点：判断是否调工具 workflow.add_node("tools", tool_node) # 工具节点：执行工具调用 # 条件流转：从chat_bot节点出发，由tool_router决定下一步 workflow.add_conditional_edges( source="chat_bot", # 起点：chat_bot节点 condition=tool_router, # 条件判断函数：tool_router # 无需手动映射目标节点（tool_router直接返回节点名） ) \n```\n\n核心节点：`chat_bot`（思考节点）\n\n核心节点：`tool_node`（工具节点）\n\n路由条件：`tool_router`（分流逻辑）\n\n**路由条件角色**：流程的 “交通警察”，解决 “下一步去哪” 的问题\n\n**路由条件角色**：判断逻辑， 检查 LLM 的最新输出是否包含 tool\\_calls（工具调用指令）：\n\n* 有 → 跳转到`tools`节点执行工具调用\n* 无 → 直接结束流程（返回最终回复）\n\n**返回值**：严格指定为`Literal["tools", "__end__"]`，确保类型安全，避免流转错误\n\n流程编排（`StateGraph`）核心逻辑：构建 “思考→判断→执行→再思考” 的闭环：\n\n```\n 入口 → chat_bot（思考）→ tool_router（判断）→ ① 无工具调用 → 结束 ② 有工具调用 → tools（执行）→ chat_bot（基于工具结果生成回复）→ 结束 \n```\n\n回顾langgraph 关键 API：\n\n* `add_node`：添加节点（思考节点、工具节点）\n* `set_entry_point`：设置流程入口（从`chat_bot`开始）\n* `add_edge`：固定流转（工具执行后返回`chat_bot`）\n* `add_conditional_edges`：条件流转（由`tool_router`决定下一步）\n\n## 3.4、工具到底怎么被调的？拆开看流程\n\n**痛点：**中间环节太多，容易断链，结果丢了都不知道。\n\n**方案：**`ToolNode` 一把梭， 解析 → 执行 → 返回结果，全自动接回对话。\n\n当模型输出：\n\n```\n tool_calls=[{\'name\': \'get_weather\', \'args\': {\'query\': \'今明两天天气\'}}] \n```\n\n→ 路由发现 `tool_calls` → 跳转到 `tool_node`\n\n→ 自动调本地方法 `get_weather()`\n\n→ 结果作为 `ToolMessage` 写入历史\n\n→ 再回到 `chat_bot`，模型结合真实数据生成最终回复\n\n## 3.5 、串成完整工作流：大脑 + 手脚 合体\n\n**痛点：**逻辑散，不成环，难维护。\n\n**方案：**用状态图把“思考→判断→执行→再思考”串成自动流水线。\n\n```\n workflow = StateGraph(MessagesState) workflow.add_node("chat_bot", chat_bot) workflow.set_entry_point("chat_bot") workflow.add_node("tools", tool_node) workflow.add_edge("tools", "chat_bot") workflow.add_conditional_edges( "chat_bot", tool_router, ) app_graph = workflow.compile() \n```\n\n> 两条路径自动切换：\n\n* 直接回答 → 结束\n* 需查数据 → 调工具 → 回模型 → 出答案\n\n相当于给AI装上了“手脚”，从嘴炮王变实干家\n\n## 3.6 、测试运行：看看它是怎么一步步干活的\n\n**痛点：**看不到过程，出问题没法 debug。\n\n**方案：**开流式输出，实时监听每一步。\n\n```\n async def run_streaming_chain(): messages = [ SystemMessage(content="你是一个智能助手..."), HumanMessage(content="帮我查一下今明两天的天气") ] initial_state = {"messages": messages} async for event in app_graph.astream(initial_state, stream_mode=\'messages\'): if isinstance(event, tuple): chunk = event[0] if chunk.type == \'AIMessageChunk\': print(\'event里监听到的流式输出------>\', chunk.content) \n```\n\n`stream_mode=\'messages\'`：逐字打印输出，调试神器\n\n## 3.7、自验证：看看结果长啥样\n\n**痛点：**用户以为模型“天生就知道”，根本看不出用了工具。\n\n**方案：**通过日志看清全过程：指令 → 执行 → 回复。\n\n用户输入：\n\n> 帮我查一下今明两天的天气\n\n模型第一步不是回答，而是发指令：\n\n```\n last_message.tool_calls ------> [{\'name\': \'get_weather\', \'args\': {\'query\': \'今明两天天气\'}, ...}] \n```\n\n→ 触发工具 → 获取真实数据 → 模型整合输出：\n\n> “好的，我来帮您查询一下。经过查询，今天天气晴朗，温度20度，明天天气多云，温度25度……”\n\n> 用户无感，但背后已完成一次“AI + 工具”的协同作业\n\n### 核心流程图\n\n> 两条路自动走：能干就干，不能就调工具，全程无人干预\n\n### 总结：工具调用核心五步\n\n| 步骤 | 组件 | 干啥用 |\n| --- | --- | --- |\n| 1. 定义工具 | `@tool` | 把函数变成 AI 能调的“能力” |\n| 2. 绑定工具 | `bind_tools()` | 告诉模型：“你现在有这技能” |\n| 3. 判断是否调 | `tool_router` | 看有没有 `tool_calls`，决定走哪条路 |\n| 4. 执行工具 | `ToolNode` | 自动跑函数，拿结果回来 |\n| 5. 整合回复 | 回到 `chat_bot` | 模型基于真实数据生成人话 |\n\n> 从此，机器人从“只会说”升级为“又能说又能做”\n\n可接入数据库、搜索、计算器……想连啥连啥。\n\n## 第四章： 为langgraph应用 添加记忆功能\n\n在写聊天机器人时，没有记忆就像金鱼，游两下就忘了自己是谁。\n\n咱们上一节给机器人装了工具，能查资料、能干活，但它还是记不住聊过啥。\n\n这不行啊，用户说一遍名字，下次还得重新介绍，体验直接掉地上。\n\n所以这一节，咱给它加上“脑子”，让它记住对话历史，真正实现多轮连贯交流。\n\n### 4.1. 添加记忆功能\n\n加记忆功能，不需要额外装包，LangGraph 自带这个本事。\n\n它用的是 `langgraph.checkpoint.memory` 模块里的 `MemorySaver`，说白了就是个内存快照工具。\n\n每次对话一结束，它就把当前状态拍个照存起来，下次接着用。\n\n开发阶段用它正合适，简单又省事，就跟手机临时存图一样。\n\n#### 解决方案：\n\n用 LangGraph 的 `MemorySaver` 当临时笔记本，自动存对话记录。 靠 `thread_id` 区分不同用户，互不串台。\n\nLangGraph 自带 `MemorySaver` 它就像一个会自动记笔记的助理：每次聊完记一笔，下次见你先翻本子再开口。\n\n#### 怎么加记忆？三步搞定：\n\n**（1）定义状态：消息列表支持累积**\n\n```\n class State(TypedDict): messages: Annotated[list, add_messages] \n```\n\n> `add_messages` 是关键：新消息不是覆盖，而是追加到历史里，上下文不断档。\n\n**（2）创建 MemorySaver，当内存记事本**\n\n```\n memory = MemorySaver() \n```\n\n**（3）编译图时传 checkpointer，开启自动保存**\n\n```\n graph = graph_builder.compile(checkpointer=memory) \n```\n\n每次对话结束，系统自动拍个“状态快照”存进去，下次按 `thread_id` 找回来继续聊。\n\n#### 多人聊天不串台？靠 thread\\_id\n\n每个用户分配唯一 `thread_id`，相当于独立聊天室：\n\n```\n config = {"configurable": {"thread_id": "1"}} \n```\n\n* 用户A用 `"1"`，B用 `"2"` → 各聊各的，不干扰\n* 回到 `"1"` → 记忆还在，接着上次聊\n\n注意：开发阶段 用 `MemorySaver`（内存存储），重启就丢。生产建议换 `SqliteSaver` 或数据库持久化。：\n\n### 4.2 使用记忆增强聊天机器人\n\n新建一个文件叫 `3-add-memory.py`，开始搞有记忆的机器人。\n\n```\n """LangGraph 教程: 添加记忆功能的聊天机器人 本示例展示了如何使用 LangGraph 的检查点功能为聊天机器人添加记忆功能， 使其能够记住对话历史并在多轮对话中保持上下文。 """ from typing import Annotated from langchain.chat_models import init_chat_model from langchain_tavily import TavilySearch from langchain_core.messages import BaseMessage from typing_extensions import TypedDict # 导入 MemorySaver 用于实现记忆功能 from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph from langgraph.graph.message import add_messages from langgraph.prebuilt import ToolNode, tools_condition import os from dotenv import load_dotenv # 加载.env文件中的环境变量 load_dotenv() # 定义状态类型，使用 add_messages 注解来自动合并消息列表 class State(TypedDict): messages: Annotated[list, add_messages] # 消息列表将使用 add_messages reducer 自动合并 # 初始化 DeepSeek 聊天模型 llm = init_chat_model( "deepseek-chat", # 使用DeepSeek模型 api_key=os.environ.get("DEEPSEEK_API_KEY") # 从环境变量中获取API密钥 ) # 创建状态图构建器 graph_builder = StateGraph(State) # 初始化Tavily搜索工具 print("\\n初始化Tavily搜索工具...") tool = TavilySearch(max_results=2) # 设置最多返回2个搜索结果 tools = [tool] # 将工具绑定到LLM llm_with_tools = llm.bind_tools(tools) # 定义聊天机器人节点函数 def chatbot(state: State): """LLM节点函数，处理用户输入并生成响应""" return {"messages": [llm_with_tools.invoke(state["messages"])]} # 添加聊天机器人节点 graph_builder.add_node("chatbot", chatbot) # 添加工具节点 tool_node = ToolNode(tools=[tool]) graph_builder.add_node("tools", tool_node) # 添加条件边 graph_builder.add_conditional_edges( "chatbot", tools_condition, ) # 工具调用完成后，返回到聊天机器人节点 graph_builder.add_edge("tools", "chatbot") graph_builder.set_entry_point("chatbot") print("\\n构建图并添加记忆功能...") # 创建内存保存器 print("\\n创建 MemorySaver 实例作为检查点保存器...") memory = MemorySaver() # 在内存中保存状态，适用于开发和测试 # 使用内存保存器编译图 print("使用检查点保存器编译图...") graph = graph_builder.compile(checkpointer=memory) # 将内存保存器传递给图 # 打印图结构 print("\\n图结构如下：") print(graph.get_graph().draw_mermaid()) # 定义对话线程ID print("\\n设置对话线程 ID = \'1\'...") config = {"configurable": {"thread_id": "1"}} # 使用线程ID来标识和区分不同的对话 # 示例 1: 第一次对话 print("\\n示例 1: 第一次对话 - 用户介绍自己") user_input = "Hi there! My name is Will." print(f"\\n用户输入: \'{user_input}\'") # 注意: config 是 stream() 函数的第二个参数! print("使用线程 ID \'1\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 传递包含 thread_id 的配置 stream_mode="values", ) print("\\n助理回应:") for event in events: event["messages"][-1].pretty_print() # 打印助理的回应 # 示例 2: 测试记忆功能 print("\\n\\n示例 2: 第二次对话 - 测试记忆功能") user_input = "Remember my name?" print(f"\\n用户输入: \'{user_input}\'") # 使用相同的线程ID再次调用图 print("使用相同的线程 ID \'1\' 再次调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 使用相同的配置，图将加载之前保存的状态 stream_mode="values", ) print("\\n助理回应 (应该记得用户名字):") for event in events: event["messages"][-1].pretty_print() # 示例 3: 新对话线程 print("\\n\\n示例 3: 新对话线程 - 测试线程隔离") print("创建新的线程 ID = \'2\'...") # 使用不同的线程ID print("使用新的线程 ID \'2\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, {"configurable": {"thread_id": "2"}}, # 使用新的线程ID stream_mode="values", ) print("\\n助理回应 (不应该记得用户名字):") for event in events: event["messages"][-1].pretty_print() # 示例 4: 返回第一个线程 print("\\n\\n示例 4: 返回第一个线程 - 验证记忆持久性") print(f"\\n用户输入: \'{user_input}\'") # 再次使用第一个线程ID print("再次使用线程 ID \'1\' 调用图...") events = graph.stream( {"messages": [{"role": "user", "content": user_input}]}, config, # 使用原始线程ID stream_mode="values", ) print("\\n助理回应 (应该仍然记得用户名字):") for event in events: event["messages"][-1].pretty_print() \n```\n\n### 4.3 代码解析\n\n#### 关键改动点\n\n```\n from langgraph.checkpoint.memory import MemorySaver \n```\n\n这是新加的核心依赖，相当于给机器人配了个“小本本”。\n\n以前每次对话都像第一次见面，现在它会翻本本看看你们之前聊了啥。\n\n```\n memory = MemorySaver() graph = graph_builder.compile(checkpointer=memory) \n```\n\n这两行是关键操作：\n\n第一行，创建一个内存记录员；\n\n第二行，把这个记录员塞进图里，让它自动拍照存档每一步状态。\n\n只要你不关服务，它就能一直记住。\n\n> 生产环境别用这个，得换成数据库版的 `SqliteSaver` 或 `PostgresSaver`，不然重启就全丢了。\n\n```\n config = {"configurable": {"thread_id": "1"}} \n```\n\n这个 `thread_id` 就像是对话身份证。 同一个 ID，读的是同一份记忆；换个 ID，就是全新对话。\n\n多用户场景下，靠它实现隔离，不会张冠李戴。\n\n```\n events = graph.stream(..., config, stream_mode="values") \n```\n\n注意啊，`config` 是第二个参数，位置不能错。\n\n传进去之后，图就知道该去哪找之前的快照，而不是从头开始。\n\n### 4.4 运行聊天机器人\n\n运行命令：\n\n```\n uv run 3-add-memory.py \n```\n\n你会看到输出长这样：\n\n```\n 设置对话线程 ID = \'1\'... 示例 1: 第一次对话 - 用户介绍自己 用户输入: \'Hi there! My name is Will.\' ... Assistant: Hi Will! It\'s great to meet you. How can I assist you today? 示例 2: 第二次对话 - 测试记忆功能 用户输入: \'Remember my name?\' ... Assistant: Of course, Will! I’ll remember your name for the rest of our conversation. 示例 3: 新对话线程 - 测试线程隔离 用户输入: \'Remember my name?\' ... Assistant: I don’t have the ability to remember personal details like names between interactions. 示例 4: 返回第一个线程 - 验证记忆持久性 用户输入: \'Remember my name?\' ... Assistant: Got it, Will! I’ll keep your name in mind while we chat. \n```\n\n你看：\n\n* 同一个线程 ID，第二次还能叫出“Will”；\n* 换个 ID，立马变脸不认人；\n* 回到原来的 ID，记忆还在，没丢。\n\n这就是线程级记忆的威力。\n\n### 4.5 LangGraph 中 thread\\_id（线程 ID）\n\n在 LangGraph 中，**thread\\_id（线程 ID）的隔离能力核心依赖「检查点（Checkpointer）」的状态分区机制**——\n\n简单说：`thread_id` 是状态的「唯一索引」，不同 `thread_id` 对应独立的对话状态存储，彼此互不干扰。\n\n简单记：**LangGraph 的 thread\\_id 管 “对话记忆”，理论上叫做memory\\_id，和 Python “并发干活”的线程 ID ，八竿子打不着**。\n\n#### （1）先明确memory\\_id核心前提\n\n要理解隔离，先搞懂两个关键概念：\n\n**(1) 检查点（Checkpoint）：LangGraph 中用于「持久化存储对话状态」的核心机制，本质是「状态快照」—— 每次流程执行后，会把最新的 `state`（比如 `messages` 对话历史）保存起来，下次执行时可恢复。**\n\n**(2) thread\\_id（理论的memory\\_id）：给「每一组独立对话」分配的唯一标识，相当于对话的「身份证」，用于区分不同用户 / 不同会话的状态。**\n\n#### （2）隔离的核心原理：`thread_id` 作为状态的「隔离键」\n\nLangGraph 的 `Checkpointer`（这里用的是 `MemorySaver`）会把所有对话状态，按照 `thread_id` 进行「分区存储」—— 不同 `thread_id` 的状态存在完全独立的「命名空间」里，彼此不会覆盖或混淆。\n\n可以把它想象成「文件柜」：\n\n* `Checkpointer` = 一个文件柜\n* `thread_id` = 文件柜里的「抽屉编号」\n* 每个抽屉（`thread_id`）里存放对应对话的状态（`messages` 等）\n* 打开抽屉时，只能看到当前 `thread_id` 下的文件（状态），看不到其他抽屉的内容\n\n关键逻辑：状态的「存储」与「读取」都绑定 `thread_id`\n\n**(1) 存储时：流程执行后，`Checkpointer` 会把更新后的 `state` 与传入的 `thread_id` 绑定，存入对应分区；**\n\n**(2) 读取时：下次调用流程传入相同 `thread_id`，`Checkpointer` 会自动加载该分区下的历史状态，继续推进对话；**\n\n**(3) 隔离时：不同 `thread_id` 对应不同分区，加载状态时只会读取自己分区的内容，自然实现隔离。**\n\n#### （3）核心组件：`MemorySaver` 如何实现分区存储？\n\n代码中用的 `MemorySaver` 是 LangGraph 提供的「内存级检查点实现」（适用于开发测试），其内部维护了一个「字典结构」，专门用于按 `thread_id` 分区存储状态。\n\n`MemorySaver` 的简化内部逻辑（伪代码）\n\n```\n class MemorySaver: def __init__(self): # 核心存储结构：key=thread_id，value=该线程的最新状态（checkpoint） self.storage = {} # 保存状态：绑定 thread_id def save_checkpoint(self, thread_id, state): self.storage[thread_id] = state # 按 thread_id 存入对应键值对 # 读取状态：按 thread_id 提取 def get_checkpoint(self, thread_id): return self.storage.get(thread_id, None) # 只返回当前 thread_id 的状态 \n```\n\n结合 的代码看：\n\n* 当 传入 `config = {"configurable": {"thread_id": "1"}}` 时，`MemorySaver` 会把对话状态存入 `self.storage["1"]`；\n* 后续传入相同 `thread_id="1"`，会读取 `self.storage["1"]` 中的历史状态（比如第一次对话的 `Hi there! My name is Will.`）；\n* 当传入 `thread_id="2"` 时，会读取 `self.storage["2"]`（初始为空，所以不记得名字）。\n\n#### (4)、完整执行流程：用你的代码示例验证隔离逻辑\n\n以你代码中的 4 个示例为例，一步步看 `thread_id` 如何隔离：\n\n示例 1：第一次对话（thread\\_id="1"）\n\n**(1) 传入 `config={"configurable": {"thread_id": "1"}}` 和用户输入 `Hi there! My name is Will.`；**\n\n**(2) `graph.stream()` 启动流程，`Checkpointer` 检查 `thread_id="1"` 的存储：无历史状态；**\n\n**(3) 执行 `chatbot` 节点，生成回复（比如 `Hello Will! Nice to meet you!`）；**\n\n**(4) `Checkpointer` 把更新后的 `state`（包含用户输入 + AI 回复）存入 `storage["1"]`；**\n\n**(5) 输出回复，流程结束。**\n\n示例 2：测试记忆（thread\\_id="1"）\n\n**(1) 传入相同 `config`（`thread_id="1"`）和用户输入 `Remember my name?`；**\n\n**(2) `Checkpointer` 读取 `storage["1"]` 中的历史状态（包含第一次的对话）；**\n\n**(3) `chatbot` 节点基于历史状态生成回复（`Yes! Your name is Will.`）；**\n\n**(4) 更新后的状态（追加本次对话）再次存入 `storage["1"]`；**\n\n**(5) 输出回复，记忆生效。**\n\n示例 3：新线程（thread\\_id="2"）\n\n**(1) 传入 `config={"configurable": {"thread_id": "2"}}` 和用户输入 `Remember my name?`；**\n\n**(2) `Checkpointer` 检查 `storage["2"]`：无历史状态（新线程）；**\n\n**(3) `chatbot` 节点没有历史上下文，生成回复（`I don\'t think we\'ve met before. Could you tell me your name?`）；**\n\n**(4) 状态存入 `storage["2"]`；**\n\n**(5) 输出回复，实现隔离（不记得名字）。**\n\n示例 4：返回旧线程（thread\\_id="1"）\n\n**(1) 再次传入 `thread_id="1"`，`Checkpointer` 读取 `storage["1"]` 中的历史状态（包含前两次对话）；**\n\n**(2) `chatbot` 节点基于历史状态，仍然记得名字，生成正确回复；**\n\n**(3) 验证记忆持久性和隔离性。**\n\n#### (4)、关键细节：为什么必须通过 `config` 传入 `thread_id`？\n\nLangGraph 中，`config` 是「流程配置的统一入口」，`configurable` 字段用于传递「可配置的全局参数」，其中 `thread_id` 是 LangGraph 约定的「状态隔离关键字」—— 只有通过 `config={"configurable": {"thread_id": "xxx"}}` 传入，`Checkpointer` 才能识别并用于状态分区。\n\n如果不传入 `thread_id`：\n\n* `Checkpointer` 会使用默认的 `thread_id`（通常是一个随机值或固定值）；\n* 所有对话会共享同一个状态，无法实现隔离（比如新用户会看到上一个用户的对话历史）。\n\n#### (6)、生产环境扩展：除了 MemorySaver，还有哪些 Checkpointer？\n\n`MemorySaver` 是内存级实现，程序重启后状态会丢失，适用于开发测试。\n\n生产环境中，为了实现「持久化隔离」，可以使用以下 `Checkpointer`：\n\n**(1) `RedisSaver`：基于 Redis 存储状态，支持分布式部署，`thread_id` 作为 Redis 的 key；**\n\n**(2) `SQLiteSaver`：基于 SQLite 数据库存储，适合轻量生产环境；**\n\n**(3) `PostgresSaver`：基于 PostgreSQL 数据库存储，适合大规模生产环境。**\n\n它们的隔离原理完全一致 —— 都是以 `thread_id` 作为「隔离键」，只是存储介质不同，确保不同对话的状态独立存储。\n\n#### (7)、核心总结\n\n`thread_id` 的隔离能力，本质是「Checkpointer 按 thread\\_id 分区存储对话状态」：\n\n**(1) `thread_id` 是对话的唯一标识，作为状态存储的「分区键」；**\n\n**(2) `Checkpointer`（如 `MemorySaver`）内部维护按 `thread_id` 划分的存储结构；**\n\n**(3) 相同 `thread_id` 复用历史状态，不同 `thread_id` 从零开始，实现完全隔离；**\n\n**(4) 隔离的核心价值：支持多用户 / 多会话同时使用，彼此不干扰，且能保持各自的对话上下文。**\n\n这种设计让 LangGraph 能够轻松支持「多用户聊天机器人」「会话级记忆」等场景，是生产级对话系统的核心能力之一。\n\n#### (8)、这个thread\\_id（线程 ID） 和 python的线程id 有关系吗\n\n**完全没有关系**！这是两个名字碰巧都带 “线程”，但语义、作用、实现逻辑完全独立的概念 —— 可以理解为 “同名不同姓”，毫无关联。\n\n| 对比维度 | LangGraph 的 `thread_id`（对话线程 ID） | Python 的 `threading.Thread.ident`（系统线程 ID） |\n| --- | --- | --- |\n| **核心作用** | 区分「不同的对话会话」，实现对话状态隔离（记忆隔离） | 区分「操作系统中的执行线程」，实现并发任务调度 |\n| **本质** | 对话的「唯一标识 / 身份证」（字符串 / 数字均可） | 操作系统分配给线程的「执行单元编号」（整数） |\n| **使用场景** | 多用户 / 多会话聊天机器人（比如用户 A 和用户 B 的对话互不干扰） | Python 程序中并发执行多个任务（比如同时下载文件 + 处理数据） |\n| **存储关联** | 与 LangGraph 的 `Checkpointer`（状态存储）绑定，用于分区存储对话历史 | 与操作系统的线程调度器绑定，用于标识执行上下文 |\n| **生命周期** | 随对话存在（可手动指定，比如 `thread_id="user_123"` 可长期有效） | 随 Python 线程启动而创建，线程结束而销毁 |\n| **示例取值** | `"1"`、`"user_888"`、`"session_xyz"`（自定义字符串 / 数字） | `140703324567360`（操作系统分配的整数，不可自定义） |\n\n通俗解释：两个 “线程” 的本质区别\n\n**(1) LangGraph 的 `thread_id`：对话的 “专属文件夹”**\n\n* 它不是 “执行任务的线程”，而是「对话会话的唯一标识」。\n* 类比：你在聊天软件上和 A 朋友、B 朋友的对话，会存在两个独立的 “聊天窗口”（文件夹），`thread_id` 就是这两个窗口的编号 —— 确保你和 A 的聊天记录不会出现在和 B 的窗口里。\n* 核心：**用于 “数据隔离”（对话状态 / 记忆隔离）**。\n\n**(2) Python 的线程 ID：程序的 “干活工人编号”**\n\n* 它是操作系统层面的「执行单元标识」，对应一个 “干活的工人”。\n* 类比：你让程序同时 “下载文件” 和 “处理数据”，操作系统会分配两个 “工人”（线程），每个工人有唯一编号（线程 ID），负责独立完成任务，互不干扰。\n* 核心：**用于 “并发执行”（任务调度隔离）**。\n\n（1）LangGraph 的 `thread_id`（对话隔离）\n\n```\n # 两个不同的对话会话，用 thread_id 隔离 config_user_a = {"configurable": {"thread_id": "user_a"}} # 用户A的对话标识 config_user_b = {"configurable": {"thread_id": "user_b"}} # 用户B的对话标识 # 两个会话的状态独立存储，互不干扰 graph.stream({"messages": [{"role": "user", "content": "我是A"}]}, config_user_a) graph.stream({"messages": [{"role": "user", "content": "我是B"}]}, config_user_b) \n```\n\n（2）Python 的线程 ID（并发执行）\n\n```\n import threading def task(name): # 获取当前 Python 线程的 ID（操作系统分配） print(f"任务 {name} 运行在 Python 线程 ID: {threading.get_ident()}") # 启动两个并发线程，各自有独立的线程 ID thread1 = threading.Thread(target=task, args=("下载文件",)) thread2 = threading.Thread(target=task, args=("处理数据",)) thread1.start() # 输出：任务 下载文件 运行在 Python 线程 ID: 140703324567360 thread2.start() # 输出：任务 处理数据 运行在 Python 线程 ID: 140703324567361 \n```\n\n**(1) 两者无任何技术关联：LangGraph 的 `thread_id` 是 LangGraph 框架自定义的「对话标识」，Python 的线程 ID 是操作系统分配的「执行单元标识」；**\n\n**(2) 作用完全不同：一个管「对话数据隔离」（记忆不混淆），一个管「程序并发执行」（任务不干扰）；**\n\n**(3) 可以叠加使用：比如用 Python 多线程同时处理 100 个用户的对话，每个用户的对话用独立的 `thread_id` 隔离状态 —— 此时 Python 线程 ID 是 “工人编号”，LangGraph 的 `thread_id` 是 “每个工人处理的对话文件夹编号”，互不冲突。**\n\n简单记：**LangGraph 的 thread\\_id 管 “对话记忆”，Python 的线程 ID 管 “并发干活”，八竿子打不着**。\n\n### 4.6 小结\n\n加了 `MemorySaver`，它就开始记事了，能维持上下文，像个正常人聊天。\n\n一是 `MemorySaver`，内存存状态，开发够用；\n\n二是 `thread_id`，区分不同用户的对话流；\n\n三是 `checkpointer`，让图自动保存和恢复。\n\n> 流程：读历史 → 推理 → 工具调用 → 保存状态。一次完整带记忆的交互。\n\n### 第四章小节：LangGraph总结与展望\n\n整个过程像搭积木，一步接一步，核心靠的就是 **LangGraph** —— 它像个智能调度员，把各个功能串起来，让机器人真正“活”了起来。\n\n### LangGraph 五件事 小节\n\n**(1) LangGraph 是怎么工作的？**\n\n**(2) 上下文怎么记住？**\n\n**(3) 怎么让它会用工具？**\n\n**(4) 跨会话记忆怎么实现？**\n\n**(5) 多人聊天不串台？**\n\n> 图解：识别身份 → 加载记忆 → 判断是否需查资料 → 更新状态并回复。颜色代表：初始化（绿）、线程处理（蓝）、工具执行（橙）、状态操作（紫），数据流向一目了然。\n\n### 4.6 LangGraph 展望\n\n以 LangGraph 状态机为核心，扩展持久化、协同和人工干预能力，升级成可落地的AI系统。\n\n**(1) 长期记忆落地**\n\n当前状态存在内存里，重启全丢。应该接入 Redis 或 PostgreSQL，实现持久存储——把便签本换成保险柜。\n\n**(2) 关键操作让人把关**\n\n**(3) 多个AI组队干活**\n\n一个干不过来？那就组团。比如一个接待、一个查资料、一个写报告。用 LangGraph 编排多个智能体协作，打造“AI员工团队”。\n\n**(4) 对话支持“撤销”**\n\n**(5) 上生产环境扛高并发**\n\n本地跑得好，线上可能崩。要结合检查点 + 容器化部署，加上监控、容错、负载均衡，才能撑住真实流量。\n\nLangGraph 正在飞速进化，功能越来越多。但万变不离其宗：**用状态机管理AI流程**。掌握了这一点，你就拿到了构建高级AI系统的钥匙。\n\n## 第五章：langgraph 常见陷阱和解决方案\n\n> ......... 略5000字+\n>\n> ...................由于平台篇幅限制， 剩下的内容(5000字+)，请参参见原文地址\n\n#### 原始的内容，请参考 本文 的 原文 地址\n\n[本文 的 原文 地址](https://mp.weixin.qq.com/s/3Unzc4dHuYDwQyBgBaRKtA)\n\n## 第七章 大实操 预告 ：使用LangGraph从零构建多智能体AI系统：实现智能协作的完整指南\n\n尼恩团队即将推出， 基于langgraph的java 运维智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent\n\n在当今快速发展的AI领域，单一的大语言模型（LLM）已经难以满足复杂、多步骤的业务需求。多智能体（Multi-Agent）系统应运而生，它通过将多个专用的子智能体（sub-agent）组合起来，协同完成一项复杂任务，从而显著提升了系统的鲁棒性、可扩展性和问题解决能力。  \n 本文将以一个**智能 运维助手**的实际场景为例，深入探讨如何使用强大的[图计算框架， 从零开始构建一个功能完备的多Agent系统。\n\n我们将覆盖从基础架构设计、状态与记忆管理、人机回环 ），到最终使用开源可观测性平台 **LangFuse** 进行系统评测的全过程。\n\n* **指标查询Agent (metric Status Agent)** ：负责连接 Prometheus 数据库 ，查询的实时运行参数，如cpu、内存、jvm gc等。\n* **日志查询Agent (log search Agent)** ：负责连接 Elasticsearch\'日志数据库 ，查询的error 日志。\n* **问题定位Agent (question alanasys Agent)** ：负责根据日志和指标，分析的error 日志的根因。\n* **工单任务调度Agent (Maintenance Scheduler Agent)** ：负责与工单系统（Ticketing System）交互，根据故障信息创建、查询和更新维保任务。\n* **主管Agent (Supervisor)** ：作为总协调者，负责理解用户意图，并将任务路由给合适的子Agent。它不直接执行工具，而是“指挥”其他Agent工作。\n\n这个第七章 大实操 ，即将结合 尼恩团队后续 的langgraph **学习圣经推出，敬请期待。**\n\n* langgraph 学习圣经 （1）： 从0到1穿透 multi-agent多智能体 入门实战\n* langgraph 学习圣经 （2）： 从0到1穿透LangGraph架构与源码\n* langgraph 学习圣经（3） : 基于langgraph的java 运维多智能体， 包括 智能日志分析agent， 智能bug 定位agent， 智能指标分析agent 等\n\nposted @ 2025-12-28 21:38\xa0 [技术自由圈](https://www.cnblogs.com/crazymakercircle)\xa0 阅读(292)\xa0 评论(0)\xa0 \xa0 [收藏](javascript:void(0))\xa0 [举报](javascript:void(0))\n\n[刷新页面](#)[返回顶部](#top)\n\n[博客园](https://www.cnblogs.com/)  \xa0©\xa0 2004-2026   \n [浙公网安备 33010602011771号](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33010602011771) [浙ICP备2021040463号-3](https://beian.miit.gov.cn)\n\n ', 'doi': '', 'published_date': None, 'pdf_url': '', 'url': 'https://www.cnblogs.com/crazymakercircle/p/19412858', 'source': 'tavily', 'categories': ['web_search'], 'keywords': [], 'citations': 0, 'references': [], 'extra': {'score': 0.99986124, 'saved_path': '/home/qinshan/widthresearch/data/downloads/tavily_LangGraph .md'}}
