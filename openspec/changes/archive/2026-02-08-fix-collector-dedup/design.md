## Context

当前 `collector_first` 与 `collector_second` 都调用 `FileDeduplicator.deduplicate` 对同一目录做全量去重，无法表达“第二阶段必须让位于第一阶段结果”的优先级规则。两阶段去重的输入边界不清晰，导致 `execute_second` 的文档可能与 `collector_first` 重复但仍被保留。

## Goals / Non-Goals

**Goals:**
- 明确两阶段去重的输入范围与优先级规则。
- 第一阶段仅基于 `execute_first` 的文档做 TF-IDF 相似度去重，重复任意保留一份，不进行物理删除。
- 第二阶段对 `execute_second` 的文档先做内部去重，再与第一阶段结果做跨阶段去重，若重复则仅保留第一阶段结果，不进行物理删除。
- 保留现有 `Config.DOC_FILTER` 作为相似度阈值，维持一致的去重敏感度。

**Non-Goals:**
- 不调整 `ExecutorAgent` 的搜索与下载逻辑。
- 不引入新的外部依赖或替换现有 TF-IDF 实现。
- 不改变文档处理、向量化与检索流程的结构。

## Decisions

- **第一阶段去重范围**：仅以 `execute_first` 下载的文件集合为输入，按 TF-IDF 余弦相似度去重，保留任一结果。
  - 备选方案：继续对目录全量去重。否决原因：无法隔离第二阶段输入，且跨阶段优先级无法表达。

- **第二阶段优先级去重**：先对 `execute_second` 内部进行相似度去重，再将其与第一阶段保留结果进行比对；若判定为重复，仅输出第一阶段结果。
  - 备选方案：合并两阶段结果后统一去重。否决原因：无法保证第一阶段结果优先保留。

- **输出与统计**：沿用现有元数据结构与日志输出，补充区分“阶段内去重”和“跨阶段去重”的统计，并仅输出保留结果。
  - 备选方案：引入新的统计结构。否决原因：增加状态复杂度且无必要。

## Risks / Trade-offs

- **相似度误判** → 过度去重会丢失第二阶段的有效补充 → 保留现有阈值并记录跨阶段未输出明细，便于回溯。
- **跨阶段比较成本** → 大量文件时相似度计算开销上升 → 复用已有批处理能力，优先在阶段内收缩集合后再跨阶段比对。
- **重复项留存** → 不删除重复文件可能增加磁盘占用 → 仅在输出中保留非重复结果，便于后续流程可控。

## Migration Plan

- 无需数据迁移。
- 修改 `collector_first` 与 `collector_second` 逻辑后进行一次完整流程验证；若异常可回滚到当前单一去重实现。

## Open Questions

- 跨阶段去重是否需要额外的 URL/标题规则作为先验过滤？
- 对于极短文本或非文本文件，是否需要降级为 MD5 去重以避免相似度误判？
