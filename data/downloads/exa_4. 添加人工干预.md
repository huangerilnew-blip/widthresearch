# 4. æ·»åŠ äººå·¥å¹²é¢„ - LangChain æ¡†æ¶

**URL**:
https://github.langchain.ac.cn/langgraph/tutorials/get-started/4-human-in-the-loop

## å…ƒæ•°æ®
- å‘å¸ƒæ—¥æœŸ: 2025-03-01T00:00:00+00:00

## å®Œæ•´å†…å®¹
---
[è·³åˆ°å†…å®¹] 

# æ·»åŠ äººæœºå›åœˆæ§åˆ¶ [Â¶] 

æ™ºèƒ½ä½“å¯èƒ½ä¸å¯é ï¼Œéœ€è¦äººç±»è¾“å…¥æ‰èƒ½æˆåŠŸå®Œæˆä»»åŠ¡ã€‚åŒæ ·ï¼Œå¯¹äºæŸäº›æ“ä½œï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨è¿è¡Œå‰éœ€è¦äººç±»æ‰¹å‡†ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸè¿›è¡Œã€‚

LangGraph çš„ [æŒä¹…åŒ–] å±‚æ”¯æŒ **äººæœºå›åœˆ** å·¥ä½œæµï¼Œå…è®¸æ ¹æ®ç”¨æˆ·åé¦ˆæš‚åœå’Œæ¢å¤æ‰§è¡Œã€‚æ­¤åŠŸèƒ½çš„ä¸»è¦æ¥å£æ˜¯ [`interrupt`] å‡½æ•°ã€‚åœ¨èŠ‚ç‚¹å†…éƒ¨è°ƒç”¨ `interrupt` å°†æš‚åœæ‰§è¡Œã€‚å¯ä»¥é€šè¿‡ä¼ å…¥ä¸€ä¸ª [Command] æ¥æ¢å¤æ‰§è¡Œï¼ŒåŒæ—¶å¯ä»¥é™„å¸¦æ¥è‡ªäººç±»çš„æ–°è¾“å…¥ã€‚

`interrupt` åœ¨äººä½“å·¥ç¨‹å­¦ä¸Šç±»ä¼¼äº Python å†…ç½®çš„ `input()`ï¼Œä½† [æœ‰ä¸€äº›æ³¨æ„äº‹é¡¹] ã€‚

æ³¨æ„

æœ¬æ•™ç¨‹å»ºç«‹åœ¨ [æ·»åŠ è®°å¿†] çš„åŸºç¡€ä¸Šã€‚

## 1\. æ·»åŠ  `human_assistance` å·¥å…· [Â¶] 

ä» [å‘èŠå¤©æœºå™¨äººæ·»åŠ è®°å¿†] æ•™ç¨‹çš„ç°æœ‰ä»£ç å¼€å§‹ï¼Œå‘èŠå¤©æœºå™¨äººæ·»åŠ  `human_assistance` å·¥å…·ã€‚æ­¤å·¥å…·ä½¿ç”¨ `interrupt` æ¥æ¥æ”¶æ¥è‡ªäººç±»çš„ä¿¡æ¯ã€‚

è®©æˆ‘ä»¬é¦–å…ˆé€‰æ‹©ä¸€ä¸ªèŠå¤©æ¨¡å‹

OpenAIAnthropicAzureGoogle GeminiAWS Bedrock

```
pipinstall-U"langchain[openai]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["OPENAI_API_KEY"] = "sk-..."llm = init_chat_model("openai:gpt-4.1")
```

ğŸ‘‰ é˜…è¯» [OpenAI é›†æˆæ–‡æ¡£] 

```
pipinstall-U"langchain[anthropic]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["ANTHROPIC_API_KEY"] = "sk-..."llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")
```

ğŸ‘‰ é˜…è¯» [Anthropic é›†æˆæ–‡æ¡£] 

```
pipinstall-U"langchain[openai]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["AZURE_OPENAI_API_KEY"] = "..."os.environ["AZURE_OPENAI_ENDPOINT"] = "..."os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"llm = init_chat_model("azure_openai:gpt-4.1",azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],)
```

ğŸ‘‰ é˜…è¯» [Azure é›†æˆæ–‡æ¡£] 

```
pipinstall-U"langchain[google-genai]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["GOOGLE_API_KEY"] = "..."llm = init_chat_model("google_genai:gemini-2.0-flash")
```

ğŸ‘‰ é˜…è¯» [Google GenAI é›†æˆæ–‡æ¡£] 

```
pipinstall-U"langchain[aws]"
```

```
fromlangchain.chat_modelsimport init_chat_model# Follow the steps here to configure your credentials:# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.htmlllm = init_chat_model("anthropic.claude-3-5-sonnet-20240620-v1:0",model_provider="bedrock_converse",)
```

ğŸ‘‰ é˜…è¯» [AWS Bedrock é›†æˆæ–‡æ¡£] 

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†å…¶ä¸ä¸€ä¸ªé¢å¤–çš„å·¥å…·ä¸€èµ·æ•´åˆåˆ°æˆ‘ä»¬çš„ `StateGraph` ä¸­

```
fromtypingimport Annotatedfromlangchain_tavilyimport TavilySearchfromlangchain_core.toolsimport toolfromtyping_extensionsimport TypedDictfromlanggraph.checkpoint.memoryimport InMemorySaverfromlanggraph.graphimport StateGraph, START, ENDfromlanggraph.graph.messageimport add_messagesfromlanggraph.prebuiltimport ToolNode, tools_conditionfromlanggraph.typesimport Command, interruptclassState(TypedDict):messages: Annotated[list, add_messages]graph_builder = StateGraph(State)@tooldefhuman_assistance(query: str) -> str:"""Request assistance from a human."""human_response = interrupt({"query": query})return human_response["data"]tool = TavilySearch(max_results=2)tools = [tool, human_assistance]llm_with_tools = llm.bind_tools(tools)defchatbot(state: State):message = llm_with_tools.invoke(state["messages"])# Because we will be interrupting during tool execution,# we disable parallel tool calling to avoid repeating any# tool invocations when we resume.assert len(message.tool_calls) <= 1return {"messages": [message]}graph_builder.add_node("chatbot", chatbot)tool_node = ToolNode(tools=tools)graph_builder.add_node("tools", tool_node)graph_builder.add_conditional_edges("chatbot",tools_condition,)graph_builder.add_edge("tools", "chatbot")graph_builder.add_edge(START, "chatbot")
```

æç¤º

æœ‰å…³äººæœºå›åœˆå·¥ä½œæµçš„æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹ï¼Œè¯·å‚è§ [äººæœºå›åœˆ] ã€‚

## 2\. ç¼–è¯‘å›¾ [Â¶] 

æˆ‘ä»¬å’Œä¹‹å‰ä¸€æ ·ï¼Œä½¿ç”¨æ£€æŸ¥ç‚¹ç¼–è¯‘å›¾

```
memory = InMemorySaver()graph = graph_builder.compile(checkpointer=memory)
```

## 3\. å¯è§†åŒ–å›¾ï¼ˆå¯é€‰ï¼‰ [Â¶] 

å¯è§†åŒ–å›¾ï¼Œä½ ä¼šå¾—åˆ°å’Œä¹‹å‰ä¸€æ ·çš„å¸ƒå±€â€”â€”åªæ˜¯å¤šäº†ä¸€ä¸ªå·¥å…·ï¼

```
fromIPython.displayimport Image, displaytry:display(Image(graph.get_graph().draw_mermaid_png()))except Exception:# This requires some extra dependencies and is optionalpass
```

## 4\. æç¤ºèŠå¤©æœºå™¨äºº [Â¶] 

ç°åœ¨ï¼Œç”¨ä¸€ä¸ªä¼šè°ƒç”¨æ–°çš„ `human_assistance` å·¥å…·çš„é—®é¢˜æ¥æç¤ºèŠå¤©æœºå™¨äºº

```
user_input = "I need some expert guidance for building an AI agent. Could you request assistance for me?"config = {"configurable": {"thread_id": "1"}}events = graph.stream({"messages": [{"role": "user", "content": user_input}]},config,stream_mode="values",)for event in events:if "messages" in event:event["messages"][-1].pretty_print()
```

```
================================ Human Message =================================
I need some expert guidance for building an AI agent. Could you request assistance for me?
================================== Ai Message ==================================
[{'text': "Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}]
Tool Calls:
  human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW)
 Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW
  Args:
    query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?

```

èŠå¤©æœºå™¨äººç”Ÿæˆäº†ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼Œä½†éšåæ‰§è¡Œè¢«ä¸­æ–­ã€‚å¦‚æœæ£€æŸ¥å›¾çŠ¶æ€ï¼Œä½ ä¼šçœ‹åˆ°å®ƒåœåœ¨äº†å·¥å…·èŠ‚ç‚¹

```
snapshot = graph.get_state(config)snapshot.next
```

```
('tools',)

```

ä¿¡æ¯

ä»”ç»†çœ‹çœ‹ `human_assistance` å·¥å…·

```
@tooldefhuman_assistance(query: str) -> str:"""Request assistance from a human."""human_response = interrupt({"query": query})return human_response["data"]
```

ä¸ Python çš„å†…ç½® `input()` å‡½æ•°ç±»ä¼¼ï¼Œåœ¨å·¥å…·å†…éƒ¨è°ƒç”¨ `interrupt` å°†æš‚åœæ‰§è¡Œã€‚è¿›åº¦ä¼šæ ¹æ® [æ£€æŸ¥ç‚¹] è¿›è¡ŒæŒä¹…åŒ–ï¼›å› æ­¤ï¼Œå¦‚æœå®ƒä½¿ç”¨ Postgres è¿›è¡ŒæŒä¹…åŒ–ï¼Œåªè¦æ•°æ®åº“å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œå®ƒå°±å¯ä»¥åœ¨ä»»ä½•æ—¶å€™æ¢å¤ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œå®ƒä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹è¿›è¡ŒæŒä¹…åŒ–ï¼Œåªè¦ Python å†…æ ¸æ­£åœ¨è¿è¡Œï¼Œå°±å¯ä»¥éšæ—¶æ¢å¤ã€‚

## 5\. æ¢å¤æ‰§è¡Œ [Â¶] 

è¦æ¢å¤æ‰§è¡Œï¼Œè¯·ä¼ é€’ä¸€ä¸ªåŒ…å«å·¥å…·é¢„æœŸæ•°æ®çš„ [`Command`] å¯¹è±¡ã€‚æ­¤æ•°æ®çš„æ ¼å¼å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè‡ªå®šä¹‰ã€‚

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰åä¸º `"data"` çš„é”®çš„å­—å…¸

```
human_response = ("We, the experts are here to help! We'd recommend you check out LangGraph to build your agent."" It's much more reliable and extensible than simple autonomous agents.")human_command = Command(resume={"data": human_response})events = graph.stream(human_command, config, stream_mode="values")for event in events:if "messages" in event:event["messages"][-1].pretty_print()
```

```
================================== Ai Message ==================================
[{'text': "Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}]
Tool Calls:
  human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW)
 Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW
  Args:
    query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?
================================= Tool Message =================================
Name: human_assistance
We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.
================================== Ai Message ==================================
Thank you for your patience. I've received some expert advice regarding your request for guidance on building an AI agent. Here's what the experts have suggested:
The experts recommend that you look into LangGraph for building your AI agent. They mention that LangGraph is a more reliable and extensible option compared to simple autonomous agents.
LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:
1. Reliability: The experts emphasize that LangGraph is more reliable than simpler autonomous agent approaches. This could mean it has better stability, error handling, or consistent performance.
2. Extensibility: LangGraph is described as more extensible, which suggests that it probably offers a flexible architecture that allows you to easily add new features or modify existing ones as your agent's requirements evolve.
3. Advanced capabilities: Given that it's recommended over "simple autonomous agents," LangGraph likely provides more sophisticated tools and techniques for building complex AI agents.
...
2. Look for tutorials or guides specifically focused on building AI agents with LangGraph.
3. Check if there are any community forums or discussion groups where you can ask questions and get support from other developers using LangGraph.
If you'd like more specific information about LangGraph or have any questions about this recommendation, please feel free to ask, and I can request further assistance from the experts.
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

```

è¾“å…¥å·²è¢«æ¥æ”¶å¹¶ä½œä¸ºå·¥å…·æ¶ˆæ¯å¤„ç†ã€‚æŸ¥çœ‹æ­¤è°ƒç”¨çš„ [LangSmith è·Ÿè¸ª] ï¼Œä»¥æŸ¥çœ‹ä¸Šè¿°è°ƒç”¨ä¸­å®Œæˆçš„ç¡®åˆ‡å·¥ä½œã€‚è¯·æ³¨æ„ï¼ŒçŠ¶æ€åœ¨ç¬¬ä¸€æ­¥è¢«åŠ è½½ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººå¯ä»¥ä»å®ƒç¦»å¼€çš„åœ°æ–¹ç»§ç»­ã€‚

**æ­å–œï¼** æ‚¨å·²ç»ä½¿ç”¨ `interrupt` ä¸ºæ‚¨çš„èŠå¤©æœºå™¨äººæ·»åŠ äº†äººæœºå›åœˆæ‰§è¡Œï¼Œä»è€Œåœ¨éœ€è¦æ—¶å…è®¸äººç±»ç›‘ç£å’Œå¹²é¢„ã€‚è¿™ä¸ºæ‚¨ä½¿ç”¨ AI ç³»ç»Ÿåˆ›å»ºçš„æ½œåœ¨ UI å¼€è¾Ÿäº†å¯èƒ½æ€§ã€‚ç”±äºæ‚¨å·²ç»æ·»åŠ äº† **æ£€æŸ¥ç‚¹**ï¼Œåªè¦åº•å±‚çš„æŒä¹…åŒ–å±‚æ­£åœ¨è¿è¡Œï¼Œå›¾å°±å¯ä»¥ **æ— é™æœŸåœ°** æš‚åœï¼Œå¹¶éšæ—¶æ¢å¤ï¼Œå°±åƒä»€ä¹ˆéƒ½æ²¡å‘ç”Ÿè¿‡ä¸€æ ·ã€‚

æŸ¥çœ‹ä¸‹é¢çš„ä»£ç ç‰‡æ®µï¼Œå›é¡¾æœ¬æ•™ç¨‹ä¸­çš„å›¾

OpenAIAnthropicAzureGoogle GeminiAWS Bedrock

```
pipinstall-U"langchain[openai]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["OPENAI_API_KEY"] = "sk-..."llm = init_chat_model("openai:gpt-4.1")
```

ğŸ‘‰ é˜…è¯» [OpenAI é›†æˆæ–‡æ¡£] 

```
pipinstall-U"langchain[anthropic]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["ANTHROPIC_API_KEY"] = "sk-..."llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")
```

ğŸ‘‰ é˜…è¯» [Anthropic é›†æˆæ–‡æ¡£] 

```
pipinstall-U"langchain[openai]"
```

```
importosfromlangchain.chat_modelsimport init_chat_modelos.environ["AZURE_OPENAI_API_KEY"] = "..."os.environ["AZURE_OPENAI_ENDPOINT"


---
*æ•°æ®æ¥æº: Exaæœç´¢ | è·å–æ—¶é—´: 2026-02-20 20:40:34*