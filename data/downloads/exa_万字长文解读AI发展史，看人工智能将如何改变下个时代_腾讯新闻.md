# 万字长文解读AI发展史，看人工智能将如何改变下个时代_腾讯新闻

**URL**:
https://news.qq.com/rain/a/20221112A04S4N00

## 元数据
- 发布日期: 2022-11-15T09:06:40+00:00

## 完整内容
---
万字长文解读AI发展史，看人工智能将如何改变下个时代\_腾讯新闻
# 万字长文解读AI发展史，看人工智能将如何改变下个时代
![头像]![] 
[
INDIGO的数字镜像
] 
2022-11-15 09:06发布于中国香港科技领域创作者
就在过去几个月里，因为美联储的加息，科技公司的资本狂欢宣告结束，美国上市的SaaS 公司股价基本都跌去了70%，裁员与紧缩是必要选项。但正当市场一片哀嚎的时候，Dall-E 2 发布了，紧接着就是一大批炫酷的AI 公司登场。这些事件在风投界引发了一股风潮，我们看到那些兜售着基于生成式AI（Generative AI）产品的公司，估值达到了数十亿美元，虽然收入还不到百万美元，也没有经过验证的商业模式。不久前，同样的故事在 Web 3 上也发生过！感觉我们又将进入一个全新的繁荣时代，但人工智能这次真的能带动科技产业复苏么？划重点本文将带你领略一次人工智能领域波澜壮阔的发展史，从关键人物推动的学术进展、算法和理念的涌现、公司和产品的进步、还有脑科学对神经网络的迭代影响，这四个维度来深刻理解“机器之心的进化”。先忘掉那些花里胡哨的图片生产应用，我们一起来学点接近AI 本质的东西。文章较长，累计22800 字，请留出一小时左右的阅读时间，欢迎先收藏再阅读！文中每一个链接和引用都是有价值的，特别作为衍生阅读推荐给大家。阅读之前先插播一段Elon Musk 和Jack Ma 在WAIC 2019 关于人工智能的对谈的经典老视频，全程注意Elon Ma 的表情大家觉得机器智能能否超过人类么？带着这个问题来阅读，相信看完就会有系统性的答案！本文在无特别指明的情况下，为了书写简洁，在同一个段落中重复词汇大量出现时，会用AI（Artifical Intelligence）来代表 人工智能，用ML（Machine Learning）来代表机器学习，DL（Deep Learning）来代表深度学习，以及各种英文缩写来优先表达。
01
AI 进化史对于机器是否真能&quot;知道&quot;、&quot;思考 &quot;等问题，我们很难严谨地定义这些。我们对人类心理过程的理解，或许只比鱼对游泳的理解更好一点。
- John McCarthy
早在1945 年，Alan Turing 就已经在考虑如何用计算机来模拟人脑了。他设计了ACE（Automatic Computing Engine - 自动计算引擎）来模拟大脑工作。在给一位同事的信中写道：&quot;与计算的实际应用相比，我对制作大脑运作的模型可能更感兴趣 ...... 尽管大脑运作机制是通过轴突和树突的生长来计算的复杂神经元回路，但我们还是可以在ACE 中制作一个模型，允许这种可能性的存在，ACE 的实际构造并没有改变，它只是记住了数据......&quot; 这就是机器智能的起源，至少那时在英国都这样定义。1.1 前神经网络时代神经网络是以模仿人脑中的神经元的运作为模型的计算机系统。AI 是伴随着神经网络的发展而出现的。1956年，美国心理学家 Frank Rosenblatt 实现了一个早期的神经网络演示- 感知器模型（Perceptron Model），该网络通过监督 Learning的方法将简单的图像分类，如三角形和正方形。这是一台只有八个模拟神经元的计算机，这些神经元由马达和转盘制成，与 400 个光探测器连接。![图片] 配图01：Frank Rosenblatt &amp; Perceptron Model
IBM 的Georgetown 实验室在这些研究的基础上，实现了最早的机器语言翻译系统，可以在英语和俄语之间互译。1956年的夏天，在 Dartmouth College 的一次会议上，AI被定义为计算机科学的一个研究领域，Marvin Minsky（明斯基）, John McCarthy（麦卡锡）, Claude Shannon（香农）, 还有Nathaniel Rochester（罗切斯特）组织了这次会议，他们后来被称为 AI 的&quot;奠基人&quot;。
![图片] 配图02：Participants of the 1956 Dartmouth Summer Research Project on AI
DARPA 在这个“黄金”时期，将大部分资金投入AI 领域，就在十年后他们还发明了ARPANET（互联网的前身）。早期的 AI 先驱们试图教计算机做模仿人类的复杂心理任务，他们将其分成五个子领域：推理、知识表述、规划、自然语言处理（NLP）和感知，这些听起来很笼统的术语一直沿用至今。
从专家系统到机器学习1966年，Marvin Minsky 和Seymour Papert 在《感知器：计算几何学导论》一书中阐述了因为硬件的限制，只有几层的神经网络仅能执行最基本的计算，一下子浇灭了这条路线上研发的热情，AI 领域迎来了第一次泡沫破灭。这些先驱们怎么也没想到，计算机的速度能够在随后的几十年里指数级增长，提升了上亿倍。在上世纪八十年代，随着电脑性能的提升，新计算机语言Prolog &amp; Lisp 的流行，可以用复杂的程序结构，例如条件循环来实现逻辑，这时的人工智能就是专家系统（Expert System），iRobot 公司绝对是那个时代明星；但短暂的繁荣之后，硬件存储空间的限制，还有专家系统无法解决具体的、难以计算的逻辑问题，人工智能再一次陷入窘境。我怀疑任何非常类似于形式逻辑的东西能否成为人类推理的良好模型。- Marvin Minsky
直到IBM 深蓝在1997年战胜了国际象棋冠军卡斯帕罗夫后，新的基于概率推论（Probabilistic Reasoning）思路开始被广泛应用在 AI 领域，随后IBM Watson 的项目使用这种方法在电视游戏节目《Jeopardy》中经常击败参赛的人类。
概率推论就是典型的机器学习（Machine Learning）。今天的大多数 AI 系统都是由ML 驱动的，其中预测模型是根据历史数据训练的，并用于对未来的预测。这是AI 领域的第一次范式转变，算法不指定如何解决一个任务，而是根据数据来诱导它，动态地达成目标。因为有了ML，才有了大数据（Big Data）这个概念。
1.2 Machine Learning 的跃迁Machine Learning 算法一般通过分析数据和推断模型来建立参数，或者通过与环境互动，获得反馈来学习。人类可以注释这些数据，也可以不注释，环境可以是模拟的，也可以是真实世界。Deep Learning
Deep Learning 是一种Machine Learning算法，它使用多层神经网络和反向传播（Backpropagation）技术来训练神经网络。该领域是几乎是由 Geoffrey Hinton 开创的，早在1986年，Hinton 与他的同事一起发表了关于深度神经网络（DNNs - Deep Neural Networks）的开创性论文，这篇文章引入了反向传播的概念，这是一种调整权重的算法，每当你改变权重时，神经网络就会比以前更快接近正确的输出，可以轻松的实现多层的神经网络，突破了 1966 年Minsky 写的感知器局限的魔咒。![图片] 配图03：Geoffrey Hinton &amp; Deep Neural Networks
Deep Learning 在2012 年才真正兴起，当时Hinton 和他在多伦多的两个学生表明，使用反向传播训练的深度神经网络在图像识别方面击败了最先进的系统，几乎将以前的错误率减半。由于他的工作和对该领域的贡献，Hinton 的名字几乎成为Deep Learning 的代名词。数据是新的石油Deep Learning 是一个革命性的领域，但为了让它按预期工作，需要数据。而最重要的数据集之一，就是由李飞飞创建的ImageNet。曾任斯坦福大学人工智能实验室主任，同时也是谷歌云 AI/ML 首席科学家的李飞飞，早在2009 年就看出数据对Machine Learning 算法的发展至关重要，同年在计算机视觉和模式识别（CVPR）上发表了相关论文。
![图片] 配图04：FeiFei Li &amp; ImageNet
该数据集对研究人员非常有用，正因为如此，它变得越来越有名，为最重要的年度DL 竞赛提供了基准。仅仅七年时间，ImageNet 让获胜算法对图像中的物体进行分类的准确率从72% 提高到了98%，超过了人类的平均能力。
ImageNet 成为DL 革命的首选数据集，更确切地说，是由Hinton 领导的AlexNet 卷积神经网络（CNN - Convolution Neural Networks）的数据集。ImageNet 不仅引领了DL 的革命，也为其他数据集开创了先例。自其创建以来，数十种新的数据集被引入，数据更丰富，分类更精确。神经网络大爆发在Deep Learning 理论和数据集的加持下，2012年以来深度神经网络算法开始大爆发，卷积神经网络（CNN）、递归神经网络（RNN - Recurrent Neural Network）和长短期记忆网络（LSTM - Long Short-Term Memory）等等，每一种都有不同的特性。例如，递归神经网络是较高层的神经元直接连接到较低层的神经元。
来自日本的计算机研究员福岛邦彦（Kunihiko Fukushima）根据人脑中视觉的运作方式，创建了一个人工神经网络模型。该架构是基于人脑中两种类型的神经元细胞，称为简单细胞和复杂细胞。它们存在于初级视觉皮层中，是大脑中处理视觉信息的部分。简单细胞负责检测局部特征，如边缘；复杂细胞汇集了简单细胞在一个区域内产生的结果。例如，一个简单细胞可能检测到一个椅子的边缘，复杂细胞汇总信息产生结果，通知下一个更高层次的简单细胞，这样逐级识别得到完整结果。
![图片] 配图05：深度神经网络如何识别物体（TensorFlow）
CNN 的结构是基于这两类细胞的级联模型，主要用于模式识别任务。它在计算上比大多数其他架构更有效、更快速，在许多应用中，包括自然语言处理和图像识别，已经被用来击败大多数其他算法。我们每次对大脑的工作机制的认知多一点，神经网络的算法和模型也会前进一步！1.3 开启潘多拉的魔盒从2012 到现在，深度神经网络的使用呈爆炸式增长，进展惊人。现在Machine Learning 领域的大部分研究都集中在Deep Learning 方面，就像进入了潘多拉的魔盒被开启了的时代。![图片] 配图06：AI 进化史GAN
生成对抗网络（GAN - Generative Adversarial Network） 是Deep Learning 领域里面另一个重要的里程碑，诞生于2014 年，它可以帮助神经网络用更少的数据进行学习，生成更多的合成图像，然后用来识别和创建更好的神经网络。GANs 的创造者Ian Goodfellow 是在蒙特利尔的一个酒吧里想出这个主意的，它由两个神经网络玩着猫捉老鼠的游戏，一个创造出看起来像真实图像的假图像，而另一个则决定它们是否是真的。![图片] 配图07：GANs 模拟生产人像的进化GANs 将有助于创建图像，还可以创建现实世界的软件模拟，Nvidia 就大量采用这种技术来增强他的现实模拟系统，开发人员可以在那里训练和测试其他类型的软件。你可以用一个神经网络来“压缩”图像，另一个神经网络来生成原始视频或图像，而不是直接压缩数据，Demis Hassabis 在他的一篇论文中就提到了人类大脑“海马体”的记忆回放也是类似的机制。大规模神经网络大脑的工作方式肯定不是靠某人用规则来编程。- Geoffrey Hinton
大规模神经网络的竞赛从成立于2011 年的Google Brain 开始，现在属于Google Research。他们推动了 TensorFlow 语言的开发，提出了万能模型Transformer 的技术方案并在其基础上开发了BERT，我们在第四章中将详细讨论这些。
DeepMind 是这个时代的传奇之一，在2014年被 Google 以5.25 亿美元收购的。它专注游戏算法，其使命是&quot;解决智能问题&quot;，然后用这种智能来 &quot;解决其他一切问题&quot;！DeepMind 的团队开发了一种新的算法Deep Q-Network (DQN)，它可以从经验中学习。2015 年10 月AlphaGo 项目首次在围棋中击败人类冠军李世石；之后的AlphaGo Zero 用新的可以自我博弈的改进算法让人类在围棋领域再也无法翻盘。另一个传奇OpenAI，它是一个由Elon Musk, Sam Altman, Peter Thiel, 还有Reid Hoffman 在2015年共同出资十亿美金创立的科研机构，其主要的竞争对手就是 DeepMind。OpenAI 的使命是通用人工智能（AGI –Artificial General Intelligence），即一种高度自主且在大多数具有经济价值的工作上超越人类的系统。2020年推出的 GPT-3 是目前最好的自然语言生成工具（NLP - Natural Language Processing）之一，通过它的 API 可以实现自然语言同步翻译、对话、撰写文案，甚至是代码（Codex），以及现在最流行的生成图像（DALL·E）。
Gartner AI HypeCycle
Gartner 的技术炒作周期（HypeCycle）很值得一看，这是他们 2022 年最新的关于AI 领域下各个技术发展的成熟度预估，可以快速了解AI 进化史这一章中不同技术的发展阶段。![图片] 配图08：Gartner AI HypeCycle 2022
神经网络，这个在上世纪60 年代碰到的挫折，然后在2012 年之后却迎来了新生。反向传播花了这么长时间才被开发出来的原因之一就是该功能需要计算机进行乘法矩阵运算。在上世纪70 年代末，世界上最强的的超级电脑之一Cray-1，每秒浮点运算速度 50 MFLOP，现在衡量 GPU 算力的单位是TFLOP（Trillion FLOPs），Nvidia 用于数据中心的最新GPU Nvidia Volta 的性能可以达到125 TFLOP，单枚芯片的速度就比五十年前世界上最快的电脑强大 250 万倍。技术的进步是多维度的，一些生不逢时的理论或者方法，在另一些技术条件达成时，就能融合出巨大的能量。02
软件2.0 的崛起未来的计算机语言将更多得关注目标，而不是由程序员来考虑实现的过程。- Marvin Minsky
Software 2.0 概念的最早提出人是Andrej Karpathy，这位从小随家庭从捷克移民来加拿大的天才少年在多伦多大学师从 Geoffrey Hinton，然后在斯坦福李飞飞团队获得博士学位，主要研究 NLP 和计算机视觉，同时作为创始团队成员加入了OpenAI，Deep Learning 的关键人物和历史节点都被他点亮。在2017 年被Elon Musk 挖墙脚到了Tesla 负责自动驾驶研发，然后就有了重构的FSD（Full Self-Driving）。
按照Andrej Karpathy 的定义- “软件2.0 使用更抽象、对人类不友好的语言生成，比如神经网络的权重。没人参与编写这些代码，一个典型的神经网络可能有数百万个权重，用权重直接编码比较困难”。Andrej 说他以前试过，这几乎不是人类能干的事儿。。![图片] 配图09：Andrej Karpathy 和神经网络权重2.1 范式转移在创建深度神经网络时，程序员只写几行代码，让神经网络自己学习，计算权重，形成网络连接，而不是手写代码。这种软件开发的新范式始于第一个Machine Learning 语言TensorFlow，我们也把这种新的编码方式被称为软件 2.0。在 Deep Learning 兴起之前，大多数人工智能程序是用Python 和JavaScript 等编程语言手写的。人类编写了每一行代码，也决定了程序的所有规则。![图片] 配图10：How does Machine Learning work？（TensorFlow）
相比之下，随着Deep Learning 技术的出现，程序员利用这些新方式，给程序指定目标。如赢得围棋比赛，或通过提供适当输入和输出的数据，如向算法提供具有&quot;SPAM” 特征的邮件和其他没有&quot;SPAM” 特征的邮件。编写一个粗略的代码骨架（一个神经网络架构），确定一个程序空间的可搜索子集，并使用我们所能提供的算力在这个空间中搜索，形成一个有效的程序路径。在神经网络里，我们一步步地限制搜索范围到连续的子集上，搜索过程通过反向传播和随机梯度下降（Stochastic Gradient Descent）而变得十分高效。
神经网络不仅仅是另一个分类器，它代表着我们开发软件的范式开始转移，它是软件2.0。
软件1.0 人们编写代码，编译后生成可以执行的二进制文件；但在软件2.0 中人们提供数据和神经网络框架，通过训练将数据编译成二进制的神经网络。在当今大多数实际应用中，神经网络结构和训练系统日益标准化为一种商品，因此大多数软件2.0 的开发都由模型设计实施和数据清理标记两部分组成。这从根本上改变了我们在软件开发迭代上的范式，团队也会因此分成了两个部分:2.0 程序员负责模型和数据，而那些1.0 程序员则负责维护和迭代运转模型和数据的基础设施、分析工具以及可视化界面。Marc Andreessen 的经典文章标题《Why Software Is Eating the World》现在可以改成这样：“软件（1.0）正在吞噬世界，而现在人工智能（2.0）正在吞噬软件！
2.2 软件的演化软件从1.0 发展到软件2.0，经过了一个叫做“数据产品”的中间态。当顶级软件公司在了解大数据的商业潜力后，并开始使用 Machine Learning 构建数据产品时，这种状态就出现了。下图来自Ahmad Mustapha 的一篇文章《The Rise of Software 2.0》很好地呈现了这个过渡。
![图片] 配图11：软件产品演化的三种状态
这个中间态也叫大数据和算法推荐。在现实生活中，这样的产品可以是Amazon 的商品推荐，它们可以预测客户会感兴趣什么，可以是Facebook 好友推荐，还可以是Netflix 电影推荐或Tiktok 的短视频推荐。还有呢？Waze 的路由算法、Airbnb 背后的排名算法等等，总之琳琅满目。数据产品有几个重要特点：1、它们都不是软件的主要功能，通常是为了增加体验，达成更好的用户活跃以及销售目标；2、能够随着数据的增加而进化；3、大部分都是基于传统 ML 实现的，最重要的一点数据产品是可解释的。但有些行业正在改变，Machine Learning 是主体。当我们放弃通过编写明确的代码来解决复杂问题时，这个到2.0 技术栈的转变就发生了，在过去几年中，很多领域都在突飞猛进。语音识别曾经涉及大量的预处理、高斯混合模型和隐式Markov 模型，但今天几乎完全被神经网络替代了。早在1985 年，知名信息论和语言识别专家Fred Jelinek 就有一句经常被引用的段子：“每当我解雇一个语言学家，我们的语音识别系统的性能就会得到提高”。![图片] 配图12：图解软件 2.0 的代表应用除了大家熟悉的图像语音识别、语音合成、机器翻译、游戏挑战之外，AI 在很多传统系统也看到了早期的转型迹象。例如The Case for Learned Index Structures 用神经网络取代了数据管理系统的核心组件，在速度上比B-Trees 缓存优化快70%，同时节省了一个数量级的内存。
所以，软件2.0 的范式具备了这几个新特征：1、Deep Learning 是主体，所有的功能都是围绕神经网络的输入输出构建的，例如语音识别、自动驾驶；2、可解释性并不重要，一个好的大数据推荐广告可以告诉客户用户看到这条广告的理由，但你没法从神经网络中找到规则，至少目前不行；3、高研发投入与低开发投入，现在大量的成功都来自大学和科技公司的研究部门，论文绝对比应用多。。
2.3 软件2.0 的优势为什么我们应该倾向于将复杂的程序移植到软件2.0 中？Andrej Karpathy 在《Software 2.0》中给出了一个简单的答案：它们在实践中表现得更好！
容易被写入芯片由于神经网络的指令集相对较小，主要是矩阵乘法（Matrix Multiplication）和阈值判断（Thresholding at Zero），因此把它们写入芯片要容易得多，例如使用定制的 ASIC、神经形态芯片等等（Alan Turing 在设计ACE 时就这样考虑了）。例如，小而廉价的芯片可以带有一个预先训练好的卷积网络，它们可以识别语音、合成音频、处理视觉信号。当我们周围充斥着低能耗的智能时，世界将会因此而大不同（好坏皆可）。非常敏捷敏捷开发意味着灵活高效。如果你有一段C++ 代码，有人希望你把它的速度提高一倍，那么你需要系统性地调优甚至是重写。然而，在软件2.0 中，我们在网络中删除一半的通道，重新训练，然后就可以了。。它的运行速度正好提升两倍，只是输出更差一些，这就像魔法。相反，如果你有更多的数据或算力，通过添加更多的通道和再次训练，你的程序就能工作得更好。模块可以融合成一个最佳的整体做过软件开发的同学都知道，程序模块通常利用公共函数、API 或远程调用来通讯。然而，如果让两个原本分开训练的软件2.0 模块进行互动，我们可以很容易地通过整体进行反向传播来实现。想象一下，如果你的浏览器能够自动整合改进低层次的系统指令，来提升网页加载效率，这将是一件令人惊奇的事情。但在软件2.0 中，这是默认行为。它做得比你好最后，也是最重要的一点，神经网络比你能想到的任何有价值的垂直领域的代码都要好，目前至少在图像、视频、声音、语音相关的任何东西上，比你写的代码要好。2.4 Bug 2.0
对于传统软件，即软件1.0，大多数程序都通过源代码保存，这些代码可能少至数千行，多至上亿行。据说，谷歌的整个代码库大约有 20 亿行代码。无论代码有多少，传统的软件工程实践表明，使用封装和模块化设计，有助于创建可维护的代码，很容易隔离Bug 来进行修改。但在新的范式中，程序被存储在内存中，作为神经网络架构的权重，程序员编写的代码很少。软件2.0 带来了两个新问题：不可解释和数据污染。因为训练完成的神经网络权重，工程师无法理解（不过现在对理解神经网络的研究有了很多进展，第六章会讲到），所以我们无法知道正确的执行是为什么？错误又是因为什么？这个和大数据算法有很大的不同，虽然大多数的应用只关心结果，无需解释；但对于一些安全敏感的领域，比如自动驾驶和医疗应用，这确实很重要。在2.0 的堆栈中，数据决定了神经网络的连接，所以不正确的数据集和标签，都会混淆神经网络。错误的数据可能来自失误、也可能是人为设计，或者是有针对性地投喂混淆数据（这也是人工智能领域中新的程序道德规范问题）。例如iOS 系统的自动拼写功能被意外的数据训练污染了，我们在输入某些字符的时候就永远得不到正确的结果。训练模型会认为污染数据是一个重要的修正，一旦完成训练部署，这个错误就像病毒一样传播，到达了数百万部iPhone 手机。所以在这种2.0 版的Bug 中，需要对数据以及程序结果进行良好的测试，确保这些边缘案例不会使程序失败。在短期内，软件2.0 将变得越来越普遍，那些没法通过清晰算法和软件逻辑化表述的问题，都会转入2.0 的新范式，现实世界并不适合整齐地封装。


---
*数据来源: Exa搜索 | 获取时间: 2026-02-02 20:35:42*