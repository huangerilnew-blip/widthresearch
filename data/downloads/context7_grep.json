{"text": "### Compose Outer Graph with Subgraph Integration\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nCreate a parent graph that invokes subgraphs as nodes. The outer graph manages overall workflow orchestration while delegating specific tasks to inner graphs. Supports state aggregation and transformation across graph boundaries.\n\n```python\nfrom typing_extensions import Annotated\nimport operator\n\n# Outer graph state\nclass OuterState(TypedDict):\n    values: Annotated[list[int], operator.add]\n    current: int\n\ndef prepare(state: OuterState) -> dict:\n    return {\"current\": state[\"values\"][-1] if state[\"values\"] else 0}\n\ndef use_subgraph(state: OuterState) -> dict:\n    \"\"\"Call subgraph and integrate result.\"\"\"\n    result = inner_graph.invoke({\"value\": state[\"current\"]})\n    return {\"values\": [result[\"value\"]], \"current\": result[\"value\"]}\n\ndef finalize(state: OuterState) -> dict:\n    return {\"values\": [sum(state[\"values\"])]}\n\n# Build outer graph\nouter_builder = StateGraph(OuterState)\nouter_builder.add_node(\"prepare\", prepare)\nouter_builder.add_node(\"subgraph\", use_subgraph)\nouter_builder.add_node(\"finalize\", finalize)\nouter_builder.add_edge(START, \"prepare\")\nouter_builder.add_edge(\"prepare\", \"subgraph\")\nouter_builder.add_edge(\"subgraph\", \"finalize\")\nouter_builder.add_edge(\"finalize\", END)\n\nouter_graph = outer_builder.compile()\nresult = outer_graph.invoke({\"values\": [5], \"current\": 0})\nprint(result)\n```\n\n--------------------------------\n\n### Build Inner Subgraph with State Management\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nCreate a nested inner graph with its own state definition and nodes. Inner graphs process data independently and return results to parent graphs. Uses StateGraph to define state schema and node connections.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom typing_extensions import TypedDict\n\n# Inner graph state\nclass InnerState(TypedDict):\n    value: int\n\ndef inner_process(state: InnerState) -> dict:\n    return {\"value\": state[\"value\"] * 2}\n\n# Build inner graph\ninner_builder = StateGraph(InnerState)\ninner_builder.add_node(\"process\", inner_process)\ninner_builder.add_edge(START, \"process\")\ninner_builder.add_edge(\"process\", END)\ninner_graph = inner_builder.compile()\n```\n\n### Subgraphs > State Management and Integration\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nWhen building subgraphs, you define an inner state schema using TypedDict and create nodes that process this state. The inner graph is compiled separately using `StateGraph` and can be invoked from parent graph nodes. The parent graph can call the subgraph using `inner_graph.invoke()` with appropriate state data, then integrate the results back into the parent's state using reducer functions like `operator.add` for accumulating values across multiple invocations.\n\n--------------------------------\n\n### Subgraphs > Composing Graphs\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nSubgraphs enable modular architecture in LangGraph by allowing graphs to be nested within parent graphs. Inner graphs can be defined with their own state types and node logic, then compiled independently. Parent graphs can invoke subgraphs as nodes, passing state data and integrating results back into the parent's state. This composition pattern supports code reuse and separation of concerns while maintaining flexible state sharing between parent and child graphs.\n\n--------------------------------\n\n### LangGraph > StateGraph - Building Stateful Workflows\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `StateGraph` class is the primary builder for creating graphs whose nodes communicate through shared state. Nodes are Python functions that receive the current state and return partial updates. The graph must be compiled before execution using the `compile()` method. State schemas can define reducers for fields like lists that aggregate values across node updates, while other fields like counters use simple overwrite semantics. Once compiled with a checkpointer for persistence, the graph can be invoked with an initial state and configuration including a thread ID for maintaining separate execution contexts.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Implement Dynamic Routing with Conditional Edges in LangGraph (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThis example illustrates the use of `add_conditional_edges` for dynamic routing in a `StateGraph`. It defines a routing function that inspects the current state and determines the next node(s) to execute, showcasing how to create branching logic in workflows based on state values. This enables more complex, adaptive agent behaviors.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom typing import Literal\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    value: int\n    path_taken: str\n\ndef process_input(state: State) -> dict:\n    return {\"value\": state[\"value\"]}\n\ndef route_by_value(state: State) -> Literal[\"high_path\", \"low_path\"]:\n    \"\"\"Route based on state value.\"\"\"\n    if state[\"value\"] > 50:\n        return \"high_path\"\n    return \"low_path\"\n\ndef high_handler(state: State) -> dict:\n    return {\"path_taken\": \"high\", \"value\": state[\"value\"] * 2}\n\ndef low_handler(state: State) -> dict:\n    return {\"path_taken\": \"low\", \"value\": state[\"value\"] + 10}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_input)\nbuilder.add_node(\"high_path\", high_handler)\nbuilder.add_node(\"low_path\", low_handler)\n\nbuilder.add_edge(START, \"process\")\nbuilder.add_conditional_edges(\n    \"process\",\n    route_by_value,\n    {\"high_path\": \"high_path\", \"low_path\": \"low_path\"}\n)\nbuilder.add_edge(\"high_path\", END)\nbuilder.add_edge(\"low_path\", END)\n\ngraph = builder.compile()\n\n# Test with high value\nresult = graph.invoke({\"value\": 75, \"path_taken\": \"\"})\nprint(result)  # {'value': 150, 'path_taken': 'high'}\n\n# Test with low value\nresult = graph.invoke({\"value\": 25, \"path_taken\": \"\"})\nprint(result)  # {'value': 35, 'path_taken': 'low'}\n```\n\n--------------------------------\n\n### Define and Build a LangGraph Workflow with Conditional Edges (Python)\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb\n\nThis Python code defines a LangGraph `StateGraph` workflow by adding various nodes such as `web_search`, `retrieve`, `grade_documents`, `generate`, and `llm_fallback`. It then establishes conditional edges using the previously defined routing and grading functions (`route_question`, `decide_to_generate`, `grade_generation_v_documents_and_question`) to control the flow based on the state, creating a dynamic RAG pipeline.\n\n```python\nimport pprint\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # rag\nworkflow.add_node(\"llm_fallback\", llm_fallback)  # llm\n\n# Build graph\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"web_search\": \"web_search\",\n        \"vectorstore\": \"retrieve\",\n        \"llm_fallback\": \"llm_fallback\",\n    },\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"web_search\": \"web_search\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",  # Hallucinations: re-generate\n        \"not useful\": \"web_search\",  # Fails to answer question: fall-back to web-search\n        \"useful\": END,\n    },\n)\nworkflow.add_edge(\"llm_fallback\", END)\n\n# Compile\napp = workflow.compile()\n```\n\n--------------------------------\n\n### Build and Compile a LangGraph Workflow (Python)\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_local.ipynb\n\nThis Python snippet demonstrates how to construct a LangGraph workflow. It initializes a `StateGraph`, adds various processing nodes like 'web_search', 'retrieve', 'grade_documents', 'generate', and 'transform_query'. It then defines conditional and unconditional edges to orchestrate the flow based on the outputs of routing and grading functions, finally compiling the workflow into an executable application.\n\n```python\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # generate\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\n\n# Build graph\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"web_search\": \"web_search\",\n        \"vectorstore\": \"retrieve\",\n    },\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"transform_query\": \"transform_query\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"transform_query\", \"retrieve\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"transform_query\",\n    },\n)\n\n# Compile\napp = workflow.compile()\n```\n\n--------------------------------\n\n### Define LangGraph Workflow Edges and Compile Custom Agent\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n\nThis code defines the edges and conditional edges for a LangGraph workflow, establishing the flow of execution between different nodes. It then compiles the workflow into a custom graph, which can be invoked for agent operations. The `display` function is used to visualize the compiled graph.\n\n```python\nworkflow.add_edge(START, \"retrieve\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"search\": \"web_search\",\n        \"generate\": \"generate\"\n    }\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"generate\", END)\n\ncustom_graph = workflow.compile()\n\ndisplay(Image(custom_graph.get_graph(xray=True).draw_mermaid_png()))\n```\n\n### LangGraph > Conditional Edges - Dynamic Routing\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nConditional edges allow dynamic routing based on state values, enabling graphs to branch execution paths at runtime. The `add_conditional_edges` method takes a source node and a path function that evaluates the current state and returns the next node name(s) to execute. This enables sophisticated control flow where different branches handle different conditions\u2014for example, routing to a high-value handler when a state value exceeds a threshold, or to a low-value handler otherwise. The path function receives the full state object and can implement complex routing logic based on multiple state fields.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Define agent state with message accumulation\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb\n\nCreates a TypedDict-based state class for the LangGraph agent that maintains a sequence of messages. Uses the add_messages function to append new messages to the state instead of replacing them, enabling conversation history tracking.\n\n```python\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom langchain_core.messages import BaseMessage\n\nfrom langgraph.graph.message import add_messages\n\n\nclass AgentState(TypedDict):\n    # The add_messages function defines how an update should be processed\n    # Default is to replace. add_messages says \"append\"\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n```\n\n--------------------------------\n\n### Build Stateful Workflows with LangGraph's StateGraph in Python\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThis snippet demonstrates how to define a `StateGraph` with a custom state schema (`TypedDict`), add nodes as Python functions, define edges, and compile the graph with an `InMemorySaver` for basic checkpointing. It shows how nodes update the shared state and how the graph is invoked, illustrating the core building blocks of a stateful agent.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\n# Define state schema with optional reducer for list aggregation\nclass State(TypedDict):\n    messages: Annotated[list[str], operator.add]  # Reducer adds new items to list\n    counter: int  # Simple overwrite\n\n# Define node functions\ndef first_node(state: State) -> dict:\n    return {\"messages\": [\"Hello from first node\"], \"counter\": state[\"counter\"] + 1}\n\ndef second_node(state: State) -> dict:\n    return {\"messages\": [\"Hello from second node\"], \"counter\": state[\"counter\"] + 1}\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"first\", first_node)\nbuilder.add_node(\"second\", second_node)\nbuilder.add_edge(START, \"first\")\nbuilder.add_edge(\"first\", \"second\")\nbuilder.add_edge(\"second\", END)\n\n# Compile with checkpointer for persistence\nmemory = InMemorySaver()\ngraph = builder.compile(checkpointer=memory)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\nresult = graph.invoke({\"messages\": [], \"counter\": 0}, config)\nprint(result)\n# {'messages': ['Hello from first node', 'Hello from second node'], 'counter': 2}\n```\n\n--------------------------------\n\n### Access LangGraph State Securely in Tools using InjectedState in Python\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThis snippet illustrates how to use the `InjectedState` annotation to allow LangChain tools within a LangGraph agent to access the graph's state without directly exposing it to the language model. It demonstrates injecting the full state into a tool and also injecting a specific field of the state, enabling personalized tool execution based on user context or preferences.\n\n```python\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import ToolNode, InjectedState\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom typing_extensions import TypedDict, Annotated\n\nclass AgentState(TypedDict):\n    messages: Annotated[list, add_messages]\n    user_id: str\n    preferences: dict\n\n@tool\ndef personalized_search(\n    query: str,\n    state: Annotated[dict, InjectedState]  # Full state injected\n) -> str:\n    \"\"\"Search with user context.\"\"\"\n    user_id = state.get(\"user_id\", \"unknown\")\n    prefs = state.get(\"preferences\", {})\n    return f\"Results for '{query}' (user: {user_id}, prefs: {prefs})\"\n\n@tool\ndef get_user_preference(\n    key: str,\n    preferences: Annotated[dict, InjectedState(\"preferences\")]  # Specific field\n) -> str:\n    \"\"\"Get a specific user preference.\"\"\"\n    return preferences.get(key, \"Not set\")\n\ntool_node = ToolNode([personalized_search, get_user_preference])\n\n# Test direct invocation\nfrom langchain_core.messages import AIMessage\n\nstate = {\n    \"messages\": [\n        AIMessage(\n            content=\"\",\n            tool_calls=[\n                {\"name\": \"personalized_search\", \"args\": {\"query\": \"restaurants\"}, \"id\": \"1\"},\n                {\"name\": \"get_user_preference\", \"args\": {\"key\": \"cuisine\"}, \"id\": \"2\"}\n            ]\n        )\n    ],\n    \"user_id\": \"user-123\",\n    \"preferences\": {\"cuisine\": \"italian\", \"budget\": \"moderate\"}\n}\n\nresult = tool_node.invoke(state)\nfor msg in result[\"messages\"]:\n    print(f\"{msg.name}: {msg.content}\")\n```\n\n### LangGraph > StateGraph - Building Stateful Workflows\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `StateGraph` class is the primary builder for creating graphs whose nodes communicate through shared state. Nodes are Python functions that receive the current state and return partial updates. The graph must be compiled before execution using the `compile()` method. State schemas can define reducers for fields like lists that aggregate values across node updates, while other fields like counters use simple overwrite semantics. Once compiled with a checkpointer for persistence, the graph can be invoked with an initial state and configuration including a thread ID for maintaining separate execution contexts.\n\n--------------------------------\n\n### Agentic RAG > Agent State\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb\n\nWe will define a graph. A `state` object that it passes around to each node. Our state will be a list of `messages`. Each node in our graph will append to it. The `add_messages` function defines how an update should be processed within this state. By default, updates replace existing content, but `add_messages` specifies an 'append' behavior for the messages.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Control LangGraph execution flow and state with Command (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `Command` class provides programmatic control over LangGraph execution, allowing nodes to update the graph's state and direct the flow to specific subsequent nodes using the `goto` parameter. This is particularly useful for implementing complex routing logic or human-in-the-loop workflows where decisions influence the graph's path.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.types import Command, interrupt\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing_extensions import TypedDict\nfrom typing import Literal\n\nclass State(TypedDict):\n    value: int\n    approved: bool\n\ndef process_node(state: State) -> dict:\n    return {\"value\": state[\"value\"] * 2}\n\ndef approval_node(state: State) -> Command[Literal[\"finalize\", \"reject\"]]:\n    \"\"\"Request human approval.\"\"\"\n    answer = interrupt(f\"Approve value {state['value']}? (yes/no)\")\n    if answer == \"yes\":\n        return Command(update={\"approved\": True}, goto=\"finalize\")\n    return Command(update={\"approved\": False}, goto=\"reject\")\n\ndef finalize_node(state: State) -> dict:\n    return {\"value\": state[\"value\"] + 100}\n\ndef reject_node(state: State) -> dict:\n    return {\"value\": 0}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_node(\"approval\", approval_node)\nbuilder.add_node(\"finalize\", finalize_node)\nbuilder.add_node(\"reject\", reject_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", \"approval\")\nbuilder.add_edge(\"finalize\", END)\nbuilder.add_edge(\"reject\", END)\n\ngraph = builder.compile(checkpointer=InMemorySaver())\n\n# Start execution\nconfig = {\"configurable\": {\"thread_id\": \"approval-flow\"}}\nresult = graph.invoke({\"value\": 50, \"approved\": False}, config)\n# Execution pauses at interrupt\n\n# Resume with approval\nresult = graph.invoke(Command(resume=\"yes\"), config)\nprint(result)  # {'value': 200, 'approved': True}\n```\n\n--------------------------------\n\n### Implement Query Router with Cohere Command R and LangChain in Python\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb\n\nDefines a routing mechanism using Cohere Command R to direct user questions to either a web search tool or a local vector store. It uses Pydantic models to define tool schemas and `llm.bind_tools` to enable the LLM to select the appropriate tool based on the query, demonstrating dynamic tool invocation.\n\n```python\nfrom langchain_cohere import ChatCohere\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n\n# Data model\nclass web_search(BaseModel):\n    \"\"\"\n    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n    \"\"\"\n\n    query: str = Field(description=\"The query to use when searching the internet.\")\n\n\nclass vectorstore(BaseModel):\n    \"\"\"\n    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n    \"\"\"\n\n    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n\n\n# Preamble\npreamble = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n\n# LLM with tool use and preamble\nllm = ChatCohere(model=\"command-r\", temperature=0)\nstructured_llm_router = llm.bind_tools(\n    tools=[web_search, vectorstore], preamble=preamble\n)\n\n# Prompt\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nresponse = question_router.invoke(\n    {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n)\nprint(response.response_metadata[\"tool_calls\"])\nresponse = question_router.invoke({\"question\": \"What are the types of agent memory?\"})\nprint(response.response_metadata[\"tool_calls\"])\nresponse = question_router.invoke({\"question\": \"Hi how are you?\"})\nprint(\"tool_calls\" in response.response_metadata)\n```\n\n--------------------------------\n\n### Define and Build a LangGraph Workflow with Conditional Edges (Python)\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb\n\nThis Python code defines a LangGraph `StateGraph` workflow by adding various nodes such as `web_search`, `retrieve`, `grade_documents`, `generate`, and `llm_fallback`. It then establishes conditional edges using the previously defined routing and grading functions (`route_question`, `decide_to_generate`, `grade_generation_v_documents_and_question`) to control the flow based on the state, creating a dynamic RAG pipeline.\n\n```python\nimport pprint\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # rag\nworkflow.add_node(\"llm_fallback\", llm_fallback)  # llm\n\n# Build graph\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"web_search\": \"web_search\",\n        \"vectorstore\": \"retrieve\",\n        \"llm_fallback\": \"llm_fallback\",\n    },\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"web_search\": \"web_search\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",  # Hallucinations: re-generate\n        \"not useful\": \"web_search\",  # Fails to answer question: fall-back to web-search\n        \"useful\": END,\n    },\n)\nworkflow.add_edge(\"llm_fallback\", END)\n\n# Compile\napp = workflow.compile()\n```\n\n--------------------------------\n\n### Decide to Generate or Re-query in LangGraph (Python)\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb\n\nThis Python function assesses the relevance of filtered documents within the LangGraph workflow. If no relevant documents are found, it triggers a web search to re-generate a new query; otherwise, it proceeds to generate an answer. It takes the current graph state as input and returns a decision string ('web_search' or 'generate') indicating the next action.\n\n```python\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n        return \"web_search\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n```\n\n### Command - Control Flow and State Updates\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `Command` class provides programmatic control over graph execution, allowing nodes to update state and navigate to specific nodes. This is particularly useful for implementing complex routing logic or human-in-the-loop workflows where the graph's path depends on external input or dynamic conditions. For instance, a node can return a `Command` to not only modify the global state but also explicitly direct the graph to jump to a different node, bypassing default edge transitions, or even to `interrupt` execution for external input.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Integrate Store with LangGraph Checkpointer\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nCompiles a LangGraph state graph, enabling threads and integrating both a checkpointer (e.g., InMemorySaver) and the memory store. This allows for persistent state saving and arbitrary data storage across different conversational threads.\n\n```python\nfrom dataclasses import dataclass\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph\n\n@dataclass\nclass Context:\n    user_id: str\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\nbuilder = StateGraph(MessagesState, context_schema=Context)\n# ... add nodes and edges ...\ngraph = builder.compile(checkpointer=checkpointer, store=store)\n```\n\n--------------------------------\n\n### LangGraph with Async Redis Persistence\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nThis example showcases using LangGraph with an asynchronous Redis store and saver for persistence. It includes setup for the Redis store and checkpointer, defines an asynchronous model invocation function, and compiles the graph. The code demonstrates streaming responses asynchronously and retrieving user-specific information.\n\n```python\npip install -U langgraph langgraph-checkpoint-redis\n\nfrom dataclasses import dataclass\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis.aio import AsyncRedisSaver\nfrom langgraph.store.redis.aio import AsyncRedisStore  # [!code highlight]\nfrom langgraph.runtime import Runtime  # [!code highlight]\nimport uuid\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n@dataclass\nclass Context:\n    user_id: str\n\nasync def call_model(  # [!code highlight]\n    state: MessagesState,\n    runtime: Runtime[Context],  # [!code highlight]\n):\n    user_id = runtime.context.user_id  # [!code highlight]\n    namespace = (\"memories\", user_id)\n    memories = await runtime.store.asearch(namespace, query=str(state[\"messages\"][-1].content))  # [!code highlight]\n    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # Store new memories if the user asks the model to remember\n    last_message = state[\"messages\"][-1]\n    if \"remember\" in last_message.content.lower():\n        memory = \"User name is Bob\"\n        await runtime.store.aput(namespace, str(uuid.uuid4()), {\"data\": memory})  # [!code highlight]\n\n    response = await model.ainvoke(\n        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n    )\n    return {\"messages\": response}\n\nDB_URI = \"redis://localhost:6379\"\n\nasync with (\n    AsyncRedisStore.from_conn_string(DB_URI) as store,  # [!code highlight]\n    AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # await store.setup()\n    # await checkpointer.asetup()\n\n    builder = StateGraph(MessagesState, context_schema=Context)  # [!code highlight]\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        store=store,  # [!code highlight]\n    )\n\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        config,\n        stream_mode=\"values\",\n        context=Context(user_id=\"1\"),  # [!code highlight]\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    config = {\"configurable\": {\"thread_id\": \"2\"}}\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        config,\n        stream_mode=\"values\",\n        context=Context(user_id=\"1\"),  # [!code highlight]\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\n--------------------------------\n\n### Configure InMemoryStore for Semantic Search\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nInitializes an InMemoryStore configured for semantic search. This involves specifying an embedding model (e.g., OpenAI's text-embedding-3-small), the dimensionality of the embeddings, and the fields within the memory to be embedded.\n\n```python\nfrom langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n```\n\n### Using in LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nIntegrating the memory store with LangGraph enables stateful applications by allowing information to persist and be accessed across different threads or conversations. While the checkpointer handles saving and restoring graph states for individual threads, the memory store provides a mechanism to store arbitrary data that can be accessed globally or across threads. When compiling the graph using `builder.compile`, you can pass both the `checkpointer` and the `store` instances. To invoke the graph with memory support, you provide a configuration object that includes a `thread_id` for thread management and a `user_id` which is used to namespace the memories associated with that specific user, ensuring data isolation and context.\n\n--------------------------------\n\n### Memory store\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nA state schema defines the keys that are populated during graph execution. State can be persisted across graph steps using a checkpointer. However, to retain information *across threads*, such as user-specific data in a chatbot that should persist across all conversations with that user, checkpointers alone are insufficient. This is where the `Store` interface becomes necessary. An `InMemoryStore` can be defined to store information about a user across threads, and the graph can be compiled with both a checkpointer and the store.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Sync LangGraph with Postgres Checkpointer\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nProvides a synchronous example of using PostgresSaver with LangGraph. It includes setting up the chat model, defining the graph state and nodes, compiling the graph with the checkpointer, and streaming interactions with a unique thread ID for state persistence.\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres import PostgresSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\n--------------------------------\n\n### LangGraph with Redis Checkpointer (Sync)\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nThis example illustrates the synchronous usage of a Redis checkpointer with LangGraph. It includes the necessary pip installation command, highlights the `checkpointer.setup()` call (commented out), and demonstrates initializing the RedisSaver, compiling the graph, and streaming execution. The output messages are printed.\n\n```bash\npip install -U langgraph langgraph-checkpoint-redis\n```\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis import RedisSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"redis://localhost:6379\"\nwith RedisSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\n--------------------------------\n\n### LangGraph with Redis Checkpointer (Async)\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nThis code demonstrates the asynchronous implementation of a Redis checkpointer with LangGraph. It includes the installation command, highlights the `checkpointer.asetup()` call (commented out), and shows how to initialize `AsyncRedisSaver`, compile the async graph, and use `astream` for message processing. Outputs are printed asynchronously.\n\n```bash\npip install -U langgraph langgraph-checkpoint-redis\n```\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis.aio import AsyncRedisSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"redis://localhost:6379\"\nasync with AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    # await checkpointer.asetup()\n\n    async def call_model(state: MessagesState):\n        response = await model.ainvoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n```\n\n--------------------------------\n\n### Async LangGraph with Postgres Checkpointer\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nIllustrates an asynchronous implementation using AsyncPostgresSaver for LangGraph. This example mirrors the synchronous version but utilizes async/await syntax for model invocation and graph streaming, suitable for non-blocking I/O operations.\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nasync with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    # await checkpointer.setup()\n\n    async def call_model(state: MessagesState):\n        response = await model.ainvoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\n### Memory store\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\n`InMemoryStore` is appropriate for development and testing environments. For production deployments, it is recommended to use persistent storage solutions such as `PostgresStore` or `RedisStore`. All store implementations inherit from `BaseStore`, which serves as the type annotation for use within node function signatures.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Define Custom Reducer for State Updates (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis snippet demonstrates how to define a custom reducer function (`add`) to manage updates for a specific state key (`messages`). It shows how to annotate the state field with the reducer and how this simplifies the node function by automatically applying the update logic.\n\n```python\nfrom typing_extensions import Annotated\nfrom typing import Any\nfrom langchain_core.messages import AnyMessage, HumanMessage, AIMessage\nfrom langgraph.graph import StateGraph, START\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]\n    extra_field: int\n\ndef add(left, right):\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\n    return left + right\n\ndef node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}\n\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n```\n\n--------------------------------\n\n### Define Graph State with MessagesReducer (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/graph-api\n\nDemonstrates how to define a graph state that automatically handles message updates using the `add_messages` reducer. This is useful for chat applications where message history needs to be managed.\n\n```python\nfrom langchain.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n\n--------------------------------\n\n### Define State with Reducer for Parent Graph Navigation\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis snippet defines a state schema for a LangGraph, specifically highlighting the use of `Annotated` with `operator.add` to define a reducer for the 'foo' key. This is crucial when updating shared state from a subgraph to a parent graph.\n\n```python\nimport operator\nfrom typing_extensions import Annotated\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    # NOTE: we define a reducer here\n    foo: Annotated[str, operator.add]\n```\n\n### Graph API overview > State\n\nSource: https://docs.langchain.com/oss/python/langgraph/graph-api\n\nThe first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.\n\n--------------------------------\n\n### Process state updates with reducers\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified, then it is assumed that all updates to the key should override it. For `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function. For example, if a node updates a 'messages' key by appending a message, a reducer can be added to this key to ensure updates are automatically appended.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Full Example: Shared State Schemas Between Parent and Subgraphs (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n\nThis comprehensive example illustrates how a parent graph and its subgraph can communicate using shared state keys. It defines distinct state schemas for both, highlighting how updates to shared keys propagate and how private keys are managed within the subgraph.\n\n```python\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # shared with parent graph state\n    bar: str  # private to SubgraphState\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}):\n    print(chunk)\n```\n\n--------------------------------\n\n### Two Levels of Subgraphs with Different State Schemas (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n\nDemonstrates a parent graph invoking a child graph, which in turn invokes a grandchild graph, with each level having its own distinct `TypedDict` state. This showcases complex nested graph execution and state management.\n\n```python\n# Grandchild graph\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START, END\n\nclass GrandChildState(TypedDict):\n    my_grandchild_key: str\n\ndef grandchild_1(state: GrandChildState) -> GrandChildState:\n    # NOTE: child or parent keys will not be accessible here\n    return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}\n\n\ngrandchild = StateGraph(GrandChildState)\ngrandchild.add_node(\"grandchild_1\", grandchild_1)\n\ngrandchild.add_edge(START, \"grandchild_1\")\ngrandchild.add_edge(\"grandchild_1\", END)\n\ngrandchild_graph = grandchild.compile()\n\n# Child graph\nclass ChildState(TypedDict):\n    my_child_key: str\n\ndef call_grandchild_graph(state: ChildState) -> ChildState:\n    # NOTE: parent or grandchild keys won't be accessible here\n    grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}\n    grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n    return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}\n\nchild = StateGraph(ChildState)\n# We're passing a function here instead of just compiled graph (`grandchild_graph`)\nchild.add_node(\"child_1\", call_grandchild_graph)\nchild.add_edge(START, \"child_1\")\nchild.add_edge(\"child_1\", END)\nchild_graph = child.compile()\n\n# Parent graph\nclass ParentState(TypedDict):\n    my_key: str\n\ndef parent_1(state: ParentState) -> ParentState:\n    # NOTE: child or grandchild keys won't be accessible here\n    return {\"my_key\": \"hi \" + state[\"my_key\"]}\n\ndef parent_2(state: ParentState) -> ParentState:\n    return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n\ndef call_child_graph(state: ParentState) -> ParentState:\n    child_graph_input = {\"my_child_key\": state[\"my_key\"]}\n    child_graph_output = child.invoke(child_graph_input)\n    return {\"my_key\": child_graph_output[\"my_child_key\"]}\n\nparent = StateGraph(ParentState)\nparent.add_node(\"parent_1\", parent_1)\n# We're passing a function here instead of just a compiled graph (`child_graph`)\nparent.add_node(\"child\", call_child_graph)\nparent.add_node(\"parent_2\", parent_2)\n\nparent.add_edge(START, \"parent_1\")\nparent.add_edge(\"parent_1\", \"child\")\nparent.add_edge(\"child\", \"parent_2\")\nparent.add_edge(\"parent_2\", END)\n\nparent_graph = parent.compile()\n\nfor chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n    print(chunk)\n```\n\n--------------------------------\n\n### Update Graph State with Reducers\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nThis example shows how to update the state of a LangGraph execution using the `update_state` method. It illustrates how values are merged with existing state, respecting reducer functions defined for specific state channels. Channels without reducers are overwritten, while those with reducers (like `add` for lists) have their values appended.\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n\n# Assuming 'graph' is an initialized LangGraph instance and 'config' is defined\n# Initial state might be: {\"foo\": 1, \"bar\": [\"a\"]}\ngraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n```\n\n--------------------------------\n\n### Define State with Reducer for Parent Graph Navigation\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis snippet defines a state schema for a LangGraph, specifically highlighting the use of `Annotated` with `operator.add` to define a reducer for the 'foo' key. This is crucial when updating shared state from a subgraph to a parent graph.\n\n```python\nimport operator\nfrom typing_extensions import Annotated\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    # NOTE: we define a reducer here\n    foo: Annotated[str, operator.add]\n```\n\n### LangGraph Python > Update state\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nIn addition to re-playing the graph from specific `checkpoints`, we can also *edit* the graph state. We do this using [`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state). This method accepts three different arguments: `config`, `values`. The `config` should contain `thread_id` specifying which thread to update. When only the `thread_id` is passed, we update (or fork) the current state. Optionally, if we include `checkpoint_id` field, then we fork that selected checkpoint. `values` are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the [reducer](/oss/python/langgraph/graph-api#reducers) functions, if they are defined for some of the channels in the graph state. This means that [`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Python LangGraph Multi-Node Conditional Routing\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis Python code snippet illustrates how to configure conditional edges in LangGraph to route to multiple destination nodes. The `route_bc_or_cd` function returns a sequence of node names, allowing the graph to transition to more than one node in a single step based on the provided state. This is useful for parallel processing or complex branching logic.\n\n```python\nfrom typing import Sequence, Literal\n\n# Assuming 'State' is defined as in the first example\n# def route_bc_or_cd(state: State) -> Sequence[str]:\n#     if state[\"which\"] == \"cd\":\n#         return [\"c\", \"d\"]\n#     return [\"b\", \"c\"]\n\n```\n\n--------------------------------\n\n### LangGraph Graph API: Conditional Branching Example\n\nSource: https://docs.langchain.com/oss/python/langgraph/choosing-apis\n\nDemonstrates how to define conditional edges in LangGraph's Graph API for complex decision trees. It uses a TypedDict for state management and a function to determine the next node based on retry count and current tool.\n\n```python\n# Graph API: Clear visualization of decision paths\nfrom langgraph.graph import StateGraph\nfrom typing import TypedDict\n\nclass AgentState(TypedDict):\n    messages: list\n    current_tool: str\n    retry_count: int\n\ndef should_continue(state):\n    if state[\"retry_count\"] > 3:\n        return \"end\"\n    elif state[\"current_tool\"] == \"search\":\n        return \"process_search\"\n    else:\n        return \"call_llm\"\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"call_llm\", call_llm_node)\nworkflow.add_node(\"process_search\", search_node)\nworkflow.add_conditional_edges(\"call_llm\", should_continue)\n\n```\n\n--------------------------------\n\n### Python LangGraph Routing Workflow Example\n\nSource: https://docs.langchain.com/oss/python/langgraph/workflows-agents\n\nDemonstrates building a routing workflow in Python using LangGraph. It defines a schema for routing decisions, sets up state, creates nodes for different tasks (story, joke, poem), a router node, and conditional edges to direct the flow based on the LLM's decision. The workflow is compiled and invoked to process an input.\n\n```python\nfrom typing_extensions import Literal\nfrom langchain.messages import HumanMessage, SystemMessage\n\n\n# Schema for structured output to use as routing logic\nclass Route(BaseModel):\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n        None, description=\"The next step in the routing process\"\n    )\n\n\n# Augment the LLM with schema for structured output\nrouter = llm.with_structured_output(Route)\n\n\n# State\nclass State(TypedDict):\n    input: str\n    decision: str\n    output: str\n\n\n# Nodes\ndef llm_call_1(state: State):\n    \"\"\"Write a story\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_2(state: State):\n    \"\"\"Write a joke\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_3(state: State):\n    \"\"\"Write a poem\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_router(state: State):\n    \"\"\"Route the input to the appropriate node\"\"\"\n\n    # Run the augmented LLM with structured output to serve as routing logic\n    decision = router.invoke(\n        [\n            SystemMessage(\n                content=\"Route the input to story, joke, or poem based on the user's request.\"\n            ),\n            HumanMessage(content=state[\"input\"]),\n        ]\n    )\n\n    return {\"decision\": decision.step}\n\n\n# Conditional edge function to route to the appropriate node\ndef route_decision(state: State):\n    # Return the node name you want to visit next\n    if state[\"decision\"] == \"story\":\n        return \"llm_call_1\"\n    elif state[\"decision\"] == \"joke\":\n        return \"llm_call_2\"\n    elif state[\"decision\"] == \"poem\":\n        return \"llm_call_3\"\n\n\n# Build workflow\nrouter_builder = StateGraph(State)\n\n# Add nodes\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\n\n# Add edges to connect nodes\nrouter_builder.add_edge(START, \"llm_call_router\")\nrouter_builder.add_conditional_edges(\n    \"llm_call_router\",\n    route_decision,\n    {  # Name returned by route_decision : Name of next node to visit\n        \"llm_call_1\": \"llm_call_1\",\n        \"llm_call_2\": \"llm_call_2\",\n        \"llm_call_3\": \"llm_call_3\",\n    },\n)\nrouter_builder.add_edge(\"llm_call_1\", END)\nrouter_builder.add_edge(\"llm_call_2\", END)\nrouter_builder.add_edge(\"llm_call_3\", END)\n\n# Compile workflow\nrouter_workflow = router_builder.compile()\n\n# Show the workflow\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\nprint(state[\"output\"])\n\n```\n\n--------------------------------\n\n### Define Graph with Conditional Loop in Python\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis snippet demonstrates how to define a graph with a loop using `StateGraph`. It includes adding nodes, defining a conditional edge for termination based on state, and compiling the graph. The `State` is defined using `TypedDict` with an 'aggregate' list that is appended to.\n\n```python\nimport operator\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Node A sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Node B sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n# Define nodes\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\n# Define edges\ndef route(state: State) -> Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) < 7:\n        return \"b\"\n    else:\n        return END\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n```\n\n--------------------------------\n\n### Graph Compilation with Checkpointer (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph\n\nDemonstrates how to compile a LangGraph workflow, including adding nodes with retry policies and defining essential edges. It specifically shows the integration of a `MemorySaver` checkpointer to enable state persistence between runs, crucial for human-in-the-loop interactions.\n\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START\nfrom langgraph.graph.state import StateGraph\nfrom langgraph.types import RetryPolicy\n\n# Assume EmailAgentState, read_email, classify_intent, search_documentation, bug_tracking, draft_response, human_review, send_reply are defined elsewhere\n\n# Placeholder for EmailAgentState definition\nclass EmailAgentState:\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n    def get(self, key, default=None):\n        return getattr(self, key, default)\n\n# Placeholder node functions\ndef read_email(state: EmailAgentState) -> dict:\n    return {}\ndef classify_intent(state: EmailAgentState) -> dict:\n    return {}\ndef search_documentation(state: EmailAgentState) -> dict:\n    return {}\ndef bug_tracking(state: EmailAgentState) -> dict:\n    return {}\ndef draft_response(state: EmailAgentState) -> dict:\n    return {}\ndef human_review(state: EmailAgentState) -> dict:\n    return {}\ndef send_reply(state: EmailAgentState) -> dict:\n    return {}\n\n# Create the graph\nworkflow = StateGraph(EmailAgentState)\n\n# Add nodes with appropriate error handling\nworkflow.add_node(\"read_email\", read_email)\nworkflow.add_node(\"classify_intent\", classify_intent)\n\n# Add retry policy for nodes that might have transient failures\nworkflow.add_node(\n    \"search_documentation\",\n    search_documentation,\n    retry_policy=RetryPolicy(max_attempts=3)\n)\nworkflow.add_node(\"bug_tracking\", bug_tracking)\nworkflow.add_node(\"draft_response\", draft_response)\nworkflow.add_node(\"human_review\", human_review)\nworkflow.add_node(\"send_reply\", send_reply)\n\n# Add only the essential edges\nworkflow.add_edge(START, \"read_email\")\nworkflow.add_edge(\"read_email\", \"classify_intent\")\nworkflow.add_edge(\"send_reply\", END)\n\n# Compile with checkpointer for persistence\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Build and Invoke Approval Graph with Interrupt (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/interrupts\n\nProvides a full example of creating a stateful graph using LangGraph's StateGraph. It includes nodes for approval, proceeding, and canceling, utilizing the `interrupt` function to pause execution for user decisions. The graph is compiled with a MemorySaver checkpointer and invoked with initial state and configuration.\n\n```python\nfrom typing import Literal, Optional, TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass ApprovalState(TypedDict):\n    action_details: str\n    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\n\n\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\n    # Expose details so the caller can render them in a UI\n    decision = interrupt({\n        \"question\": \"Approve this action?\",\n        \"details\": state[\"action_details\"],\n    })\n\n    # Route to the appropriate node after resume\n    return Command(goto=\"proceed\" if decision else \"cancel\")\n\n\ndef proceed_node(state: ApprovalState):\n    return {\"status\": \"approved\"}\n\n\ndef cancel_node(state: ApprovalState):\n    return {\"status\": \"rejected\"}\n\n\nbuilder = StateGraph(ApprovalState)\nbuilder.add_node(\"approval\", approval_node)\nbuilder.add_node(\"proceed\", proceed_node)\nbuilder.add_node(\"cancel\", cancel_node)\nbuilder.add_edge(START, \"approval\")\nbuilder.add_edge(\"proceed\", END)\nbuilder.add_edge(\"cancel\", END)\n\n# Use a more durable checkpointer in production\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\ninitial = graph.invoke(\n    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\n    config=config,\n)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'question': ..., 'details': ...})]\n\n# Resume with the decision; True routes to proceed, False to cancel\nresumed = graph.invoke(Command(resume=True), config=config)\nprint(resumed[\"status\"])  # -> \"approved\"\n```\n\n--------------------------------\n\n### Subgraph Resumption with Interrupt (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/interrupts\n\nExplains how execution resumes in both parent graphs and subgraphs when an interrupt is triggered within a subgraph. Both the parent node and the subgraph node containing the interrupt will re-execute from their beginning.\n\n```python\ndef node_in_parent_graph(state: State):\n    some_code()  # <-- This will re-execute when resumed\n    # Invoke a subgraph as a function.\n    # The subgraph contains an `interrupt` call.\n    subgraph_result = subgraph.invoke(some_input)\n    # ...\n\ndef node_in_subgraph(state: State):\n    some_other_code()  # <-- This will also re-execute when resumed\n    result = interrupt(\"What's your name?\")\n    # ...\n\n```\n\n--------------------------------\n\n### Approval Node with Interrupt\n\nSource: https://docs.langchain.com/oss/python/langgraph/interrupts\n\nDefines a node that pauses execution using `interrupt()` to request user approval for a critical action. The `interrupt()` function takes a dictionary with details for the user and returns a value based on the user's response, which is then used to route the execution flow.\n\n```python\nfrom typing import Literal\nfrom langgraph.types import interrupt, Command\n\ndef approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\n    # Pause execution; payload shows up under result[\"__interrupt__\"]\n    is_approved = interrupt({\n        \"question\": \"Do you want to proceed with this action?\",\n        \"details\": state[\"action_details\"]\n    })\n\n    # Route based on the response\n    if is_approved:\n        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\n    else:\n        return Command(goto=\"cancel\")\n```\n\n--------------------------------\n\n### Define Tool with Interrupt for Email Sending (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/interrupts\n\nDefines a Python tool function `send_email` that uses `langgraph.types.interrupt` to pause execution before sending an email. It allows for user approval or cancellation of the email sending action. The `interrupt` function surfaces a dictionary containing the email details and a message for the user.\n\n```python\nfrom langchain.tools import tool\nfrom langgraph.types import interrupt\n\n@tool\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n    response = interrupt({\n        \"action\": \"send_email\",\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body,\n        \"message\": \"Approve sending this email?\"\n    })\n\n    if response.get(\"action\") == \"approve\":\n        # Resume value can override inputs before executing\n        final_to = response.get(\"to\", to)\n        final_subject = response.get(\"subject\", subject)\n        final_body = response.get(\"body\", body)\n        return f\"Email sent to {final_to} with subject '{final_subject}'\"\n    return \"Email cancelled by user\"\n\n```\n\n--------------------------------\n\n### Full LangGraph Example with Interruptible Email Tool (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/interrupts\n\nA comprehensive LangGraph example demonstrating an agent that uses an interruptible `send_email` tool. It sets up a graph with an agent node, uses `ChatAnthropic` as the LLM, and configures a SQLite checkpointer for state persistence. The example shows how to invoke the graph, print the interrupt payload, and resume execution with approval or modifications.\n\n```python\nimport sqlite3\nfrom typing import TypedDict\n\nfrom langchain.tools import tool\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass AgentState(TypedDict):\n    messages: list[dict]\n\n\n@tool\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n    response = interrupt({\n        \"action\": \"send_email\",\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body,\n        \"message\": \"Approve sending this email?\",\n    })\n\n    if response.get(\"action\") == \"approve\":\n        final_to = response.get(\"to\", to)\n        final_subject = response.get(\"subject\", subject)\n        final_body = response.get(\"body\", body)\n\n        # Actually send the email (your implementation here)\n        print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\n        return f\"Email sent to {final_to}\"\n\n    return \"Email cancelled by user\"\n\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\").bind_tools([send_email])\n\n\ndef agent_node(state: AgentState):\n    # LLM may decide to call the tool; interrupt pauses before sending\n    result = model.invoke(state[\"messages\"])\n    return {\"messages\": state[\"messages\"] + [result]}\n\n\nbuilder = StateGraph(AgentState)\nbuilder.add_node(\"agent\", agent_node)\nbuilder.add_edge(START, \"agent\")\nbuilder.add_edge(\"agent\", END)\n\ncheckpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\ninitial = graph.invoke(\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"}\n        ]\n    },\n    config=config,\n)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'action': 'send_email', ...})]\n\n# Resume with approval and optionally edited arguments\nresumed = graph.invoke(\n    Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\n    config=config,\n)\nprint(resumed[\"messages\"][-1])  # -> Tool result returned by send_email\n\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
