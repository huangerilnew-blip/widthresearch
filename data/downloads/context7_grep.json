{"text": "### Define Expected Agent Reasoning Trajectories\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n\nDefines two expected tool call sequences that agents should follow during reasoning. These trajectories represent the ideal execution paths for document retrieval, grading, web search, and answer generation workflows.\n\n```python\nexpected_trajectory_1 = [\n    \"retrieve_documents\",\n    \"grade_document_retrieval\",\n    \"web_search\",\n    \"generate_answer\",\n]\nexpected_trajectory_2 = [\n    \"retrieve_documents\",\n    \"grade_document_retrieval\",\n    \"generate_answer\",\n]\n```\n\n--------------------------------\n\n### Implement Decide to Generate Edge for Conditional Routing\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb\n\nEdge function that determines whether to proceed with generation or transform the query based on document relevance. Returns 'transform_query' if no relevant documents remain, otherwise returns 'generate'. Controls workflow branching based on document filtering results.\n\n```python\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n        )\n        return \"transform_query\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n```\n\n--------------------------------\n\n### Implement RAG Generation Chain with LangChain and Ollama\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n\nThis code block illustrates the construction of a Retrieval Augmented Generation (RAG) chain. It pulls a standard RAG prompt from LangChain Hub, integrates with an Ollama LLM, and defines a post-processing function to format retrieved documents. The chain combines these components to generate an answer based on a question and retrieved context, outputting a string.\n\n```python\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Prompt\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# LLM\nllm = ChatOllama(model=local_llm, temperature=0)\n\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n# Chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)\n```\n\n### Agent Inbox > Interrupts and Human Interaction\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/libs/prebuilt/README.md\n\nThe Agent Inbox feature in `langgraph-prebuilt` enables human-in-the-loop interactions within LangGraph agents through interrupts. This allows agents to pause execution and request human intervention or approval before taking certain actions. The interrupt system includes configuration options to control whether humans can ignore, respond to, edit, or accept action requests, providing fine-grained control over the types of human interactions allowed. This is particularly useful for scenarios requiring human oversight, approval workflows, or collaborative decision-making between AI agents and human operators.\n\n--------------------------------\n\n### Self-RAG\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb\n\nSelf-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. In the [paper](https://arxiv.org/abs/2310.11511), a few decisions are made to implement this strategy. These decisions include determining whether to retrieve from a retriever based on the input question or generation, assessing if the retrieved passages are relevant to the question, evaluating whether the LLM generation from each chunk is relevant to that chunk (to detect hallucinations), and finally, judging if the LLM generation from each chunk provides a useful response to the original question.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Implement Dynamic Routing with Conditional Edges in LangGraph (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThis example illustrates the use of `add_conditional_edges` for dynamic routing in a `StateGraph`. It defines a routing function that inspects the current state and determines the next node(s) to execute, showcasing how to create branching logic in workflows based on state values. This enables more complex, adaptive agent behaviors.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom typing import Literal\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    value: int\n    path_taken: str\n\ndef process_input(state: State) -> dict:\n    return {\"value\": state[\"value\"]}\n\ndef route_by_value(state: State) -> Literal[\"high_path\", \"low_path\"]:\n    \"\"\"Route based on state value.\"\"\"\n    if state[\"value\"] > 50:\n        return \"high_path\"\n    return \"low_path\"\n\ndef high_handler(state: State) -> dict:\n    return {\"path_taken\": \"high\", \"value\": state[\"value\"] * 2}\n\ndef low_handler(state: State) -> dict:\n    return {\"path_taken\": \"low\", \"value\": state[\"value\"] + 10}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_input)\nbuilder.add_node(\"high_path\", high_handler)\nbuilder.add_node(\"low_path\", low_handler)\n\nbuilder.add_edge(START, \"process\")\nbuilder.add_conditional_edges(\n    \"process\",\n    route_by_value,\n    {\"high_path\": \"high_path\", \"low_path\": \"low_path\"}\n)\nbuilder.add_edge(\"high_path\", END)\nbuilder.add_edge(\"low_path\", END)\n\ngraph = builder.compile()\n\n# Test with high value\nresult = graph.invoke({\"value\": 75, \"path_taken\": \"\"})\nprint(result)  # {'value': 150, 'path_taken': 'high'}\n\n# Test with low value\nresult = graph.invoke({\"value\": 25, \"path_taken\": \"\"})\nprint(result)  # {'value': 35, 'path_taken': 'low'}\n```\n\n--------------------------------\n\n### Control LangGraph execution flow and state with Command (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `Command` class provides programmatic control over LangGraph execution, allowing nodes to update the graph's state and direct the flow to specific subsequent nodes using the `goto` parameter. This is particularly useful for implementing complex routing logic or human-in-the-loop workflows where decisions influence the graph's path.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.types import Command, interrupt\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing_extensions import TypedDict\nfrom typing import Literal\n\nclass State(TypedDict):\n    value: int\n    approved: bool\n\ndef process_node(state: State) -> dict:\n    return {\"value\": state[\"value\"] * 2}\n\ndef approval_node(state: State) -> Command[Literal[\"finalize\", \"reject\"]]:\n    \"\"\"Request human approval.\"\"\"\n    answer = interrupt(f\"Approve value {state['value']}? (yes/no)\")\n    if answer == \"yes\":\n        return Command(update={\"approved\": True}, goto=\"finalize\")\n    return Command(update={\"approved\": False}, goto=\"reject\")\n\ndef finalize_node(state: State) -> dict:\n    return {\"value\": state[\"value\"] + 100}\n\ndef reject_node(state: State) -> dict:\n    return {\"value\": 0}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_node(\"approval\", approval_node)\nbuilder.add_node(\"finalize\", finalize_node)\nbuilder.add_node(\"reject\", reject_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", \"approval\")\nbuilder.add_edge(\"finalize\", END)\nbuilder.add_edge(\"reject\", END)\n\ngraph = builder.compile(checkpointer=InMemorySaver())\n\n# Start execution\nconfig = {\"configurable\": {\"thread_id\": \"approval-flow\"}}\nresult = graph.invoke({\"value\": 50, \"approved\": False}, config)\n# Execution pauses at interrupt\n\n# Resume with approval\nresult = graph.invoke(Command(resume=\"yes\"), config)\nprint(result)  # {'value': 200, 'approved': True}\n```\n\n--------------------------------\n\n### Implement Conditional Branching Logic in LangGraph (Python)\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n\nThis `decide_to_generate` function defines a conditional edge within the LangGraph, determining the next node to execute. It inspects the 'search' flag in the current graph state. If 'search' is 'Yes', the graph transitions to the 'search' node; otherwise, it proceeds to the 'generate' node.\n\n```python\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n    search = state[\"search\"]\n    if search == \"Yes\":\n        return \"search\"\n    else:\n        return \"generate\"\n```\n\n--------------------------------\n\n### Define and Build a LangGraph Workflow with Conditional Edges (Python)\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb\n\nThis Python code defines a LangGraph `StateGraph` workflow by adding various nodes such as `web_search`, `retrieve`, `grade_documents`, `generate`, and `llm_fallback`. It then establishes conditional edges using the previously defined routing and grading functions (`route_question`, `decide_to_generate`, `grade_generation_v_documents_and_question`) to control the flow based on the state, creating a dynamic RAG pipeline.\n\n```python\nimport pprint\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search\", web_search)  # web search\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\nworkflow.add_node(\"generate\", generate)  # rag\nworkflow.add_node(\"llm_fallback\", llm_fallback)  # llm\n\n# Build graph\nworkflow.add_conditional_edges(\n    START,\n    route_question,\n    {\n        \"web_search\": \"web_search\",\n        \"vectorstore\": \"retrieve\",\n        \"llm_fallback\": \"llm_fallback\",\n    },\n)\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"web_search\": \"web_search\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",  # Hallucinations: re-generate\n        \"not useful\": \"web_search\",  # Fails to answer question: fall-back to web-search\n        \"useful\": END,\n    },\n)\nworkflow.add_edge(\"llm_fallback\", END)\n\n# Compile\napp = workflow.compile()\n```\n\n--------------------------------\n\n### Compose Outer Graph with Subgraph Integration\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nCreate a parent graph that invokes subgraphs as nodes. The outer graph manages overall workflow orchestration while delegating specific tasks to inner graphs. Supports state aggregation and transformation across graph boundaries.\n\n```python\nfrom typing_extensions import Annotated\nimport operator\n\n# Outer graph state\nclass OuterState(TypedDict):\n    values: Annotated[list[int], operator.add]\n    current: int\n\ndef prepare(state: OuterState) -> dict:\n    return {\"current\": state[\"values\"][-1] if state[\"values\"] else 0}\n\ndef use_subgraph(state: OuterState) -> dict:\n    \"\"\"Call subgraph and integrate result.\"\"\"\n    result = inner_graph.invoke({\"value\": state[\"current\"]})\n    return {\"values\": [result[\"value\"]], \"current\": result[\"value\"]}\n\ndef finalize(state: OuterState) -> dict:\n    return {\"values\": [sum(state[\"values\"])]}\n\n# Build outer graph\nouter_builder = StateGraph(OuterState)\nouter_builder.add_node(\"prepare\", prepare)\nouter_builder.add_node(\"subgraph\", use_subgraph)\nouter_builder.add_node(\"finalize\", finalize)\nouter_builder.add_edge(START, \"prepare\")\nouter_builder.add_edge(\"prepare\", \"subgraph\")\nouter_builder.add_edge(\"subgraph\", \"finalize\")\nouter_builder.add_edge(\"finalize\", END)\n\nouter_graph = outer_builder.compile()\nresult = outer_graph.invoke({\"values\": [5], \"current\": 0})\nprint(result)\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Implement synchronous LangGraph PostgresSaver for checkpoint management in Python\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/libs/checkpoint-postgres/README.md\n\nThis code demonstrates the synchronous usage of `PostgresSaver.from_conn_string` to manage LangGraph checkpoints in a Postgres database. It shows how to initialize the saver, call `.setup()` to create necessary tables, and then perform operations to store, load, and list checkpoints.\n\n```python\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nwrite_config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_ns\": \"\"}}\nread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nDB_URI = \"postgres://postgres:postgres@localhost:5432/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    # call .setup() the first time you're using the checkpointer\n    checkpointer.setup()\n    checkpoint = {\n        \"v\": 4,\n        \"ts\": \"2024-07-31T20:14:19.804150+00:00\",\n        \"id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\",\n        \"channel_values\": {\n            \"my_key\": \"meow\",\n            \"node\": \"node\"\n        },\n        \"channel_versions\": {\n            \"__start__\": 2,\n            \"my_key\": 3,\n            \"start:node\": 3,\n            \"node\": 3\n        },\n        \"versions_seen\": {\n            \"__input__\": {},\n            \"__start__\": {\n            \"__start__\": 1\n            },\n            \"node\": {\n            \"start:node\": 2\n            }\n        }\n    }\n\n    # store checkpoint\n    checkpointer.put(write_config, checkpoint, {}, {})\n\n    # load checkpoint\n    checkpointer.get(read_config)\n\n    # list checkpoints\n    list(checkpointer.list(read_config))\n```\n\n--------------------------------\n\n### Demonstrate incorrect manual Postgres connection setup in Python\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/libs/checkpoint-postgres/README.md\n\nThis snippet illustrates an incorrect way to manually connect to Postgres for LangGraph's `PostgresSaver`. It highlights the necessity of `autocommit=True` and `row_factory=dict_row` in the `psycopg.connect` call to prevent `TypeError` exceptions during checkpointer operations and ensure table persistence.\n\n```python\n# \u274c This will fail with TypeError during checkpointer operations\nwith psycopg.connect(DB_URI) as conn:  # Missing autocommit=True and row_factory=dict_row\n    checkpointer = PostgresSaver(conn)\n    checkpointer.setup()  # May not persist tables properly\n    # Any operation that reads from database will fail with:\n    # TypeError: tuple indices must be integers or slices, not str\n```\n\n--------------------------------\n\n### Compile Graph with Checkpointer for State Persistence\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nDemonstrates how to compile a LangGraph with an InMemorySaver checkpointer to persist conversation state across multiple invocations. Uses thread_id configuration to maintain separate conversation contexts and retrieve historical checkpoints and current state snapshots.\n\n```python\nmemory = InMemorySaver()\ngraph = builder.compile(checkpointer=memory)\n\nconfig = {\"configurable\": {\"thread_id\": \"conversation-1\"}}\nresult1 = graph.invoke({\"messages\": [\"Hello!\"]}, config)\nprint(result1)\n\nresult2 = graph.invoke({\"messages\": [\"How are you?\"]}, config)\nprint(result2)\n\nfor checkpoint in memory.list(config):\n    print(f\"Checkpoint: {checkpoint.config['configurable']['checkpoint_id']}\")\n\nstate = graph.get_state(config)\nprint(f\"Current messages: {state.values['messages']}\")\n```\n\n### LangGraph > InMemorySaver - Checkpoint Persistence\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `InMemorySaver` provides in-memory checkpointing for development and testing environments. It enables state persistence across invocations, allowing graphs to be paused, resumed, and time-traveled through their execution history. This is particularly useful during development for testing agent behavior and debugging. For production deployments requiring persistent storage across application restarts, LangGraph provides `PostgresSaver` from the `langgraph-checkpoint-postgres` package, which stores checkpoints in a PostgreSQL database.\n\n--------------------------------\n\n### AGENTS Instructions > Libraries\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/AGENTS.md\n\nThe repository contains several Python and JavaScript/TypeScript libraries. Below is a high-level overview:\n- **checkpoint** \u2013 base interfaces for LangGraph checkpointers.\n- **checkpoint-postgres** \u2013 Postgres implementation of the checkpoint saver.\n- **checkpoint-sqlite** \u2013 SQLite implementation of the checkpoint saver.\n- **cli** \u2013 official command-line interface for LangGraph.\n- **langgraph** \u2013 core framework for building stateful, multi-actor agents.\n- **prebuilt** \u2013 high-level APIs for creating and running agents and tools.\n- **sdk-js** \u2013 JS/TS SDK for interacting with the LangGraph REST API.\n- **sdk-py** \u2013 Python SDK for the LangGraph Server API.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Initialize LangGraph with Redis Persistence (Python)\n\nSource: https://github.com/redis-developer/langgraph-redis/blob/main/examples/human_in_the_loop/review-tool-calls-openai.ipynb\n\nSets up a LangGraph with Redis for state persistence. It establishes a connection to a Redis instance and configures the RedisSaver for checkpointing graph states. This enables state recovery and multi-run execution.\n\n```python\nfrom typing_extensions import Literal\nfrom langgraph.graph import StateGraph, START, END, MessagesState\nfrom langgraph.checkpoint.redis import RedisSaver\nfrom langgraph.types import Command, interrupt\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom IPython.display import Image, display\n\n# Set up Redis connection\nREDIS_URI = \"redis://redis:6379\"\nmemory = None\nwith RedisSaver.from_conn_string(REDIS_URI) as cp:\n    cp.setup()\n    memory = cp\n```\n\n--------------------------------\n\n### Configure Redis Client for LangGraph Checkpoint Saver and Store\n\nSource: https://context7.com/redis-developer/langgraph-redis/llms.txt\n\nDemonstrates how to configure Redis clients for both checkpoint saving and data storage within LangGraph. It covers Azure Cache for Redis Enterprise and on-premises Redis Enterprise, as well as standard local Redis instances with and without authentication. Ensure `decode_responses=False` is set for the checkpoint saver.\n\n```python\nfrom redis import Redis\nfrom langgraph.checkpoint.redis import RedisSaver\nfrom langgraph.store.redis import RedisStore\n\n# Azure Cache for Redis Enterprise configuration\nazure_client = Redis(\n    host=\"your-cache.redisenterprise.cache.azure.net\",\n    port=10000,  # Enterprise TLS port (use 6379 for standard tier)\n    password=\"your-access-key\",\n    ssl=True,\n    ssl_cert_reqs=\"required\",  # Use \"none\" for self-signed certs\n    decode_responses=False  # Required for checkpoint saver\n)\n\n# Use pre-configured client with checkpoint saver\nsaver = RedisSaver(redis_client=azure_client)\nsaver.setup()\n\n# Or with Redis Enterprise on-premises\nenterprise_client = Redis(\n    host=\"redis-enterprise.internal.company.com\",\n    port=12000,\n    password=\"enterprise-password\",\n    ssl=True,\n    decode_responses=False\n)\n\n# Store also accepts pre-configured clients\nstore = RedisStore(enterprise_client)\nstore.setup()\n\n# For standard Redis (local/Docker)\nwith RedisSaver.from_conn_string(\"redis://localhost:6379\") as saver:\n    saver.setup()\n    # Use checkpointer...\n\n# With authentication\nwith RedisSaver.from_conn_string(\"redis://:password@localhost:6379\") as saver:\n    saver.setup()\n\n```\n\n--------------------------------\n\n### Configure Redis Checkpointer for LangGraph (Python)\n\nSource: https://github.com/redis-developer/langgraph-redis/blob/main/examples/subgraphs-manage-state.ipynb\n\nSets up a Redis connection for LangGraph's checkpointing mechanism. It demonstrates initializing RedisSaver from a connection string and calling the setup() method to prepare the Redis instance for use.\n\n```python\nfrom typing import Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.checkpoint.redis import RedisSaver\n\n# Set up Redis connection for checkpointer\nREDIS_URI = \"redis://redis:6379\"\nmemory = None\nwith RedisSaver.from_conn_string(REDIS_URI) as cp:\n    cp.setup()\n    memory = cp\n```\n\n--------------------------------\n\n### Initialize and Use RedisSaver Checkpoint Saver in Python\n\nSource: https://context7.com/redis-developer/langgraph-redis/llms.txt\n\nDemonstrates how to initialize the synchronous RedisSaver, set up Redis indices, create a LangGraph agent with persistent memory, and perform direct checkpoint operations like saving, retrieving, listing, and deleting.\n\n```python\nfrom langgraph.checkpoint.redis import RedisSaver\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the checkpoint saver with connection string\nwith RedisSaver.from_conn_string(\"redis://localhost:6379\") as checkpointer:\n    # Required: Initialize Redis indices before first use\n    checkpointer.setup()\n\n    # Create an agent with persistent memory\n    model = ChatOpenAI(model=\"gpt-4o\")\n    agent = create_react_agent(model, tools=[], checkpointer=checkpointer)\n\n    # Use thread_id for conversation persistence\n    config = {\"configurable\": {\"thread_id\": \"user-123\", \"checkpoint_ns\": \"\"}}\n\n    # First interaction\n    response = agent.invoke(\n        {\"messages\": [(\"user\", \"My name is Alice\")]},\n        config=config\n    )\n\n    # Later interaction - agent remembers context\n    response = agent.invoke(\n        {\"messages\": [(\"user\", \"What's my name?\")]},\n        config=config\n    )\n    # Agent responds with \"Alice\" due to checkpoint persistence\n\n    # Direct checkpoint operations\n    write_config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_ns\": \"\"}}\n    checkpoint = {\n        \"v\": 1,\n        \"ts\": \"2024-07-31T20:14:19.804150+00:00\",\n        \"id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\",\n        \"channel_values\": {\"my_key\": \"value\", \"node\": \"node\"},\n        \"channel_versions\": {\"__start__\": 2, \"my_key\": 3},\n        \"versions_seen\": {\"__input__\": {}, \"__start__\": {\"__start__\": 1}},\n        \"pending_sends\": [],\n    }\n\n    # Store checkpoint\n    saved_config = checkpointer.put(write_config, checkpoint, {\"source\": \"input\", \"step\": 1}, {})\n\n    # Retrieve checkpoint\n    read_config = {\"configurable\": {\"thread_id\": \"1\"}}\n    checkpoint_tuple = checkpointer.get_tuple(read_config)\n    print(f\"Checkpoint ID: {checkpoint_tuple.checkpoint['id']}\")\n\n    # List all checkpoints for a thread\n    for cp in checkpointer.list(read_config):\n        print(f\"Found checkpoint: {cp.checkpoint['id']}\")\n\n    # Delete all data for a thread\n    checkpointer.delete_thread(\"1\")\n\n```\n\n--------------------------------\n\n### Set up Redis Connection with LangGraph\n\nSource: https://github.com/redis-developer/langgraph-redis/blob/main/examples/human_in_the_loop/review-tool-calls.ipynb\n\nEstablishes a connection to Redis using a connection string and sets up the memory for the LangGraph. This is crucial for persisting graph states.\n\n```python\nREDIS_URI = \"redis://redis:6379\"\nmemory = None\nwith RedisSaver.from_conn_string(REDIS_URI) as cp:\n    cp.setup()\n    memory = cp\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Define Expected Agent Reasoning Trajectories\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n\nDefines two expected tool call sequences that agents should follow during reasoning. These trajectories represent the ideal execution paths for document retrieval, grading, web search, and answer generation workflows.\n\n```python\nexpected_trajectory_1 = [\n    \"retrieve_documents\",\n    \"grade_document_retrieval\",\n    \"web_search\",\n    \"generate_answer\",\n]\nexpected_trajectory_2 = [\n    \"retrieve_documents\",\n    \"grade_document_retrieval\",\n    \"generate_answer\",\n]\n```\n\n--------------------------------\n\n### Implement Decide to Generate Edge for Conditional Routing\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb\n\nEdge function that determines whether to proceed with generation or transform the query based on document relevance. Returns 'transform_query' if no relevant documents remain, otherwise returns 'generate'. Controls workflow branching based on document filtering results.\n\n```python\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n        )\n        return \"transform_query\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n```\n\n--------------------------------\n\n### Implement RAG Generation Chain with LangChain and Ollama\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n\nThis code block illustrates the construction of a Retrieval Augmented Generation (RAG) chain. It pulls a standard RAG prompt from LangChain Hub, integrates with an Ollama LLM, and defines a post-processing function to format retrieved documents. The chain combines these components to generate an answer based on a question and retrieved context, outputting a string.\n\n```python\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Prompt\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# LLM\nllm = ChatOllama(model=local_llm, temperature=0)\n\n\n# Post-processing\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n# Chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)\n```\n\n### Agent Inbox > Interrupts and Human Interaction\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/libs/prebuilt/README.md\n\nThe Agent Inbox feature in `langgraph-prebuilt` enables human-in-the-loop interactions within LangGraph agents through interrupts. This allows agents to pause execution and request human intervention or approval before taking certain actions. The interrupt system includes configuration options to control whether humans can ignore, respond to, edit, or accept action requests, providing fine-grained control over the types of human interactions allowed. This is particularly useful for scenarios requiring human oversight, approval workflows, or collaborative decision-making between AI agents and human operators.\n\n--------------------------------\n\n### Self-RAG\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb\n\nSelf-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. In the [paper](https://arxiv.org/abs/2310.11511), a few decisions are made to implement this strategy. These decisions include determining whether to retrieve from a retriever based on the input question or generation, assessing if the retrieved passages are relevant to the question, evaluating whether the LLM generation from each chunk is relevant to that chunk (to detect hallucinations), and finally, judging if the LLM generation from each chunk provides a useful response to the original question.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Build LangGraph Workflow with Edges and Conditional Edges\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n\nConstructs a directed graph workflow by adding edges between nodes and conditional edges with routing logic. The workflow implements a RAG pipeline that retrieves documents, grades their relevance, optionally transforms queries, generates responses, and validates generation quality. Uses START and END sentinels to mark entry and exit points.\n\n```python\nworkflow.add_edge(START, \"retrieve\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"transform_query\": \"transform_query\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"transform_query\", \"retrieve\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"transform_query\",\n    },\n)\n```\n\n--------------------------------\n\n### Grade Generation vs Documents and Question Edge - Python LangGraph\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n\nValidates the generated answer by checking for hallucinations and verifying it answers the question. Returns 'useful' if generation is grounded and answers the question, 'not useful' if grounded but doesn't answer, or 'not supported' if hallucinations are detected.\n\n```python\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    score = hallucination_grader.invoke(\n        {\"documents\": documents, \"generation\": generation}\n    )\n    grade = score[\"score\"]\n\n    # Check hallucination\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        grade = score[\"score\"]\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"\n```\n\n--------------------------------\n\n### Implement Grade Generation Edge for Hallucination and Relevance Checking\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb\n\nEdge function that validates the generated answer by checking for hallucinations against documents and verifying it answers the question. Returns 'useful' if valid, 'not useful' if it doesn't answer the question, or 'not supported' if hallucinated. Implements multi-stage quality validation.\n\n```python\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    score = hallucination_grader.invoke(\n        {\"documents\": documents, \"generation\": generation}\n    )\n    grade = score.binary_score\n\n    # Check hallucination\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"\n```\n\n--------------------------------\n\n### Implement Decide to Generate Edge for Conditional Routing\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb\n\nEdge function that determines whether to proceed with generation or transform the query based on document relevance. Returns 'transform_query' if no relevant documents remain, otherwise returns 'generate'. Controls workflow branching based on document filtering results.\n\n```python\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n        )\n        return \"transform_query\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n```\n\n### Self-RAG using local LLMs > Self-RAG Strategy\n\nSource: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag_local.ipynb\n\nThe Self-RAG framework makes four key decisions during the generation process. First, it determines whether to retrieve from the retriever based on the question or previous generation, outputting yes, no, or continue. Second, it evaluates whether retrieved passages are relevant to the question by checking if each chunk provides useful information. Third, it verifies that LLM generations are supported by the retrieved chunks, checking for hallucinations and ensuring statements are grounded in the source material. Finally, it rates the usefulness of each generation as a response to the original question on a scale from 1 to 5.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
