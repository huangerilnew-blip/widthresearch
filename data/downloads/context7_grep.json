{"text": "### Add Metadata and Tags to Traces\n\nSource: https://docs.langchain.com/oss/python/langgraph/observability\n\nAnnotate your LangSmith traces with custom metadata and tags by passing them during agent invocation or within the `tracing_context`. This helps in organizing and filtering traces.\n\n```python\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]},\n    config={\n        \"tags\": [\"production\", \"email-assistant\", \"v1.0\"],\n        \"metadata\": {\n            \"user_id\": \"user_123\",\n            \"session_id\": \"session_456\",\n            \"environment\": \"production\"\n        }\n    }\n)\n\nwith ls.tracing_context(\n    project_name=\"email-agent-test\",\n    enabled=True,\n    tags=[\"production\", \"email-assistant\", \"v1.0\"],\n    metadata={\"user_id\": \"user_123\", \"session_id\": \"session_456\", \"environment\": \"production\"}):\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}\n    )\n```\n\n--------------------------------\n\n### Stream Agent Runs with LangGraph SDK (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/deploy\n\nDemonstrates how to stream runs from a deployed LangSmith agent using the LangGraph SDK in Python. It shows how to initialize a client with deployment URL and API key, and then stream responses for a given input, printing each received update.\n\n```python\nfrom langgraph_sdk import get_sync_client # or get_client for async\n\nclient = get_sync_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")\n\nfor chunk in client.runs.stream(\n    None,    # Threadless run\n    \"agent\", # Name of agent. Defined in langgraph.json.\n    input={\n        \"messages\": [{\n            \"role\": \"human\",\n            \"content\": \"What is LangGraph?\",\n        }],\n    },\n    stream_mode=\"updates\",\n):\n    print(f\"Receiving new event of type: {chunk.event}...\")\n    print(chunk.data)\n    print(\"\\n\\n\")\n```\n\n--------------------------------\n\n### Stream Agent Runs via REST API (Bash)\n\nSource: https://docs.langchain.com/oss/python/langgraph/deploy\n\nShows how to stream runs from a deployed LangSmith agent using a cURL command. This is useful for testing the agent's API endpoint without writing Python code, by sending a POST request with the deployment URL, API key, and input payload.\n\n```bash\ncurl -s --request POST \\\n    --url <DEPLOYMENT_URL>/runs/stream \\\n    --header 'Content-Type: application/json' \\\n    --header \"X-Api-Key: <LANGSMITH API KEY> \\\n    --data \"{\n        \\\"assistant_id\\\": \\\"agent\\\", `# Name of agent. Defined in langgraph.json.`\n        \\\"input\\\": {\n            \\\"messages\\\": [\n                {\n                    \\\"role\\\": \\\"human\\\",\n                    \\\"content\\\": \\\"What is LangGraph?\\\"\n                }\n            ]\n        },\n        \\\"stream_mode\\\": \\\"updates\\\"\n    }\"\n```\n\n### LangGraph overview > LangGraph ecosystem\n\nSource: https://docs.langchain.com/oss/python/langgraph/overview\n\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\n\nLangSmith: Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\n\nLangSmith Agent Server: Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n\nLangChain: Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\n\n--------------------------------\n\n### LangGraph overview\n\nSource: https://docs.langchain.com/oss/python/langgraph/overview\n\nTrusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\n\nLangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\n\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's agents that provide pre-built architectures for common LLM and tool-calling loops.\n\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
