{"text": "### Re-assemble LangGraph with Human Review and Checkpointer (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/sql-agent\n\nThis code reassembles a LangGraph, replacing programmatic checks with human review and introducing a checkpointer for pausing and resuming execution. It defines the graph structure, nodes, edges, and initializes an InMemorySaver checkpointer.\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ndef should_continue(state: MessagesState) -> Literal[END, \"run_query\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return END\n    else:\n        return \"run_query\"\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node, \"get_schema\")\nbuilder.add_node(generate_query)\nbuilder.add_node(run_query_node, \"run_query\")\n\nbuilder.add_edge(START, \"list_tables\")\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\nbuilder.add_edge(\"get_schema\", \"generate_query\")\nbuilder.add_conditional_edges(\n    \"generate_query\",\n    should_continue,\n)\nbuilder.add_edge(\"run_query\", \"generate_query\")\n\ncheckpointer = InMemorySaver() # [!code highlight]\nagent = builder.compile(checkpointer=checkpointer) # [!code highlight]\n```\n\n--------------------------------\n\n### Compile Parent Graph with Checkpointer in Python\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n\nDemonstrates compiling a parent LangGraph with a checkpointer, automatically propagating it to child subgraphs. This is the standard way to add persistence to the main graph.\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\n--------------------------------\n\n### Subgraph Checkpointer Propagation\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nDemonstrates how LangGraph automatically propagates checkpointers to subgraphs when the parent graph is compiled with one. This ensures state consistency across nested graph structures. The example shows a parent graph compiling a subgraph, with the checkpointer being applied at the parent level.\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\n--------------------------------\n\n### Compile and Stream Graph with Checkpointer\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nCompiles a LangGraph graph with a checkpointer and demonstrates streaming messages. The checkpointer allows for state persistence across runs. Inputs are message dictionaries and configuration objects, outputs are streamed message chunks.\n\n```python\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nasync for chunk in graph.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    chunk[\"messages\"][-1].pretty_print()\n\nasync for chunk in graph.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    chunk[\"messages\"][-1].pretty_print()\n```\n\n--------------------------------\n\n### Invoke LangGraph and Inspect Result (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nInvokes the compiled LangGraph with an initial state and captures the result. The result contains the entire updated state, including the messages list and any extra fields populated by the nodes.\n\n```python\nfrom langchain.messages import HumanMessage\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\nresult\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Invoke Graph with Checkpoints (Python)\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nThis snippet demonstrates how to set up and invoke a LangGraph with an `InMemorySaver` checkpointer. It defines a simple stateful graph and invokes it, resulting in multiple checkpoints being saved. The `RunnableConfig` includes a `thread_id` for state management.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\", \"bar\":[]}, config)\n```\n\n--------------------------------\n\n### Resume LangGraph Execution After Errors with Checkpointing\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-functional-api\n\nShows how to use InMemorySaver for checkpointing to resume workflow execution after an error. Tasks that have already completed successfully are not re-run, saving computation time. This is useful for long-running or potentially failing workflows.\n\n```python\nimport time\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import StreamWriter\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n@task()\ndef get_info():\n    \"\"\"\n    Simulates a task that fails once before succeeding.\n    Raises an exception on the first attempt, then returns \"OK\" on subsequent tries.\n    \"\"\"\n    global attempts\n    attempts += 1\n\n    if attempts < 2:\n        raise ValueError(\"Failure\")  # Simulate a failure on the first attempt\n    return \"OK\"\n\n# Initialize an in-memory checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@task\ndef slow_task():\n    \"\"\"\n    Simulates a slow-running task by introducing a 1-second delay.\n    \"\"\"\n    time.sleep(1)\n    return \"Ran slow task.\"\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer: StreamWriter):\n    \"\"\"\n    Main workflow function that runs the slow_task and get_info tasks sequentially.\n\n    Parameters:\n    - inputs: Dictionary containing workflow input values.\n    - writer: StreamWriter for streaming custom data.\n\n    The workflow first executes `slow_task` and then attempts to execute `get_info`,\n    which will fail on the first invocation.\n    \"\"\"\n    slow_task_result = slow_task().result()  # Blocking call to slow_task\n    get_info().result()  # Exception will be raised here on the first attempt\n    return slow_task_result\n\n# Workflow execution configuration with a unique thread identifier\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"  # Unique identifier to track workflow execution\n    }\n}\n\n# This invocation will take ~1 second due to the slow_task execution\ntry:\n    # First invocation will raise an exception due to the `get_info` task failing\n    main.invoke({'any_input': 'foobar'}, config=config)\nexcept ValueError:\n    pass  # Handle the failure gracefully\n\n\nmain.invoke(None, config=config)\n\n```\n\n--------------------------------\n\n### Async Workflow with AsyncPostgreSQL Checkpointer in LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nIllustrates an asynchronous implementation using `AsyncPostgresSaver` for LangGraph checkpointers. This example covers async model invocation, graph streaming, and state management, suitable for non-blocking production applications.\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nasync with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    # await checkpointer.setup()\n\n    async def call_model(state: MessagesState):\n        response = await model.ainvoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\n--------------------------------\n\n### LangGraph with Redis Checkpointer (Sync)\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nThis snippet illustrates using Redis as a checkpointer for synchronous LangGraph operations. It includes the necessary installation command and demonstrates the setup and usage of the Redis checkpointer.\n\n```bash\npip install -U langgraph langgraph-checkpoint-redis\n```\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis import RedisSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"redis://localhost:6379\"\nwith RedisSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  # [!code highlight]\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  # [!code highlight]\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n```\n\n--------------------------------\n\n### Integrate MongoDB Checkpointer in LangGraph (Sync)\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nShows how to configure and use the `MongoDBSaver` for LangGraph checkpointers with a MongoDB backend. This example covers the synchronous setup for persisting graph states in a MongoDB cluster.\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.mongodb import MongoDBSaver  # [!code highlight]\n\nmodel = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\nDB_URI = \"localhost:27017\"\nwith MongoDBSaver.from_conn_string(DB_URI) as checkpointer:  # [!code highlight]\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Parallel Graph Execution with State Reducers in Python\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis Python code defines a LangGraph state and nodes to demonstrate parallel execution. It uses `operator.add` as a reducer to accumulate values in the 'aggregate' list, ensuring that new values are appended rather than overwriting existing ones. The graph structure fans out from node 'a' to 'b' and 'c', then fans in to 'd', before ending. This setup allows for concurrent execution of 'b' and 'c' within the same superstep.\n\n```python\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n--------------------------------\n\n### Define Graph with Conditional Loop in Python\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis snippet demonstrates how to define a graph with a loop using `StateGraph`. It includes adding nodes, defining a conditional edge for termination based on state, and compiling the graph. The `State` is defined using `TypedDict` with an 'aggregate' list that is appended to.\n\n```python\nimport operator\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Node A sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Node B sees {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\n# Define nodes\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\n# Define edges\ndef route(state: State) -> Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) < 7:\n        return \"b\"\n    else:\n        return END\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n```\n\n--------------------------------\n\n### Python LangGraph Conditional Branching Example\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis Python code demonstrates how to set up conditional branching in a LangGraph. It defines a state, nodes that update the state, and uses `add_conditional_edges` with a custom function to determine the next node based on the state. This allows for runtime variation in the graph's execution flow.\n\n```python\nimport operator\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n    # Add a key to the state. We will set this key to determine\n    # how we branch.\n    which: str\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}  # [!code highlight]\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"b\", END)\nbuilder.add_edge(\"c\", END)\n\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\n    # Fill in arbitrary logic here that uses the state\n    # to determine the next node\n    return state[\"which\"]\n\nbuilder.add_conditional_edges(\"a\", conditional_edge)  # [!code highlight]\n\ngraph = builder.compile()\n```\n\n--------------------------------\n\n### Update Graph State with Reducers\n\nSource: https://docs.langchain.com/oss/python/langgraph/persistence\n\nThis example shows how to update the state of a LangGraph execution using the `update_state` method. It illustrates how values are merged with existing state, respecting reducer functions defined for specific state channels. Channels without reducers are overwritten, while those with reducers (like `add` for lists) have their values appended.\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n\n# Assuming 'graph' is an initialized LangGraph instance and 'config' is defined\n# Initial state might be: {\"foo\": 1, \"bar\": [\"a\"]}\ngraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n```\n\n--------------------------------\n\n### Define State with Reducer for Parent Graph Navigation\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis snippet defines a state schema for a LangGraph, specifically highlighting the use of `Annotated` with `operator.add` to define a reducer for the 'foo' key. This is crucial when updating shared state from a subgraph to a parent graph.\n\n```python\nimport operator\nfrom typing_extensions import Annotated\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    # NOTE: we define a reducer here\n    foo: Annotated[str, operator.add]\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Parallel Graph Execution with State Reducers in Python\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis Python code defines a LangGraph state and nodes to demonstrate parallel execution. It uses `operator.add` as a reducer to accumulate values in the 'aggregate' list, ensuring that new values are appended rather than overwriting existing ones. The graph structure fans out from node 'a' to 'b' and 'c', then fans in to 'd', before ending. This setup allows for concurrent execution of 'b' and 'c' within the same superstep.\n\n```python\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n--------------------------------\n\n### Parallel Task Execution with LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-functional-api\n\nDemonstrates executing multiple tasks concurrently using the `@task` decorator and collecting results. Useful for I/O-bound operations like LLM API calls to improve performance.\n\n```python\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize checkpointer\ncheckpointer = InMemorySaver()\n\n@task\ndef add_one(number: int) -> int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -> list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n```\n\n```python\nimport uuid\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize the LLM model\nmodel = init_chat_model(\"gpt-3.5-turbo\")\n\n# Task that generates a paragraph about a given topic\n@task\ndef generate_paragraph(topic: str) -> str:\n    response = model.invoke([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n        {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n    ])\n    return response.content\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(topics: list[str]) -> str:\n    \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n    futures = [generate_paragraph(topic) for topic in topics]\n    paragraphs = [f.result() for f in futures]\n    return \"\\n\\n\".join(paragraphs)\n\n# Run the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\nprint(result)\n```\n\n--------------------------------\n\n### LangGraph Node with Async Invocation\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nDefines an asynchronous node function for a LangGraph. This node takes a `MessagesState`, invokes a language model (`llm`), and returns updated messages. It's designed to be used within a LangGraph `StateGraph`.\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import MessagesState, StateGraph\n\nasync def node(state: MessagesState):\n    new_message = await llm.ainvoke(state[\"messages\"])\n    return {\"messages\": [new_message]}\n\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\ngraph = builder.compile()\n\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\nresult = await graph.ainvoke({\"messages\": [input_message]})\n```\n\n--------------------------------\n\n### Stream Subgraph Outputs in LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n\nThis snippet shows how to enable streaming of outputs from subgraphs within a parent LangGraph. By setting `subgraphs=True` in the `stream` method, outputs from both the parent graph and its subgraphs are included in the streamed results. This is useful for monitoring and processing intermediate states within complex graph structures.\n\n```python\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    subgraphs=True, # [!code highlight]\n):\n    print(chunk)\n```\n\n--------------------------------\n\n### Using Subgraphs with Checkpointers in LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nIllustrates how to use subgraphs within a parent LangGraph. The checkpointer is compiled at the parent level and automatically propagated to subgraphs. It also shows how to compile a subgraph with its own checkpointer for independent memory.\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\n```python\nsubgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Parallel Graph Execution with State Reducers in Python\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nThis Python code defines a LangGraph state and nodes to demonstrate parallel execution. It uses `operator.add` as a reducer to accumulate values in the 'aggregate' list, ensuring that new values are appended rather than overwriting existing ones. The graph structure fans out from node 'a' to 'b' and 'c', then fans in to 'd', before ending. This setup allows for concurrent execution of 'b' and 'c' within the same superstep.\n\n```python\nimport operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n```\n\n--------------------------------\n\n### Parallel Task Execution with LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-functional-api\n\nDemonstrates executing multiple tasks concurrently using the `@task` decorator and collecting results. Useful for I/O-bound operations like LLM API calls to improve performance.\n\n```python\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize checkpointer\ncheckpointer = InMemorySaver()\n\n@task\ndef add_one(number: int) -> int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -> list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n```\n\n```python\nimport uuid\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize the LLM model\nmodel = init_chat_model(\"gpt-3.5-turbo\")\n\n# Task that generates a paragraph about a given topic\n@task\ndef generate_paragraph(topic: str) -> str:\n    response = model.invoke([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n        {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n    ])\n    return response.content\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(topics: list[str]) -> str:\n    \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n    futures = [generate_paragraph(topic) for topic in topics]\n    paragraphs = [f.result() for f in futures]\n    return \"\\n\\n\".join(paragraphs)\n\n# Run the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\nprint(result)\n```\n\n--------------------------------\n\n### LangGraph Node with Async Invocation\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-graph-api\n\nDefines an asynchronous node function for a LangGraph. This node takes a `MessagesState`, invokes a language model (`llm`), and returns updated messages. It's designed to be used within a LangGraph `StateGraph`.\n\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import MessagesState, StateGraph\n\nasync def node(state: MessagesState):\n    new_message = await llm.ainvoke(state[\"messages\"])\n    return {\"messages\": [new_message]}\n\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\ngraph = builder.compile()\n\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\nresult = await graph.ainvoke({\"messages\": [input_message]})\n```\n\n--------------------------------\n\n### Stream Subgraph Outputs in LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n\nThis snippet shows how to enable streaming of outputs from subgraphs within a parent LangGraph. By setting `subgraphs=True` in the `stream` method, outputs from both the parent graph and its subgraphs are included in the streamed results. This is useful for monitoring and processing intermediate states within complex graph structures.\n\n```python\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    subgraphs=True, # [!code highlight]\n):\n    print(chunk)\n```\n\n--------------------------------\n\n### Using Subgraphs with Checkpointers in LangGraph\n\nSource: https://docs.langchain.com/oss/python/langgraph/add-memory\n\nIllustrates how to use subgraphs within a parent LangGraph. The checkpointer is compiled at the parent level and automatically propagated to subgraphs. It also shows how to compile a subgraph with its own checkpointer for independent memory.\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n```\n\n```python\nsubgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
{"text": "### Execute LangGraph nodes in parallel using Send (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `Send` class in LangGraph enables map-reduce patterns by allowing a node to dispatch custom states to multiple parallel executions of a target node. Each `Send` object specifies the target node and the specific state it should receive, facilitating fan-out operations within a `StateGraph`.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.types import Send\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\nclass OverallState(TypedDict):\n    topics: list[str]\n    summaries: Annotated[list[str], operator.add]\n\ndef fan_out(state: OverallState) -> list[Send]:\n    \"\"\"Create parallel Send for each topic.\"\"\"\n    return [Send(\"summarize\", {\"topic\": t}) for t in state[\"topics\"]]\n\ndef summarize(state: TopicState) -> dict:\n    \"\"\"Process individual topic.\"\"\"\n    return {\"summaries\": [f\"Summary of {state['topic']}\"]}\n\nbuilder = StateGraph(OverallState)\nbuilder.add_node(\"summarize\", summarize)\nbuilder.add_conditional_edges(START, fan_out)\nbuilder.add_edge(\"summarize\", END)\n\ngraph = builder.compile()\nresult = graph.invoke({\n    \"topics\": [\"AI\", \"ML\", \"Deep Learning\"],\n    \"summaries\": []\n})\nprint(result[\"summaries\"])\n# ['Summary of AI', 'Summary of ML', 'Summary of Deep Learning']\n```\n\n--------------------------------\n\n### Implement Dynamic Routing with Conditional Edges in LangGraph (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThis example illustrates the use of `add_conditional_edges` for dynamic routing in a `StateGraph`. It defines a routing function that inspects the current state and determines the next node(s) to execute, showcasing how to create branching logic in workflows based on state values. This enables more complex, adaptive agent behaviors.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom typing import Literal\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    value: int\n    path_taken: str\n\ndef process_input(state: State) -> dict:\n    return {\"value\": state[\"value\"]}\n\ndef route_by_value(state: State) -> Literal[\"high_path\", \"low_path\"]:\n    \"\"\"Route based on state value.\"\"\"\n    if state[\"value\"] > 50:\n        return \"high_path\"\n    return \"low_path\"\n\ndef high_handler(state: State) -> dict:\n    return {\"path_taken\": \"high\", \"value\": state[\"value\"] * 2}\n\ndef low_handler(state: State) -> dict:\n    return {\"path_taken\": \"low\", \"value\": state[\"value\"] + 10}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_input)\nbuilder.add_node(\"high_path\", high_handler)\nbuilder.add_node(\"low_path\", low_handler)\n\nbuilder.add_edge(START, \"process\")\nbuilder.add_conditional_edges(\n    \"process\",\n    route_by_value,\n    {\"high_path\": \"high_path\", \"low_path\": \"low_path\"}\n)\nbuilder.add_edge(\"high_path\", END)\nbuilder.add_edge(\"low_path\", END)\n\ngraph = builder.compile()\n\n# Test with high value\nresult = graph.invoke({\"value\": 75, \"path_taken\": \"\"})\nprint(result)  # {'value': 150, 'path_taken': 'high'}\n\n# Test with low value\nresult = graph.invoke({\"value\": 25, \"path_taken\": \"\"})\nprint(result)  # {'value': 35, 'path_taken': 'low'}\n```\n\n--------------------------------\n\n### Control LangGraph execution flow and state with Command (Python)\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThe `Command` class provides programmatic control over LangGraph execution, allowing nodes to update the graph's state and direct the flow to specific subsequent nodes using the `goto` parameter. This is particularly useful for implementing complex routing logic or human-in-the-loop workflows where decisions influence the graph's path.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.types import Command, interrupt\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing_extensions import TypedDict\nfrom typing import Literal\n\nclass State(TypedDict):\n    value: int\n    approved: bool\n\ndef process_node(state: State) -> dict:\n    return {\"value\": state[\"value\"] * 2}\n\ndef approval_node(state: State) -> Command[Literal[\"finalize\", \"reject\"]]:\n    \"\"\"Request human approval.\"\"\"\n    answer = interrupt(f\"Approve value {state['value']}? (yes/no)\")\n    if answer == \"yes\":\n        return Command(update={\"approved\": True}, goto=\"finalize\")\n    return Command(update={\"approved\": False}, goto=\"reject\")\n\ndef finalize_node(state: State) -> dict:\n    return {\"value\": state[\"value\"] + 100}\n\ndef reject_node(state: State) -> dict:\n    return {\"value\": 0}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_node(\"approval\", approval_node)\nbuilder.add_node(\"finalize\", finalize_node)\nbuilder.add_node(\"reject\", reject_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", \"approval\")\nbuilder.add_edge(\"finalize\", END)\nbuilder.add_edge(\"reject\", END)\n\ngraph = builder.compile(checkpointer=InMemorySaver())\n\n# Start execution\nconfig = {\"configurable\": {\"thread_id\": \"approval-flow\"}}\nresult = graph.invoke({\"value\": 50, \"approved\": False}, config)\n# Execution pauses at interrupt\n\n# Resume with approval\nresult = graph.invoke(Command(resume=\"yes\"), config)\nprint(result)  # {'value': 200, 'approved': True}\n```\n\n--------------------------------\n\n### Stream Responses from LangGraph Runs\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nStream responses from LangGraph runs in real-time using the stream method. Supports different stream modes to receive updates at various granularities. Works with both synchronous and asynchronous clients for responsive user interfaces.\n\n```python\n# Synchronous streaming\nfor chunk in client.runs.stream(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=\"agent\",\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]},\n    stream_mode=\"messages\"\n):\n    print(chunk)\n\n# Asynchronous streaming\nasync for chunk in async_client.runs.stream(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=\"agent\",\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}]}\n):\n    print(chunk)\n```\n\n--------------------------------\n\n### Implement LangGraph Streaming Modes for Real-time Output in Python\n\nSource: https://context7.com/langchain-ai/langgraph/llms.txt\n\nThis snippet demonstrates how to use LangGraph's streaming capabilities to get real-time output from graph execution. It shows a node emitting custom events via `StreamWriter` and then illustrates how to consume these streams using different `stream_mode` options like 'updates', 'values', and 'custom' to observe intermediate states or specific event types.\n\n```python\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.types import StreamWriter\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\nclass State(TypedDict):\n    messages: Annotated[list[str], operator.add]\n\ndef node_with_streaming(state: State, writer: StreamWriter) -> dict:\n    \"\"\"Node that emits custom stream events.\"\"\"\n    writer({\"event\": \"processing_start\", \"count\": len(state[\"messages\"])})\n\n    # Simulate processing\n    for i in range(3):\n        writer({\"event\": \"progress\", \"step\": i + 1})\n\n    writer({\"event\": \"processing_complete\"})\n    return {\"messages\": [\"Processing complete\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", node_with_streaming)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", END)\ngraph = builder.compile()\n\n# Stream with different modes\nprint(\"=== Updates mode ===\")\nfor chunk in graph.stream({\"messages\": [\"Hello\"]}, stream_mode=\"updates\"):\n    print(chunk)\n\nprint(\"\\n=== Values mode ===\")\nfor chunk in graph.stream({\"messages\": [\"Hello\"]}, stream_mode=\"values\"):\n    print(chunk)\n\nprint(\"\\n=== Custom mode ===\")\nfor chunk in graph.stream({\"messages\": [\"Hello\"]}, stream_mode=\"custom\"):\n    print(chunk)\n```", "metadata": {"tool": "query-docs", "source": "context7_query_docs"}}
