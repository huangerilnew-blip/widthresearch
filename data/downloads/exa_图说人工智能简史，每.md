# 图说人工智能简史，每一张图片都是一个里程碑

**URL**:
https://cloud.tencent.com/developer/article/2491938

## 元数据
- 发布日期: 2026-02-03T16:00:29.783108

## 完整内容
---
图说人工智能简史，每一张图片都是一个里程碑-腾讯云开发者社区-腾讯云
[] 
[AI大眼萌
] 
## 图说人工智能简史，每一张图片都是一个里程碑原创**关注作者
[*腾讯云*] 
[*开发者社区*] 
[文档] [建议反馈] [控制台] 
登录/注册
[首页] 
学习活动专区圈层工具[MCP广场![]] 
文章/答案/技术大牛搜索**
搜索**关闭**
发布AI大眼萌
**
**
**
**
**
[社区首页] &gt;[专栏] &gt;图说人工智能简史，每一张图片都是一个里程碑
# 图说人工智能简史，每一张图片都是一个里程碑原创![作者头像] 
AI大眼萌
**关注
修改于2025-02-11 08:16:03
修改于2025-02-11 08:16:03
18.9K1
举报在人类文明的漫长历程中，对于智慧的追求从未停歇。自古代哲学家对逻辑和推理的探索，到20世纪计算机科学的诞生，我们见证了人工智能（Artificial Intelligence, AI）从概念的萌芽到技术的蓬勃发展。人工智能，作为计算机科学的一个分支，其核心目标是模拟人类思维，赋予机器学习、推理乃至创造的能力。AI大眼萌将带大家回顾人工智能发展的各个阶段。
人工智能(Artificial Intelligence，AI) 是计算机科学的一个分支领域，致力于让机器模拟人类思维，执行学习、推理等工作。人工智能的发展经历了以下六个阶段。* **前导：**萌芽阶段
* **第一阶段-AI兴起**：人工智能的诞生（1941- 1956）
* **第二阶段-AI早期成功**：AI黄金发展时代（1956-1974）
* **第三阶段-AI第一次寒冬**：神经网络遇冷，研究经费减少（1974\~1980）
* **第四阶段-AI复兴**：第二次AI黄金发展时代，专家系统流行并商用（1980\~1987）
* **第五阶段-AI第二次寒冬**：专家系统溃败，研究经费大减（1987\~1993）
* **第六阶段-AI崛起**：深度学习理论和工程突破（1993至今）
![图片] 
图片**00**
**前导：萌芽阶段**
人工智能是建立在人类思维过程可以机械化的假设之上的。中国、印度和希腊的哲学家都在公元前一千年发展出了形式演绎的结构化方法。几个世纪以来，亚里士多德Aristotle（他对三段论*Syllogism*进行形式分析）、欧几里得Euclid（他的*《几何原本》*是形式推理的一个模型）、阿尔·花拉子模al-Khwārizmī（他发展了代数，并以自己的名字命名了“*算法”*一词）等都发展了他们的思想。从古代到现在对逻辑和形式推理的研究直接导致了20世纪40年代可编程数字计算机的发明，这是一种基于抽象数学推理的机器。这个装置及其背后的想法激发了科学家们开始讨论建造电子大脑的可能性。
![图片] 
图片Al-Jazari&#x27;s programmable automata (1206 CE)
**01**
**第一阶段-AI兴起：人工智能的诞生（1941- 1956）**
* 1943**人工神经元模型Artificial\_neuron**
自1943年起，**沃尔特·皮茨Walter Pitts**和**沃伦·麦卡洛克Warren McCulloch**携手提出了**人工神经元模型Artificial\_neuron**，即阈值逻辑单元（TLU），为神经网络研究奠定了基石。有趣的是，这两位大师相差25岁，却意外地在同一年离世。皮茨更是个极其低调的人，即便有人出钱，也不愿透露自己的姓名。
![图片] 
图片![图片] 
图片* **1945年**，艾伦·图灵Alan Turing就已经在考虑如何用计算机来模拟人脑了。他设计了 ACE（Automatic Computing Engine- 自动计算引擎）来模拟大脑工作。在给一位同事的信中写道：&quot;与计算的实际应用相比，我对制作大脑运作的模型可能更感兴趣 ...... 尽管大脑运作机制是通过轴突和树突的生长来计算的复杂神经元回路，但我们还是可以在ACE 中制作一个模型，允许这种可能性的存在，ACE 的实际构造并没有改变，它只是记住了数据......&quot; 这就是机器智能的起源![图片] 
图片* **1950年**，艾伦·图灵Alan Turing发表了《计算机器与智能》，提出了著名的“图灵测试”，标志着**人工智能**概念的初步形成。
![图片] 
图片![图片] 
图片* **1951**年，Marvin Lee Minsky 与Dean Edmonds 一道建造了第一台神经网络机，称为SNARC
![图片] 
图片* **1956年**夏天，在美国新罕布什尔州汉诺斯小镇的达特茅斯学院，一群科学家聚集在一起，讨论了关于设计智能机器的可能性。约翰·麦卡锡、马文·明斯基等人首次提出了“人工智能”这一术语，此次达特茅斯会议Dartmouth被视为**人工智能学科**的正式诞生。会议上最引人瞩目的成果，是赫伯特·西蒙Herbert Simon和艾伦·纽厄尔Alan Newell介绍的一个程序“逻辑理论家”Logic Theorist，这个程序可以证明伯特兰·罗素Bertrand Russell和艾尔弗雷德·诺思·怀特海Alfred North Whitehead合著的《数学原理》中命题逻辑部分的一个很大子集，“逻辑理论家”程序被许多人认为是第一款可工作的人工智能程序。
![图片] 
图片![图片] 
图片1956年8月从左至右：Oliver Selfridge, Nathaniel Rochester, Ray Solomonoff, Marvin Minsky, Trenchard More, John McCarthy, Claude Shannon.。
![图片] 
图片![图片] 
图片* **人工智能三大学派**
在人工智能的热潮中，涌现了从不同的学科背景出发的三大学派：![图片] 
图片* **连接主义connectionism**：又称为仿生学派或生理学派，包含感知器，人工神经网络，深度学习等技术。代表人物有**罗森布莱特**（Frank Rosenblatt）等。
* 主张智能可以通过模拟大脑神经元网络来实现。* 强调使用神经网络和学习算法来处理信息。* 深度学习、卷积神经网络（CNN）和循环神经网络（RNN）是这一流派的现代发展。
![图片] 
图片连接主义的代表：多层神经网络* **符号主义symbolism**：又称为逻辑主义、心理学派或计算机学派。包含决策树，专家系统等技术。代表人物有西蒙和纽厄尔、马文·明斯基等。各类决策树相关的算法，均受益于符号主义流派。
* 主张智能可以通过符号操作来实现。* 强调使用逻辑、规则和符号来模拟人类思维过程。* 知识图谱是大数据时代的知识工程集大成者，是符号主义与连接主义相结合的产物，是实现认知智能的基石。![图片] 
图片* **行为主义**：又称为进化主义或控制论学派，包含控制论、马尔科夫决策过程、遗传算法、强化学习和某些类型的机器人技术等技术。代表人物有萨顿（Richard Sutton）等。
* 也称为进化主义或控制论，主张智能行为可以通过与环境的交互来学习。* 强调通过试错和自然选择来优化行为。* 行为主义在后来的机器人学、自动化控制、游戏AI、自动驾驶汽车等领域有着重要应用
![图片] 
图片麻省理工学院制造的六足机器人Genghis（成吉思汗）
**02**
**第二阶段-AI**
**早期成功：AI黄金发展时代（1956-1974年）**
* **1957 年**，美国心理学家弗兰克·罗森布拉特Frank Rosenblatt在康奈尔航空实验室发明了一个**早期的神经网络early neural networks- 感知器模型（Perceptron Model）**，感知器的设计包含三个部分：输入层、隐藏层和输出层。输入层由400个光敏元件组成，用于模拟视网膜的功能；隐藏层包含512个步进电动机，模拟神经元的兴奋和抑制过程；输出层则连接了8个执行器单元。通过“反向传播误差校正”原理，感知器可以不断调整自身的参数以提高分类准确率，从而在处理线性可分的分类问题上表现出良好的学习能力。
![图片] 
图片感知机模型：![图片] 
图片![图片] 
图片打个比方：![图片] 
图片1958年，纽约时报记者对人工智能未来的畅想。
![图片] 
图片* **1959年**，亚瑟·塞缪尔Arthur Samuel开发了首个自学习程序——西洋跳棋程序，并引入了“**机器学习Machine Learning”**这一概念。
![图片] 
图片* **1960年**，Frank Rosenblatt 获得了美国海军研究办公室信息系统分支和罗马航空发展中心的资助，建造了一台定制的计算机Mark I感知器。
![图片] 
图片![图片] 
图片* **1966年**，约瑟夫·魏岑鲍姆开发了 ELIZA，这是一个早期的**自然语言Natural language**处理程序。最著名的脚本DOCTOR模拟了Rogerian学派的心理治疗师（治疗师经常将患者的话反映给患者）,并使用脚本中规定的规则，对用户输入的非方向性问题做出回应。因此，ELIZA是第一个聊天机器人（现代的“聊天机器人”）和第一个能够尝试图灵测试的程序之一，展示了机器与人类进行自然语言交流的可能性。ELIZA可以说是现在Siri、小爱同学等问答交互工具的鼻祖。
![图片] 
图片* **1969年**，马文·明斯基Marvin Minsky和西摩·帕珀特Seymour Papert出版的《感知器：计算几何学导论》一书，对罗森布莱特的感知器提出了质疑。书中指出：单层感知器本质上是一个线性分类器，无法求解非线性分类问题，甚至连简单的异或（XOR）问题都无法求解。人们通常错误地认为，他们也证明了类似的结果适用于多层感知器网络。然而，这是不正确的，因为Minsky和Papert已经知道多层感知器能够产生XOR函数。经常被错误引用的Minsky和Papert文本导致神经网络研究的兴趣和资金大幅下降，导致**神经网络研究**一度陷入低谷。这些先驱们怎么也没想到，计算机的速度能够在随后的几十年里指数级增长，提升了上亿倍。
![图片] 
图片单层感知机：无法将蓝、红两类点用一条直线分开在两边。![图片] 
图片* **1970年**，第一个拟人机器人WABOT-1在日本早稻田大学建成。它由一个肢体控制系统、一个视觉系统和一个对话系统组成。
![图片] 
图片**03**
**第三阶段-AI第一次寒冬：神经网络遇冷，研究经费减少（1974\~1980）**
在20 世纪70 年代，人工智能受到批评和财务挫折。人工智能研究人员未能意识到他们所面临问题的难度。他们的巨大乐观情绪提高了公众的期望，而当承诺的结果未能实现时，针对人工智能的资金就被严重减少。1973年英国科学研究委员会消减对AI研究的资助。1973\~1974 年，美国DARPA 大幅削减对AI研究的资助，到1974年，已经很难再找到对AI项目的资助了。
![图片] 
图片* **1974 年**，哈佛大学沃伯斯（Paul Werbos）博士论文里，首次提出了通过误差的反向传播（BP）来训练人工神经网络，但在该时期未引起重视。
![图片] 
图片* 在20 世纪七八十年代的“寒冬”里，仍有一些人执着于神经网络研究，科学界把他们视为狂热的疯子。比如，芬兰人戴沃·科霍宁（Teuvo Kohonen），他研究的是一个与神经网络比较接近的课题—联想记忆。再比如，还有一群日本人，与西方不同，日本的工程科学生态系统比较孤立，其中包括数学家甘利俊一Shun-Ichi Amari和一位名为福岛邦彦Kunihiko Fukushima的业内人士，后者发布了一个被他称为**认知机****Congitron**的机器，这一命名来自术语**感知机****Perceptron**。福岛邦彦前后一共发布了这个机器的两个版本，分别是 20 世纪70 年代的认知机和**1979年**发布的**神经认知机****Neocognitron**，它是一种分层、多层人工神经网络，通过无监督学习，用于日语手写字符识别和其他模式识别任务，并成为卷积神经网络的灵感来源。
![图片] 
图片**04**
**第四阶段-AI复兴：第二次AI黄金发展时代，专家系统流行并商用（1980\~1987）**
* 专家系统的兴起:AI的第一次寒冬，让研究者们的研究热点，转向了专家系统。专家系统，是模仿人类专家决策能力的计算机系统。依据一组从专门知识中推演出的逻辑规则，来回答特定领域中的问题。专家系统包含若干子系统：知识库，推理引擎，用户界面。
![图片] 
图片知识库系统和知识工程成为80年代AI研究的主要方向，出现了许多有名的专家系统。
* MYCIN：识别可能导致急性感染的各种细菌，根据患者的体重推荐药物。
* DENDRAL：用于化学分析，可预测分子结构。
* PXDES：用于预测肺癌程度和类型。
* XCON：1980年由CMU为DEC设计，1986年之前每年为DEC省下四千万美金。
专家系统具有明显的一些优势：* 设计简单，且能够容易地编程实现或修改* 实践证明了专家系统的实用性和经济价值* 高效、准确、迅速和不知疲倦地进行工作* 使领域专家的经验不受时间和空间的限制专家系统的这一系列优势，吸引了新一轮的政府资助。1981年，日本经济产业省拨款八亿五千万美元支持第五代计算机项目，目标是造出能与人对话，翻译语言，解释图像，并像人一样推理的机器，英国开始了耗资三亿五千万英镑的Alvey工程。DARPA成立战略计算促进会，1988年向AI的投资是1984年的三倍。
* **1982 年**，物理学家约翰·霍普菲尔德John Hopfield证明了一种神经网络（现在称为“**霍普菲尔德网络Hopfield net**&quot;”）可以学习和处理信息，并在任何固定条件下经过足够的时间后可证明收敛，因为之前人们认为非线性网络通常会混乱地演化。1986年，David Rumelhart和杰弗里·辛顿Geoffrey Hinton推广了一种适用于多层感知器（MLP）的算法，称为“**反向传播算法Backpropagation**”的神经网络训练方法，推动了多层神经网络的发展。这两项进展重新点燃了**神经网络研究**的热潮。
![图片] 
图片多层感知器multilayer perceptron (MLP）
![图片] 
图片![图片] 
图片* **1985 年**，朱迪亚·珀尔Judea Pearl提出贝叶斯网络，以倡导人工智能的概率方法和发展贝叶斯网络而闻名，还因发展了一种基于结构模型的因果和反事实推理理论而受到赞誉。
![图片] 
图片![图片] 
图片* **1985年**，杰弗里·辛顿Geoffrey Hinton提出**受限玻尔兹曼机Restricted Boltzmann machine**。受限玻尔兹曼机是一种二分图结构，包含可见单元和隐藏单元。其训练算法是基于梯度的对比分歧算法，可以用于降维、分类、回归和特征学习等任务。
![图片] 
图片**05**
**第五阶段-AI第二次寒冬：专家系统溃败，研究经费大减（1987\~1993）**
在专家系统快速发展的过程中，其劣势也逐渐显露出来。专家系统的劣势有：* 知识采集和获取的难度很大，系统建立和维护费用高。* 专家系统仅限应用于某些特定情景，不具备通用性。* 使用者需要花很长时间来熟悉系统的使用“AI 之冬”一词由经历过1974 年经费削减的研究者们创造出来。他们注意到了对专家系统的狂热追捧，预计不久后人们将转向失望。**专家系统的这些劣势，使得商业化面临重重困境，从而直接引发了AI的第二次寒冬**
* 变天的最早征兆是1987 年AI 硬件市场需求的突然下跌。Apple 和IBM 生产的台式机性能不断提升，到1987 年时其性能已经超过了Symbolics 和其他厂家生产的昂贵的Lisp 机。老产品失去了存在的理由：一夜之间这个价值五亿美元的产业土崩瓦解* 80年代晚期，战略计算促进会大幅削减对AI的资助。
* DARPA认为AI并非“下一个浪潮”，拨款倾向于更容易出成果的项目。
* 1991年，日本的“第五代计算机项目”的目标未能实现，事实上其中一些目标，比如“与人展开交谈”，直到 2010 年也没有实现。与其他AI 项目一样，期望比真正可能实现的要高得多。* **1989年**，AT＆T贝尔实验室的**杨立昆Yann Lecun**和团队使用**卷积神经网络convolutional neural networks CNN**技术，实现了人工智能识别手写的邮政编码数字图像，成为深度学习在实践中的早期成功案例。
![图片] 
图片1980年代是人工智能研究方向发生重大转折的时期。机器学习和神经网络（联结主义）加速崛起，逐渐取代专家系统（符号主义），成为人工智能的主要研究方向。我们也可以理解为，人工智能原本由知识驱动的方式，逐渐变成了由数据驱动。
![图片] 
图片* **1991年**，互联网的出现使在线连接和数据共享成为可能，无论你是谁，无论你在哪里。由于数据是人工智能的燃料，这在以后将被理解为人工智能的一个关键时刻。
![图片] 
图片**06**
**第六阶段-AI崛起：深度学习理论和工程突破（1993至今）**
![图片] 
图片* **深度学习三巨头**
少数AI研究者在AI寒冬期以众人皆醉我独醒的态度，十年如一日地坚持坐冷板凳，开展神经网络方向的研究。其中代表人物是深度学习三巨头。**他们在2018年因在深度学习方面的卓越贡献，一同被授予了图灵奖**。
* 杰弗里·辛顿Jeoffrey Hinton：发明了受限玻尔兹曼机，首先将反向传播算法应用于多层神经网络。培养了杨立昆Yann Lecun等一众大牛级学生。推动谷歌的图像和音频识别性能大幅提升。
![图片] 
图片我一直以来都确信，实现人工智能的唯一方式，就是按人类大脑的方式去进行计算。——杰弗里·辛顿* 杨立昆Yann Lecun：1989年使用反向传播和神经网络识别手写数字，用来读取银行支票上的手写数字，首次实现神经网络商业化，1998 ，提出LeNet5卷积神经网络，Facebook人工智能实验室负责人。
![图片] 
图片我们之所以为人，是因为我们具有智能，而人工智能是这一能力的扩展。——杨立昆* 约书亚·本吉奥Yoshua Bengio：推动了循环神经网络的发展，带领开发出Theano框架，启发了Tensorflow等众多后续框架的发展，创办AI顶会ICLR，开创了基于神经网络的语言模型。他也是权威教材《深度学习》一书的合著者。
![图片] 
图片我一直认为“创造性”可通过计算的方式来实现。我们理解计算背后的原理。所以，只需找到更智能的神经网络或模型即可。——约书亚·本吉奥* **1995 年**，克里娜·柯尔特斯Corinna Cortes和弗拉基米尔·万普尼克Vladimir Vapnik提出联结主义经典的支持向量机（Support Vector Machine ,SVM），可以视为在感知机基础上的改进，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中
![图片] 
图片* **1997 年**，IBM 深蓝超级计算机战胜了国际象棋世界冠军卡斯帕罗夫，成为首个在标准比赛时限内击败国际象棋世界冠军的电脑系统。![图片] 
图片* **2000年**，麻省理工学院的辛西娅-布雷泽尔开发了Kismet，一种能够识别和模拟情绪的机器人。
![图片] 
图片* **2003 年**，Google 公布3 篇大数据奠基性论文，为大数据存储及分布式处理的核心问题提供了思路：非结构化文件分布式存储（GFS）、分布式计算（MapReduce）及结构化数据存储（BigTable），奠定了现代大数据技术的理论基础。
![图片] 
图片* **2006年**，杰弗里·辛顿等人发表了重要的论文《Reducing the dimensionality of data with neural networks（用神经网络降低数据维数）》, 提出了**深度信念网络Deep Belief Network(DBN)**，用于无监督特征学习，为深度学习的发展奠定了基础，**2006 年也被称为深度学习元年**。
![图片] 
图片* **2006年**，英伟达（NVIDIA）推出CUDA （统一计算架构），GPU开始用于解决商业、工业以及科学方面的复杂计算，GPU与深度学习结合，模型的训练速度有了数量级的提升。
![图片] 
图片* **2012年**，在杰弗里·辛顿的指导下，伊利亚·苏茨克沃Ilya Sutskever和亚历克斯·克里切夫斯基Alex Krizhevsky开发出 AlexNet 模型，推动了**深度卷积神经网络CNN**的发展。AlexNet在ImageNet挑战赛上取得了突破性的成果，从而引发了深度学习Deep Learning的热潮。值得一提的是，他们三人用于训练模型的，只是2张英伟达GTX 580显卡。GPU在深度神经网络训练上表现出的惊人能力，不仅让他们自己吓了一跳，也让黄仁勋和英伟达公司吓了一跳。
![图片] 
图片作为对比，2012年的早些时候，谷歌“Google Brain”项目的研究人员吴恩达（华裔美国人，1976年生于伦敦）、杰夫·迪恩Jeff Dean等人，也捣鼓了一个神经网络（10亿参数），用来训练对猫的识别。他们的训练数据是来自youtube的1000万个猫脸图片，用了1.6万个CPU，整整训练了3天。
![图片] 
图片深度学习Deep Learning是一Machine Learning 的一个重要分支,更准确来说，机器学习底下有一条“神经网络”路线，而深度学习，是加强版的“神经网络”学习, 它使用多层神经网络和反向传播Backpropagation技术来训练神经网络。经典机器学习算法使用的神经网络，具有输入层、一个或两个“隐藏”层和一个输出层。数据需要由人类专家进行结构化或标记（监督学习），以便算法能够从数据中提取特征。


---
*数据来源: Exa搜索 | 获取时间: 2026-02-03 16:00:59*