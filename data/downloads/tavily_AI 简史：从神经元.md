* [博客](https://blog.csdn.net/)
* [下载](https://download.csdn.net/)
* [社区](https://devpress.csdn.net/)
* [GitCode](https://link.csdn.net?target=https%3A%2F%2Fgitcode.com%3Futm_source%3Dcsdn_toolbar)
* [GPU算力](https://ai.csdn.net/)
* 更多

  [会议](https://www.bagevent.com/event/9117243 "会议")[学习](https://edu.csdn.net?utm_source=zhuzhantoolbar "高质量课程·大会云会员")[InsCode](https://inscode.net?utm_source=csdn_blog_top_bar "InsCode")

AI 搜索

# AI 简史：从神经元到现代大模型

原创
已于 2024-12-25 16:28:52 修改
·
1.8w 阅读

·

49

·
89
·

CC 4.0 BY-SA版权

版权声明：本文为博主原创文章，遵循 [CC 4.0 BY-NC-SA](http://creativecommons.org/licenses/by-nc-sa/4.0/) 版权协议，转载请附上原文出处链接和本声明。

文章标签：

[#深度学习](https://so.csdn.net/so/search/s.do?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[#人工智能](https://so.csdn.net/so/search/s.do?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[#ai](https://so.csdn.net/so/search/s.do?q=ai&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[#神经网络](https://so.csdn.net/so/search/s.do?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[#transformer](https://so.csdn.net/so/search/s.do?q=transformer&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[#卷积神经网络](https://so.csdn.net/so/search/s.do?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)
[#机器学习](https://so.csdn.net/so/search/s.do?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&t=all&o=vip&s=&l=&f=&viparticle=&from_tracking_code=tag_word&from_code=app_blog_art)

于 2024-12-25 10:54:28 首次发布

[2048 AI社区 文章已被社区收录](javascript:; "2048 AI社区")

[生成AI
专栏收录该内容](https://blog.csdn.net/jarodyv/category_12199878.html "生成AI")

45 篇文章

该文章已生成可运行项目，

## AI 简史：从神经元到现代大模型

人工智能 (AI) 和深度学习 (DL) 在过去的几十年中飞速发展，推动了[计算机视觉](https://so.csdn.net/so/search?q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89&spm=1001.2101.3001.7020)、自然语言处理和机器人等领域的进步。今年的诺贝尔物理学奖更是颁给了美国科学家约翰·霍普菲尔德 (John Hopfield）和英国科学家杰弗里·辛顿（Geoffrey Hinton），表彰他们“在人工神经网络机器学习方面的基础性发现和发明”。本文将为大家概述 AI 的发展历程，梳理出从早期神经网络模型到现代大型语言模型发展过程中的重要里程碑。

图 1. AI 发展全景图

#### 文章目录

* + [1. 人工智能诞生 (1956)](#1__1956_10)
  + [2. AI 的演进：从基于规则的系统到深度神经网络](#2_AI__36)
  + [3. 早期人工神经网络 (1940s – 1960s)](#3__1940s__1960s_49)
  + - [3.1 McCulloch-Pitts 神经元 (1943)](#31_McCullochPitts__1943_51)
    - [3.2 Rosenblatt 感知机模型 (1957)](#32_Rosenblatt__1957_62)
    - [3.3 ADALINE (1959)](#33_ADALINE_1959_82)
    - [3.4 异或（XOR）问题 (1969)](#34_XOR_1969_106)
  + [4. 多层感知机 (1960)](#4__1960_124)
  + - [4.1 隐藏层 (Hidden Layers)](#41__Hidden_Layers_133)
    - [4.2 多层感知机的历史背景与挑战](#42__142)
  + [5. 反向传播 (1970s – 1980s)](#5__1970s__1980s_151)
  + - [5.1 早期发展 (1970 年代)](#51__1970__166)
    - [5.2 强化与普及（1980 年代）](#52_1980__171)
    - [5.3 通用逼近定理 (1989)](#53__1989_182)
    - [5.4 第二次黄金时代 (1980 年代末 – 1990 年代初)](#54__1980___1990__191)
    - [5.5 第二次黑暗时代 (1990 年代初 – 2000 年代初)](#55__1990___2000__200)
    - [深度学习的复兴 (2000 年代末 – 现在)](#_2000____215)
  + [6. 卷积神经网络 (1980s – 2010s)](#6__1980s__2010s_226)
  + - [6.1 早期发展 (1980 – 1998)](#61__1980__1998_242)
    - [6.2 CNN 的崛起：AlexNet (2012)](#62_CNN_AlexNet_2012_258)
    - [6.3 AlexNet 开启神经网络的第三次黄金时代（2010 年代至今）](#63_AlexNet_2010__280)
    - [6.4 后续架构改进](#64__289)
    - [6.5 CNN 的应用](#65_CNN__314)
  + [7. 循环神经网络 (1986 – 2017)](#7__1986__2017_324)
  + - [7.1 早期发展 (1980s – 1990s)](#71__1980s__1990s_328)
    - [7.2 LSTM, GRU 和 Seq2Seq 模型 (1997 – 2014)](#72_LSTM_GRU__Seq2Seq__1997__2014_344)
    - [7.3 RNN 的应用](#73_RNN__362)
    - [7.4 RNN 的挑战](#74_RNN__370)
  + [8. Transformer (2017 – 现在)](#8_Transformer_2017___380)
  + - [8.1 Transformer 简介](#81_Transformer__384)
    - [8.2 Transformer 的衍生模型](#82_Transformer__405)
    - [8.3 OpenAI GPT 的发展历程](#83_OpenAI_GPT__423)
    - [8.4 其他知名大语言模型](#84__439)
  + [9. 多模态模型 (2023 – 现在)](#9__2023___457)
  + - [9.1 GPT-4V (2023) 和 GPT-4o (2024)](#91_GPT4V_2023__GPT4o_2024_459)
    - [9.2 Google’s Gemini (2023 – 现在)](#92_Googles_Gemini_2023___465)
    - [9.3 Claude 3.0 和 Claude 3.5 (2023 – 现在)](#93_Claude_30__Claude_35_2023___471)
    - [9.4 LLaVA (2023)](#94_LLaVA_2023_477)
  + [10. 扩散模型 (2015 – 现在)](#10__2015___488)
  + - [10.1 扩散模型简介 (2015)](#101__2015_492)
    - [10.2 扩散模型的发展 (2020 – 现在)](#102__2020___509)
    - [10.3 文生图模型](#103__523)
    - [10.4 文生视频模型](#104__533)
  + [11. 尾声](#11__566)

### 1. 人工智能诞生 (1956)

人工智能（AI）的概念由来已久，但现代 AI 的雏形是在 20 世纪中期逐渐形成的。“人工智能”这个术语是由计算机科学家和认知科学家约翰·麦卡锡 (John McCarthy) 在 1956 年召开的达特茅斯人工智能夏季研讨项目上首次提出并被大家接受，AI 从此走上历史舞台。

图 2.
[A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence](http://www-formal.stanford.edu/jmc/history/dartmouth.pdf) (1955)

达特茅斯会议通常被视为 AI 研究的发源地。这次会议汇聚了计算机科学家、数学家和认知科学家，共同探讨创造能够模拟人类智能的机器的可能性。与会者中大佬云集，包括：

* **约翰·麦卡锡 (John McCarthy)** ：计算机科学家、Lisp 编程语言发明人之一。
* **马文·明斯基 (Marvin Minsky)**：计算机科学家、框架理论的创立者。
* **雷·索洛莫诺夫 (Ray Solomonoff)**：算法概率论创始人，通用概率分布之父，通用归纳推理理论的创建者。
* **纳撒尼尔·罗切斯特 (Nathaniel Rochester)** ：IBM 701 的首席设计师，编写了世界上第一个汇编程序。
* **克劳德·香农 (Claude Shannon)** ：数学家、发明家、密码学家，信息论创始人。
* **奥利弗·塞弗里奇 (Oliver Selfridge)**：模式识别的奠基人、人工智能的先驱，被誉为“机器知觉之父”。

图 3. 参加达特茅斯会议的部分重量级人物

### 2. AI 的演进：从基于规则的系统到深度神经网络

纵观整个 AI 的发展史，有一条清晰的发展脉络，那就是从基于规则的系统向深度神经网络的不断进化。

人工智能 (AI) 的发展始于上个世纪 50 年代，那时人们开始开发用于国际象棋和问题求解的算法。第一个 AI 程序 Logical Theorist 于 1956 年诞生。到了 1960 和 1970 年代，基于规则的专家系统如 MYCIN 被引入，它们可以帮助进行复杂的决策。1980 年代，机器学习开始兴起，使 AI 系统能够从数据中学习并不断改进，为现代深度学习技术奠定了基础。

今天，大多数最前沿的 AI 技术都由深度学习驱动，深刻改变了 AI 的发展格局。深度学习是机器学习的一个独立分支，它通过多层人工神经网络从原始数据中提取复杂特征。在本文中，我们将探讨 AI 的发展历史，并重点介绍深度学习在其中的关键作用。

图 4. 人工智能、机器学习、神经网络、深度学习之间的关系

### 3. 早期人工神经网络 (1940s – 1960s)

#### 3.1 McCulloch-Pitts 神经元 (1943)

神经网络的概念可以追溯到 1943 年，当时 Warren McCulloch 和 Walter Pitts 提出了第一个人工神经元模型。McCulloch-Pitts (MP) 神经元模型是对生物神经元的一种突破性简化。这个模型通过聚合二进制输入，并利用阈值激活函数来做出决策，从而为人工神经网络奠定了基础，输出结果为二进制 
{
0
,
1
}
\{0, 1\}
{0,1}。

图 5. 人工神经元的结构与原理

#### 3.2 Rosenblatt 感知机模型 (1957)

Frank Rosenblatt 在 1957 年引入了感知机，这是一种能够学习和识别模式的单层神经网络。感知机模型比 MP 神经元更为通用，设计用于处理实数值输入，并通过调整权重来最小化分类错误。

图 6. 感知机模型

Rosenblatt 还为感知机开发了一种监督学习算法，使得网络能够直接从训练数据中进行学习。  
 
L
(
W
)
=
−
∑
i
∈
M
W
T
X
i
y
i
\mathcal{L}(W) = - \sum\_{i \in M} W^T X\_i y\_i
L(W)=−i∈M∑​WTXi​yi​

图 7. Mark I 感知机，是一台实现了图像识别感知机算法的机器

Rosenblatt 的感知机展示出识别个人和在不同语言间翻译语音的潜力，这在当时引发了公众对 AI 的极大兴趣。感知机模型及其相关的学习算法成为神经网络发展历程中的重要里程碑。然而，很快就显现出一个关键限制：当训练数据是非线性可分时，感知机的学习规则无法收敛。

#### 3.3 ADALINE (1959)

Widrow 和 Hoff 在 1959 年引入了 ADALINE（自适应线性神经元，也称 Delta 学习规则），对感知机学习规则进行了改进。ADALINE 解决了二进制输出和噪声敏感性等限制，并能够学习并收敛非线性可分的数据，这是神经网络发展中的一大突破。

图 8. ADALINE VS. 感知机

ADALINE 的主要特点包括：

* **线性激活函数**：不同于感知器的阶跃函数，ADALINE 使用线性激活函数，因此适用于回归任务和连续输出。
* **最小均方（LMS）算法**：ADALINE 采用 LMS 算法，该算法通过最小化预测输出与实际输出之间的均方误差，提供更高效和稳定的学习过程。
* **自适应权重**：LMS 算法根据输出误差自适应调整权重，使 ADALINE 即使在有噪声的情况下也能有效地学习和收敛。

**ADALINE 的引入标志着神经网络第一次黄金时代的开始**，它克服了 Rosenblatt 感知机学习的限制。这一突破实现了高效学习、连续输出和对噪声数据的适应能力，推动了该领域的创新和快速发展。

图 9. ADALINE 开启了神经网络第一次黄金时代

然而，与感知机类似，ADALINE 仍然无法解决线性可分的问题，无法应对更复杂的非线性任务。这一局限集中体现在异或（XOR）问题上，也促进了更高级神经网络架构的发展。

#### 3.4 异或（XOR）问题 (1969)

1969年，Marvin Minsky 和 Seymour Papert 在他们的著作《Perceptrons》中揭示了单层感知机的一个重要局限：由于其线性决策边界，感知机无法解决异或 (XOR) 问题，而这是一个简单的二元分类任务。异或问题不是线性可分的，也就是说，没有一个单一的线性边界能够正确地将所有的输入模式分类。

图 10. Marvin Minsky 和 Seymour Papert 合著的《Perceptrons: An introduction to computational geometry》

这一发现强调了需要开发更复杂的神经网络架构，以便能够学习非线性的决策边界。感知机的局限性被揭露后，人们对神经网络的信心减弱，转而研究符号人工智能方法，**这标志着从 20 世纪 70 年代初到 80 年代中期的“神经网络的第一次黑暗时代”的开始**。

图 11. 异或问题将神经网络代入第一次黑暗时代

### 4. 多层感知机 (1960)

多层感知机 (MLP) 最早于 20 世纪 60 年代提出，作为对单层感知机的改进。MLP 由多个层次的相互连接的神经元组成，能够克服单层模型的局限性。苏联科学家 A. G. Ivakhnenko 和 V. Lapa 在感知机基础上进行研究，对多层感知机的发展中做出了重要贡献。

图 12. 多层感知机模型

#### 4.1 隐藏层 (Hidden Layers)

增加隐藏层使得 MLP (多层感知器) 可以捕捉和表达数据中的复杂非线性关系。这些隐藏层极大地增强了网络的学习能力，使其能够解决诸如异或问题这样非线性可分的问题。

图 13. 隐藏层解决异或问题

#### 4.2 多层感知机的历史背景与挑战

MLP 的出现标志着神经网络的研究向前迈出了重大一步，展示了[深度学习架构](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9E%B6%E6%9E%84&spm=1001.2101.3001.7020)在解决复杂问题方面的潜力。然而，在 1960 年代和 1970 年代，MLP 的发展面临若干挑战：

* **缺乏训练算法**：早期的 MLP 模型缺乏高效的训练算法，无法有效地调整网络权重。此时反向传播算法还未诞生，训练多层深度网络非常困难。
* **算力限制**：当时的算力不足以应对训练深度神经网络所需的复杂计算。这一限制拖慢了 MLP 的研究和发展进程。

神经网络的第一个黑暗时代在 1986 年结束，**随着反向传播算法的诞生，开启了神经网络的第二个黄金时代**。

### 5. 反向传播 (1970s – 1980s)

1969 年，异或问题揭示了感知机（单层神经网络）的局限性。研究人员意识到，多层神经网络能够克服这些限制，但缺乏有效的训练算法。17年后，反向传播算法的开发使得神经网络在理论上可以逼近任何函数。值得注意的是，该算法实际上在发表之前就已被发明。如今，反向传播已成为深度学习的核心组件，自 20 世纪 60 年代和70 年代以来经历了显著的发展和完善。

图 14. 反向传播原理示意图

反向传播的关键特性：

* **梯度下降**：反向传播与梯度下降联合使用以降低误差函数。该算法计算每个权重相对于误差的梯度，从而逐步调整权重以减少误差。
* **链式法则**：反向传播算法的核心在于应用微积分的链式法则。此法则使得误差的梯度可以被分解为一系列偏导数，并通过网络的反向传递高效计算。
* **分层计算**：反向传播逐层运作，从输出层向输入层反向传递。这种分层计算确保梯度在网络中正确传播，使得深度架构的训练成为可能。

#### 5.1 早期发展 (1970 年代)

* **Seppo Linnainmaa (1970)**: 提出了自动微分的概念，这是反向传播算法的重要组成部分。
* **Paul Werbos (1974)**: 提议使用微积分的链式法则计算误差函数对网络权重的梯度，从而能够训练多层神经网络。

#### 5.2 强化与普及（1980 年代）

* **David Rumelhart, Geoffrey Hinton 和 Ronald Williams (1986)**: 将**反向传播**这一高效实用的方法，用于训练深度神经网络，并展示了其在多种问题中的应用。

图 15. 反向传播算法的三位主要贡献者

其中 Geoffrey Hinton 因其在人工神经网络和机器学习领域的贡献获得了 2018 年图灵奖和 2024 诺贝尔物理学奖，称为继 Herbert Simon 后第二位图灵奖-诺贝尔奖双料得主。

#### 5.3 通用逼近定理 (1989)

George Cybenko 在 1989 年提出的通用逼近定理，为多层神经网络的功能提供了数学基础。该定理表明，只要神经元数量足够，并且使用非线性激活函数，具有单个隐藏层的前馈神经网络就能够以任意精度逼近任意连续函数。这个定理突显了神经网络的强大能力和灵活性，使其能够应用于各种领域。

图 16. 具有单个隐藏层的神经网络可以将任意连续函数逼近到任意所需的精度，从而在各个领域解决复杂的问题

#### 5.4 第二次黄金时代 (1980 年代末 – 1990 年代初)

\*\*反向传播算法的出现和通用逼近定理的提出，开启了神经网络研究的第二个黄金时代。\*\*反向传播提供了一种高效的多层神经网络训练方法，使研究人员能够构建更深层次和更复杂的模型。通用逼近定理则为使用多层神经网络提供了理论支持，并增强了人们对其解决复杂问题能力的信心。在 1980 年代末至 1990 年代初，这一时期见证了对神经网络领域的兴趣回升和显著的进步。

图 17. 反向传播和通用逼近定理开启了神经网络研究的第二个黄金时代

#### 5.5 第二次黑暗时代 (1990 年代初 – 2000 年代初)

然而，由于一系列因素，神经网络领域在 1990 年代初至 2000 年代初经历了“第二个黑暗时代”：

* **支持向量机 (SVM) 的兴起**：支持向量机为分类和回归任务提供了更优雅的数学方法。
* **算力限制**：由于训练深度神经网络仍然耗时且对硬件要求高，计算能力受到限制。
* **过拟合和泛化问题**：这两个问题导致早期神经网络在训练数据上表现良好，但在新数据上表现不佳，限制了其实用性。

这些挑战使得许多研究人员转而关注其他领域，导致神经网络研究的停滞。

图 18. 随着 SVM 的兴起，神经网络进入第二个黑暗时代

#### 深度学习的复兴 (2000 年代末 – 现在)

在 2000 年代末和 2010 年代初，神经网络领域经历了复兴，这得益于以下方面的进步：

* **深度学习架构的发展**（如 CNNs、RNNs、Transformers、Diffusion Models）
* **硬件的改进**（如 GPUs、TPUs、LPUs）
* **大规模数据集的可用性**（如 ImageNet、COCO、OpenWebText、WikiText 等）
* **训练算法的优化**（如 SGD、Adam、dropout）

这些进展带来了计算机视觉、自然语言处理、语音识别和强化学习的重大突破。通用逼近定理与实际技术的进步相结合，为深度学习技术的广泛应用和成功奠定了基础。

### 6. 卷积神经网络 (1980s – 2010s)

卷积神经网络 (CNN) 在深度学习领域，尤其是计算机视觉和图像处理方面，带来了革命性的变化。从上个世纪 80 年代到本世纪最初的 10 年，CNN 在架构、训练技术和应用等方面取得了显著的进步。

卷积神经网络由以下三个主要组件构成：

* **卷积层 (Convolutional Layers)**：这些层通过一组可调整的滤波器，从输入图像中自动学习和提取特征的空间层次结构。
* **池化层 (Pooling Layers)**：池化层通过缩小输入的空间尺寸，来提高对输入变化的适应性，并减少计算量。
* **全连接层 (Fully Connected Layers)**：在卷积层和池化层之后，全连接层用于分类任务，负责整合之前层中提取的特征。

卷积神经网络的主要特性

* **局部感受野**：CNN 利用局部感受野来捕捉输入数据中的局部特征，使其在处理图像和其他视觉任务时表现出色。
* **权重共享**：通过在卷积层中共享权重，CNN 能够减少网络中参数的数量，从而提高训练效率。
* **平移不变性**：池化层赋予网络平移不变性，使其能够识别输入图像中不同位置的相同模式。

#### 6.1 早期发展 (1980 – 1998)

1980 年代，福岛邦彦 (Kunihiko Fukushima) 首次提出了 CNN 的概念，他设计了一种称为神经认知机 (Neocognitron) 的分层神经网络，这种网络模仿了人类视觉皮层的结构。这项开创性的研究为之后 CNN 的发展奠定了基础。

图 19. 福岛邦彦与他的神经认知机

到了 1980 年代末和 1990 年代初，Yann LeCun 和他的团队在此基础上进一步发展了 CNN，并推出了 LeNet-5 架构，该架构专为手写数字识别而设计。

图 20. Yann LeCun 与他的 LeNet-5

#### 6.2 CNN 的崛起：AlexNet (2012)

2012 年，AlexNet 在 ImageNet 大规模视觉识别挑战赛（ILSVRC）中取得了重大胜利，这是 CNN 发展中的一个重要里程碑。这次胜利不仅以压倒性优势赢得了比赛，也在图像分类领域取得了重大突破。

图 21. ILSVRC 历年冠军及其表现

ILSVRC 是一个年度图像识别基准测试，用于评估算法在一个包含 1000 万多张注释图像的数据集上的表现，这些图像被划分为 1000 个类别。AlexNet 的创新之处包括：

* **ReLU 激活函数**：为解决传统激活函数的问题而引入，ReLU 提高了训练速度并改善了性能。
* **Dropout 正则化**：这种技术通过在训练过程中随机丢弃神经元来减少过拟合现象。
* **数据增强**：通过人为增加训练数据的多样性，增强了数据集的丰富性，从而改善了模型的泛化能力。

AlexNet 的成功成为 CNN 发展中的一个转折点，为图像分类和物体检测的进一步发展奠定了基础。

图 22. AlexNet 架构

#### 6.3 AlexNet 开启神经网络的第三次黄金时代（2010 年代至今）

自 2010 年代直至今天，当前的科技发展黄金时代以深度学习、大数据和强大计算平台的结合为特征。在这一时期，图像识别、自然语言处理和机器人技术等领域取得了显著的突破。持续的研究不断推动着人工智能（AI）能力的边界。

图 23. AlexNet 开启神经网络的第三次黄金时代

#### 6.4 后续架构改进

继 AlexNet 之后，又相继出现了几个有影响力的架构：

* **VGGNet (2014)**：由牛津大学的视觉几何组开发，VGGNet 强调使用更深的网络架构，并采用较小的卷积滤波器 (
  3
  ×
  3
  3 \times 3
  3×3)，从而取得了显著的准确率。

  图 24. 原始 VGGNet 架构
* **GoogLeNet/Inception (2014)**：引入了 inception 模块，使得网络能够以更高效的方式捕捉不同尺度的特征。

  图 25. GooLeNet 架构
* **ResNet (2015)**：残差网络通过引入跳跃连接，使得训练非常深的网络成为可能，同时缓解了梯度消失问题。

  图 26. ResNet 架构

#### 6.5 CNN 的应用

CNN 的进步已经在多个领域引发了变革：

* **计算机视觉**：CNN 已成为现代计算机视觉的核心，实现了图像分类、物体检测和语义分割方面的突破。
* **医学影像**：CNN 被用于疾病诊断、肿瘤检测和图像引导手术等任务，大大提高了诊断准确性。
* **无人驾驶**：CNN 是无人驾驶感知系统的核心，使它们能够解释和响应周围环境。

CNN 从其创立到目前作为深度学习基石的历程展示了其对 AI 的重大影响。CNN 的成功也为深度学习的进一步进步铺平了道路，并激发了其他专用神经网络架构的发展，如 RNN 和 Transformer。CNN 的理论基础和实际创新显著推动了深度学习技术在各个领域的广泛应用和成功。

### 7. 循环神经网络 (1986 – 2017)

循环神经网络 (RNN) 是为了处理序列数据而设计的。与传统的前馈网络（MLP）不同，RNN 拥有一个内部的隐藏状态或“记忆”，使其能够捕捉序列元素之间的时间依赖性。因此，RNN 在语言建模、时间序列预测和语音识别等任务中尤为有效。

#### 7.1 早期发展 (1980s – 1990s)

RNN 的概念起源于 1980 年代，John Hopfield, Michael I. Jordan 和 Jeffrey L. Elman 等先驱为这些网络的发展做出了贡献。John Hopfield在 1982 年提出的 Hopfield 网络为理解神经网络中的循环连接奠定了基础。Jordan 网络和 Elman 网络分别在 1980 年代和 1990 年代提出，是早期捕捉序列数据中时间依赖性的尝试。

图 27. RNN 架构

RNN 使用历时反向传播 (BPTT) 进行训练，这是前馈网络标准反向传播算法的扩展。BPTT 需要将网络在时间上展开，将每个时间步视为一层。在前向传播时，输入序列被处理，并在输出层计算误差。然后，产生的梯度从最后一个时间步反向传播到第一个时间步，以更新 RNN 的参数。然而，由于梯度消失问题，RNN 在学习长时间依赖性时遇到困难，因为梯度会变得极小，导致无法学习。相反，梯度也可能变得过大，造成训练不稳定，这被称为梯度爆炸问题。

图 28. 反向传播 (BPTT)

#### 7.2 LSTM, GRU 和 Seq2Seq 模型 (1997 – 2014)

图 29. RNN, LSTM, GRU 单元

* **长短期记忆 (LSTM) 网络 (1997)**：Sepp Hochreiter 和 Jürgen Schmidhuber 提出了 LSTM 网络，以解决传统 RNN 中的梯度消失问题。LSTM 通过使用门控机制来控制信息流动，使其能够捕捉序列数据中的长期依赖关系。LSTM 包括单元状态（用于存储长期信息）、隐藏状态（携带当前时间步的短期输出），以及三个门（输入门、遗忘门和输出门）。在每一步中，LSTM 会根据多个数学运算和门来决定需要遗忘多少信息，向单元状态添加多少信息，以及为下一步输出多少信息。
* **门控循环单元 (GRU) (2014)**：Kyunghyun Cho 等人提出了 GRU，这是一种简化版的 LSTM，也采用门控机制来调节信息流。与 LSTM 的三个门和两个状态不同，GRUs 只有两个门和一个状态。LSTM 的遗忘门和输入门被合并为一个更新门，用于决定保留多少过去的信息和整合多少新信息。此外，GRU 用重置门代替了 LSTM 的输出门，该门决定在整合新信息之前需要“重置”或忘记多少过去的信息。由于 GRU 的参数较少，通常训练速度更快。
* **Seq2Seq 模型 (2014)**：Ilya Sutskever 和他的团队提出了 Seq2Seq 模型，这种模型使用编码器-解码器架构，将输入序列转换为输出序列。Seq2Seq 模型已被广泛应用于机器翻译、语音识别和文本摘要等任务。

  图 30. 基于 LSTM 的 Seq2Seq 编码器-解码器架构

#### 7.3 RNN 的应用

RNN 在多个领域产生了重大影响，包括：

* **自然语言处理**：RNN 在自然语言处理领域引发了革命性变化，使得语言建模、机器翻译、情感分析和文本生成等任务取得了显著进展。
* **语音识别**：RNN 广泛用于语音识别系统中，它们通过建模口语的时间依赖性，将语音信号转换为文本。
* **时间序列预测**：RNN 在时间序列预测中表现出色，它们通过建模顺序数据的时间依赖性，以预测未来值。

#### 7.4 RNN 的挑战

尽管 RNN 在许多方面取得了成功，但其仍面临若干挑战：

* **梯度消失与梯度爆炸**：传统 RNN 在处理这些问题时表现不佳，尽管 LSTM 和 GRU 提供了一些解决方案。
* **计算复杂性**：训练 RNN 可能需要大量资源，尤其是在处理大型数据集时。
* **并行化**：RNN 的顺序特性使得并行训练和推理过程变得复杂。

RNN 的成功为深度学习的进一步发展奠定了基础，并启发了其他专门化神经网络架构的发展，例如 Transformer，它们在各种序列数据任务中取得了最先进的性能。RNN 的理论基础和实际创新大大推动了深度学习技术在各个领域的广泛应用和成功。

### 8. Transformer (2017 – 现在)

Transformer 以其卓越的序列数据处理能力，深刻地改变了深度学习的格局，并在自然语言处理 (NLP) 和计算机视觉等多个领域中发挥了重要作用。

#### 8.1 Transformer 简介

Vaswani 等人于 2017 年发表了开创性论文“Attention is All You Need”，其中提出了 Transformer 模型。这个模型放弃了 RNN 的传统序列处理方式，转而采用自注意力机制，从而实现了并行处理，并能更好地处理长距离依赖关系。

图 31. 自注意力机制

Transformer 具有如下核心特性：

* **自注意力机制**：允许序列中每个位置灵活地关注其他所有位置，从而比 RNN 或 LSTM 更有效地捕捉上下文。
* **并行化**：通过同时处理所有输入数据，大大提高了训练速度，这与 RNN 的顺序处理方式形成鲜明对比。
* **编码器-解码器结构**：编码器和解码器堆栈都使用自注意力和前馈神经网络层，并通过位置编码来保持序列的顺序。

图 32. Transformer 架构

关于 Transformer 和自注意力机制的详细介绍，请参考 [《深度解析 Transformer 和注意力机制（含完整代码实现）》](https://jarod.blog.csdn.net/article/details/130867562) 和 [《图解 NLP 模型发展：从 RNN 到 Transformer》](https://jarod.blog.csdn.net/article/details/129564388)。

#### 8.2 Transformer 的衍生模型

图 33. 基于 Transformer 的模型

Transformer 有众多衍生模型，其中比较重要的有：

* **BERT (2018)**: BERT 是一种仅使用编码器的双向编码器表示模型，通过掩码语言建模和下一句预测的预训练，彻底革新了 NLP。
* **GPT (2018)**: GPT 旨在预测序列中的下一个 Token（词），展示了在理解和生成类人文本方面的强大能力。这一基础模型为生成式语言模型的后续发展奠定了基础，展示了从大型文本语料库中进行无监督学习的潜力。
* **T5 (2019)**: T5 是一种编码器-解码器结构的文本到文本转换模型，将 NLP 任务转化为统一的文本到文本格式，简化了模型架构和训练过程。

图 34. BERT vs. GTP vs. T5

#### 8.3 OpenAI GPT 的发展历程

OpenAI 的生成式预训练 Transformer (Generative Pre-trained Transformer, GPT) 系列模型自 2018 年问世以来，极大地推动了自然语言处理 (Natural Language Processing, NLP) 领域的发展。每一代模型都在前一代的基础上进行改进，引入了更大规模的模型和增强的功能。以下是每个版本的详细概述。

图 35. GPT 的自回归语言模型架构旨在根据之前输入的 Token 预测序列中的下一个 Token

* **GPT (2018)**: 原始的 GPT 模型于 2018 年推出，作为一个仅使用自回归解码器的变换器，拥有 1.17 亿个参数。它被设计用于预测序列中的下一个 Token（词），展示了在理解和生成类人文本方面的强大能力。这个基础模型为后续生成式语言模型的发展奠定了基础，展示了无监督学习从大型文本语料库中获取信息的潜力。
* **GPT-2 (2019)**: 2019 年发布的 GPT-2 在模型规模和能力上实现了显著飞跃，参数数量扩大到 15 亿个。这个版本表现出一些新兴能力，如零样本任务执行，即可以在没有专门训练的情况下执行任务。然而，它生成连贯但有时误导性文本的能力引发了关于潜在滥用的道德担忧，特别是在生成假新闻或错误信息方面。
* **GPT-3 (2020)**: GPT-3 于 2020 年推出，进一步将模型规模扩大到惊人的 1750 亿个参数。该模型在少样本学习方面表现出卓越的能力，即可以根据提示中提供的极少量示例适应各种任务。其生成类人文本的能力使其成为许多应用的多功能工具，包括内容创作、代码辅助和对话代理。GPT-3 的架构使其能够在无需大量微调的情况下执行广泛的 NLP 任务，巩固了其作为当时最强大语言模型之一的地位。
* **ChatGPT (2022):** 这是一个经过微调的 GPT-3.5 模型，通过人类反馈强化学习 (Reinforcement Learning from Human Feedback, RLHF) 进行优化，擅长处理后续问题和维护上下文，通过指令调优和用户偏好数据使响应更符合用户意图。
* **GPT-4 (2023)**: GPT-4 于 2023 年发布，继续在能力和参数数量上进行扩展，尽管其架构和参数数量的具体细节目前尚未完全公开。预计将在之前几代模型的表现上进一步提升，特别是在推理能力和理解复杂上下文的能力方面。
* **GPT-o1 (2024)**：这一版本的 GPT 与之前所有版本有了本质区别，它开创性地引入了人类的慢思考+思维链模式，将大模型从越来越离谱的参数内卷中解救出来，开辟了AI发展的新方向。GPT-o1 显著提升了逻辑推理能力，使其在数学、科研、代码等领域的表现有了质的飞跃。在若干基准测试中，GPT-o1 展现出的能力已经与博士生相当。

#### 8.4 其他知名大语言模型

随着越来越多优秀的大型语言模型（LLM）的涌现，人工智能领域得到了极大的丰富。这些模型各具特色，为人工智能技术带来了新的进展。以下是一些知名大语言模型的概况：

* **Anthropic 的 Claude (2022)**: 该模型注重 AI 输出的安全性和伦理问题，致力于与人类价值观保持一致。
* **Meta 的 LLaMA (2023)**: 提供多种规模的模型，以满足不同的计算需求，在自然语言处理的基准测试中表现卓越。
* **Mistral.AI 的 Mistral (2023)**: 兼顾高性能和资源效率，适合于实时应用，专注于开源 AI 解决方案。
* **阿里巴巴的 Qwen (2023)**: 专为创建高质量的英中双语 AI 模型而设计，促进跨语言应用并推动创新。
* **Microsoft 的 Phi (2023)**: 强调在各种应用中的多功能性和集成能力，采用先进的训练技术以提升上下文理解和用户交互。
* **Google 的 Gemma 系列 (2024)**: 这些轻量级的开放模型应用于多种领域，包括文本生成、摘要和信息提取，注重性能和效率。

更多大语言模型及其能力评估参加下图

图 36. 开源模型和闭源模型的性能

### 9. 多模态模型 (2023 – 现在)

#### 9.1 GPT-4V (2023) 和 GPT-4o (2024)

* **GPT-4V (2023)** 是 AI 发展中的重要一步，它将多模态功能集成到已经强大的文本模型中。它不仅能够处理和生成文本，还可以处理和生成图像内容，为更全面的 AI 交互奠定了基础。
* **GPT-4o (2024)** 是从 GPT-4V 演变而来的，通过复杂的上下文理解来增强多模态集成。与其前身相比，它在不同媒体之间提供了更好的连贯性，能够从文本提示生成更高级的图像，并基于视觉输入进行更精细的推理。此外，GPT-4o 通过高级训练机制实现伦理对齐，确保其输出不仅准确，而且负责任，并与人类价值观保持一致。

#### 9.2 Google’s Gemini (2023 – 现在)

* **Gemini Pro (2023)**: Google 的 Gemini 推出了一系列为多模态任务设计的模型，集成了文本、图像、音频和视频处理。特别是，Gemini Pro 因其可扩展性和效率而脱颖而出，使高级 AI 能够应用于从实时分析到跨不同媒体格式的复杂内容生成等多个领域。
* **Gemini Ultra 和 Nano (2023)**: Gemini 模型包括适用于不同规模应用的 Ultra 和 Nano 版本，能够执行需要跨多种数据类型理解的任务。它们在视频摘要、多模态翻译和互动学习环境等任务中表现出色，体现了 Google 在推动 AI 在多媒体环境中应用的决心。

#### 9.3 Claude 3.0 和 Claude 3.5 (2023 – 现在)

* **Claude 3.0 (2023)** 由 Anthropic 推出，该模型专注于提高 AI 响应的安全性和可靠性，在上下文理解和伦理考虑方面进行了改进。它被设计得更具对话性和辅助性，同时严格遵循避免有害或偏见输出的原则。
* **Claude 3.5 (2024)** 进一步提升了 Claude 3.0 的能力，在复杂任务中的表现更佳，处理效率更高，并且在用户请求的细节处理上更加细致。这个版本还强调多模态交互，虽然它主要在文本和逻辑任务中表现突出，但在处理视觉或其他感官输入方面也展现出新兴能力，提供更为综合的用户体验。

#### 9.4 LLaVA (2023)

* **LLaVA (Large Language and Vision Assistant)** 是一种创新的多模态 AI (Multimodal AI) 方法，将语言理解与视觉处理结合在一起。LLaVA 于 2023 年开发，能够解读图像并将其与文本内容相联系，使其可以回答关于图像的问题、描述视觉内容，甚至根据视觉线索生成文本。其架构充分利用 Transformer 模型的优势，在需要同时具备视觉和语言理解的任务中实现了最先进的性能。这个模型因其开源特性而备受关注，鼓励在多模态 AI 应用领域进行更多的研究和开发。

  图 37. LLaVA 架构

  这些模型的出现标志着 AI 系统的转变，这些系统不仅能够理解和生成文本，还能解释和创造跨多种模态的内容，更加贴近人类的认知能力。这种 AI 模型的发展推动了更具互动性和直观性的应用程序，它们能够结合不同的感官输入来处理现实世界中的场景，从而拓宽了 AI 在日常生活、研究和工业应用中的可能性。

### 10. 扩散模型 (2015 – 现在)

扩散模型已经成为生成模型中一个重要的类别，它为从复杂数据分布中生成高保真样本提供了一种全新的方法。与传统模型如 GAN 和 VAE 不同，扩散模型采用渐进去噪技术，并在许多应用中表现出色。

#### 10.1 扩散模型简介 (2015)

扩散模型的基础由 Sohl-Dickstein 等人于 2015 年在他们的论文中奠定。他们提出了一种生成过程，即通过逆转逐步添加的噪声，可以将噪声还原为结构化数据。

图 38. 扩散模型原理概要

扩散模型的关键特性：

* **去噪过程**: 这些模型通过逐步添加噪声（前向过程），并学习如何逆转该过程（反向过程），以有效去噪并生成样本。
* **马尔可夫链**: 这两个过程都被构建为马尔可夫链，每个前向步骤添加高斯噪声，模型学习如何在反向过程中去除这些噪声。
* **训练目标**: 目标是在每一步中最小化预测噪声与实际噪声之间的差异，优化一种证据下界（ELBO）的形式。
* **稳定性和鲁棒性**: 它们比 GAN 提供更好的稳定性，避免了模式崩溃等问题，从而能够持续生成多样化的高质量输出。

关于扩散模型的详细介绍，请参考[《Diffusion Model 深入剖析》](https://jarod.blog.csdn.net/article/details/130903760)。

#### 10.2 扩散模型的发展 (2020 – 现在)

* **去噪扩散概率模型 (Denoising Diffusion Probabilistic Models, DDPM) (2020)**: 改进了扩散过程，在图像合成领域设立了新的标杆。
* **去噪扩散隐式模型 (Denoising Diffusion Implicit Models, DDIM) (2021)**: 通过非马尔可夫采样提高了效率，使生成过程更加灵活。
* **基于分数的生成模型 (2021)**: 通过使用随机微分方程提高了样本生成的效率。
* **潜在扩散模型 (Latent Diffusion Model) (2022)**: 成为流行的文本到图像生成系统（如 Stable Diffusion）的基础，显著推动了 AI 生成图像领域的进步，并为更易于访问和高效的生成式 AI 工具铺平了道路。关于潜在扩散模型和 Stable Diffusion 的详细介绍，请参见 [《Stable Diffusion 超详细讲解》](https://jarod.blog.csdn.net/article/details/131018599) 和 [《Stable Diffusion原理详解》](https://jarod.blog.csdn.net/article/details/129280836)。

  图 39. 潜在扩散模型架构

#### 10.3 文生图模型

* **DreamBooth (2022)**: 允许在少量特定主题的图像上训练扩散模型，从而实现个性化的图像生成。
* **LoRA (2022)**: 代表低秩适应，是一种通过添加少量参数来微调扩散模型的技术，使其更容易适应特定任务或数据集。
* **ControlNet (2023)**: 通过添加如草图或深度图等输入来控制扩散模型，从而对生成图像提供更多的控制。
* **FLUX.1 (2024)**: Black Forest Lab 推出了 FLUX.1，这是一种用于 AI 图像生成的先进扩散模型，具备卓越的速度、质量和响应提示的能力。FLUX.1 提供三个版本——Schnell、Dev 和 Pro，并采用了整流流变换器等创新技术，能够生成高度逼真的图像。FLUX.1 还可以生成文字并精准处理手指和脚趾等细节，是一个全面的图像生成器。
* **Multi-SBoRA (2024)**: Multi-SBoRA 是一种为多个概念定制扩散模型的新方法。它使用正交标准基向量来构建低秩矩阵进行微调，允许区域性和非重叠的权重更新，从而减少跨概念的干扰。这种方法保留了预训练模型的知识，减少了计算开销，并提高了模型的灵活性。实验结果显示，Multi-SBoRA 在多概念定制中表现优异，保持了独立性，并减轻了串扰效应。

#### 10.4 文生视频模型

2024 年 2 月，OpenAI 发布了 [Sora](https://openai.com/sora) 文生视频模型。凭借惊艳的视频生成质量，Sora 一经发布就受到各行各业的追捧和关注。尽管在 Sora 之前已经有好几个文生视频模型，但 Sora 的发布被普遍认为拉开了文生视频的大幕。

Sora vs. Pika vs. RunwayML vs. Stable Video 生成视频效果对比

很明显可以看出 Sora 无论从分辨率、时长、精细度和对真实世界的还原程度上都远远好于其他模型。下表给出了详细的对比。

图 40. Sora vs. 早期文生视频模型

然而，Sora 发布后迟迟没有正式上线。全网苦等10个月，Sora 终于在 2024 年 12 月 10 日正式上线。在这 10 个月期间，国产文生视频模型迅速崛起，其中 MiniMax 的海螺和快手的可灵的视频生成质量比肩甚至超越 Sora。

| 名称 | 公司 | 单次生成秒数 | 是否免费 | 生成方式 | 最低月付费 |
| --- | --- | --- | --- | --- | --- |
| 可灵 | 快手 | 5s | 限制免费使用次数 | 文生视频、图生视频 | 66元 |
| 即梦 | 字节 | 5s | 限制免费使用次数 | 文生视频、图生视频 | 69元 |
| 海螺 | MiniMax | 6s | 限制免费使用次数 | 文生视频、图生视频 | 68元 |
| Vidu | 生数科技 | 4s | 限制免费使用次数 | 文生视频、图生视频 | 9.9美元 |
| 智谱清言 | 智谱科技 | 6s | 没有限制 | 文生视频、图生视频 | 免费 |
| 通义万相 | 阿里 | 6s | 没有限制 | 文生视频、图生视频 | 免费 |
| FilmAction | 瀚皓科技 | 5s | 限制免费使用次数 | 图生视频 | 50元300电影币 |
| 白日梦 | 光魔科技 | 最长6分钟 | 限制免费使用次数 | 图生视频 | 29元 |
| Sora | OpenAl | 5s、20s | 付费使用 | 文生视频、图生视频 | 20美元 |
| Runway | Runway | 10s | 限制免费使用次数 | 文生视频、图生视频 | 15美元 |

表 1. 主流文生视频模型一览表

相信 2025 年文生视频将会是各大 AI 企业主要争夺的战场。

### 11. 尾声

至此，我们的 AI 简史之旅就要接近尾声了。通过对 AI 发展的回顾，我们可以发现人工智能 (AI) 和深度学习的发展历史充满了突破性的进步和变革性的创新。从早期的简单神经网络到复杂的网络架构，从卷积神经网络 (CNN)、递归神经网络 (RNN) 到现在流行的 Transformer 和扩散模型，这些技术已经彻底改变了许多领域。

最近的技术进步催生了大型语言模型和多模态模型，例如 OpenAI 的 GPT-4o、Google 的 Gemini Pro、Antropic 的 Claude 3.5 Sonnet 和 Meta 的 LLaMA3.1 等，它们在自然语言处理和多模态能力方面表现出色。此外，生成式 AI (Generative AI) 的突破，包括文本到图像和文本到视频生成模型如 Midjourney、DALL-E 3、Stable Diffusion、FLUX.1 和 Sora，极大地拓展了 AI 的创造潜力。

随着研究继续致力于开发更高效、可解释和功能强大的模型，AI 和深度学习对社会和技术的影响将不断加深。这些技术进步不仅推动了传统行业的创新，还为创造性表达、问题解决和人机协作开辟了新可能。

然而，深度学习并不是实现 AI 的唯一途径或最佳途径。符号 AI、强化学习和神经符号 AI 各自具有独特优势，并能弥补深度学习在可解释性和计算资源需求方面的不足。对 AI 的全面理解应涵盖这些多样化的方法。

AI 的未来在于多种方法的协同效应。随着研究的深入，构建多元化的 AI 技术生态系统将确保其平衡和有效的发展，从而造福社会和科技领域。

本文章已经生成可运行项目

确定要放弃本次机会？

福利倒计时

*:*
*:*

立减 ¥

普通VIP年卡可用

[立即使用](https://mall.csdn.net/vip)

[JarodYv](https://jarod.blog.csdn.net)

[关注](javascript:;)
关注

* 49

  点赞
* 踩
* [89](javascript:;)

  收藏

  觉得还不错?
  一键收藏
* [2](#commentBox)

  评论
* [分享](javascript:;)

  复制链接

  分享到 QQ

  分享到新浪微博

  扫一扫
* [打赏](javascript:;)

  打赏
* 打赏
  举报

  举报

专栏目录

[*人工智能*（*AI*）*简史*：推动新时代的科技力量](https://blog.csdn.net/qq_17153885/article/details/144838030)

[qq\_17153885的博客](https://blog.csdn.net/qq_17153885)

12-31

1万+

[*人工智能*（*AI*，Artificial Intelligence）是计算机科学的一个分支，旨在研究和开发可以模拟、扩展或增强人类智能的系统。它涉及多种技术和方法，包括*机器学习*、*深度学习*、自然语言处理（NLP）、计算机视觉、专家系统等。](https://blog.csdn.net/qq_17153885/article/details/144838030)

[最全*AI**简史*（下）：后*深度学习*时代（*大模型*时代）](https://blog.csdn.net/jpw41/article/details/141403498)

[jpw41的博客](https://blog.csdn.net/jpw41)

08-21

2914

[💡 铺垫这么多终于到*大模型*章节了，前面两篇文章分别就*人工智能*和*深度学习*的发展历史进行了介绍，大致可以理解为：20世纪的*人工智能*发展百花齐放、坎坷中前进，进入21世纪后*深度学习*很快成为*人工智能*中的显学，2020年后则以大语言模型为代表范式。这当然不是说一些逻辑规则的、概率统计*机器学习*的甚至是非*Transformer*的*深度学习*结构已经逐渐推出历史舞台，相反大家各自在自己的领域依然是SOTA，也与*大模型*有许多交汇的地方。](https://blog.csdn.net/jpw41/article/details/141403498)

2 条评论
您还未登录，请先
登录
后发表或查看评论

[*AI*进化史*:*从图灵测试到ChatGPT](https://blog.csdn.net/qq_38145499/article/details/155749946)

1-30

[*AI**发展史*是一部人类探索智能本质、拓展认知边界的壮丽史诗,而我们正身处其中最具变革性的章节。DeepSeek等新一代*AI*力量的加入,正在书写着这段历史的新篇章。](https://blog.csdn.net/qq_38145499/article/details/155749946)

[*人工智能*(Artificial Intelligence, *AI*)\_*ai**发展史* csdn](https://blog.csdn.net/m0_68935893/article/details/150590727)

1-20

[机器翻译领域出现“ALPAC报告”*:*1966年,美国政府因机器翻译进展缓慢(如“the spirit is willing but the flesh is weak”被译为“酒是好的,但肉已变质”),停止资助相关研究,引发第一次*AI*寒冬。 1970s*:*符号主义局限性显现(如无法处理模糊、非结构化问题),加上计算能力不足,科研 funding 锐减,进入第一次*AI*寒冬。](https://blog.csdn.net/m0_68935893/article/details/150590727)

[万字长文详解 *AI* *大模型**发展史*：从萌芽到爆发，史上最全整理](https://devpress.csdn.net/v1/article/detail/149507169)

[CSDN\_430422的博客](https://blog.csdn.net/CSDN_430422)

07-21

4621

[万字长文详解 *AI* *大模型**发展史*：从萌芽到爆发，史上最全整理](https://devpress.csdn.net/v1/article/detail/149507169)

[收藏级！史上最通俗的*AI*发展历程综述（附*大模型*学习指南）

最新发布](https://devpress.csdn.net/v1/article/detail/156328628)

[大模型教程的博客](https://blog.csdn.net/Z987421)

12-27

556

[规则式*AI*的死板问题，催生了"让机器自主学习规律"的需求——*机器学习*（ML）技术应运而生，标志着*AI*从"规则驱动"迈入"数据驱动"时代。机器从数据中总结出的规律，最终会形成一个"可复用的计算模型"——这就是*AI*模型（Model）。对程序员而言，可理解为"一个经过数据训练的函数，输入新数据就能输出判断结果"。*AI*模型三大核心要素： - 输入：新的待处理数据（如收到的新邮件）；- 处理：用学到的规律对数据进行分析；- 输出：明确的结果（如"垃圾邮件"或"正常邮件"）。](https://devpress.csdn.net/v1/article/detail/156328628)

[*人工智能*发展*简史*, 没想到17世纪*AI*就出现了!\_17世纪中叶*人工智能*-CSDN...](https://blog.csdn.net/gravitylink/article/details/86089639)

12-17

[本文回顾了*人工智能*的发展历程,从17世纪笛卡尔的构想到20世纪中叶图灵测试的提出,再到达特茅斯会议的*人工智能*定义。文章详细介绍了*AI*在商业、游戏、自动驾驶等领域的应用,以及近年来*深度学习*和无监督学习的重大突破。 部署运行你感兴趣的模型镜像一键部署 *人工智能**发展史* ...](https://blog.csdn.net/gravitylink/article/details/86089639)

[*人工智能**发展史*\_*人工智能*的夏天](https://blog.csdn.net/carolynlmk/article/details/75043875)

1-17

[Judea Pearl发表于1988年的名著将概率论和决策理论引入*AI*。现已投入应用的新工具包括贝叶斯网络,隐马尔可夫模型,信息论,随机模型和经典优化理论。针对*神经网络*和进化算法等“计算智能”范式的精确数学描述也被发展出来。 大数据*:*2005 - 现在 从某种意义上讲,2005年是大数据元年,虽然大部分人感受不到数据带来的变化,但是...](https://blog.csdn.net/carolynlmk/article/details/75043875)

[*Ai**发展史*(个人理解)梳理](https://blog.csdn.net/q6115759/article/details/130200753)

[记录 IT 领域经验与见解的博客](https://blog.csdn.net/q6115759)

04-17

2355

[在21世纪初期，随着计算机硬件的不断提升和大规模数据的出现，*深度学习*成为*人工智能*领域的热门研究方向。总之，*人工智能*是一项非常重要的技术，将对我们的生活和工作产生深远的影响。随着*人工智能*应用场景的不断增多，*人工智能*将更加个性化和定制化，可以根据不同用户的需求提供不同的服务。4. *人工智能*将更加智能和自主。随着*人工智能*技术的不断进步，*人工智能*将更加智能和自主，可以自主学习和决策，提高*人工智能*的效率和智能性。随着*人工智能*应用场景的不断增多，*人工智能*将更加注重安全和隐私，保护用户的数据和信息安全。](https://blog.csdn.net/q6115759/article/details/130200753)

[*人工智能**发展史*](https://blog.csdn.net/JIA_NG_FA_N/article/details/139538850)

[JIA\_NG\_FA\_N的博客](https://blog.csdn.net/JIA_NG_FA_N)

06-08

7819

[起步发展期：1943年—20世纪60年代反思发展期：20世纪70年代应用发展期：20世纪80年代平稳发展期：20世纪90年代—2010年蓬勃发展期：2011年至今。](https://blog.csdn.net/JIA_NG_FA_N/article/details/139538850)

[最全科普｜万字长文论*人工智能*的前世今生（下篇）](https://blog.csdn.net/GentelAi/article/details/146201083)

[GentelAi的博客](https://blog.csdn.net/GentelAi)

03-12

1306

[到1976年，MYCIN的开发工作基本完成，其诊断准确率达到65%-70%，甚至超过了一些人类医生的表现，成为*人工智能*领域的里程碑。1980年，美国数字设备公司（DEC）开发了XCON（eXpert CONfigurer），这是一个用于配置计算机系统的专家系统，成功帮助公司自动化复杂的计算机配置流程，显著降低了配置错误和成本，成为专家系统商业化的成功案例。Word2Vec的提出不仅显著提升了自然语言处理（NLP）任务的性能，也为后续的语言模型（如BERT和GPT）奠定了基础，成为NLP领域的里程碑。](https://blog.csdn.net/GentelAi/article/details/146201083)

[一文了解*大模型*：*AI*（*人工智能*）的发展历程](https://devpress.csdn.net/v1/article/detail/141417916)

[m0\_56255097的博客](https://blog.csdn.net/m0_56255097)

08-23

4429

[*AI**大模型*作为*人工智能*领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住*AI**大模型*的风口，掌握*AI**大模型*的知识和技能将变得越来越重要。学习*AI**大模型*是一个系统的过程，需要从基础开始，逐步深入到更高级的技术。这里给大家精心整理了一份全面的*AI**大模型*学习资源，包括：*AI**大模型*全套学习路线图（从入门到实战）、精品*AI**大模型*学习书籍手册、视频教程、实战学习、面试题等，资料免费分享！](https://devpress.csdn.net/v1/article/detail/141417916)

[看 *人工智能**简史*](https://devpress.csdn.net/v1/article/detail/79386740)

[我相信......](https://blog.csdn.net/wireless_com)

02-25

2819

[这个春节有些心神不定，只得靠读书和学习平复心情。《*人工智能**简史*》去年很火，在京东的销售榜中也很考前，未能免俗，自己抽空读了一遍，随记随想。（图片来自百度百科）过去只是序幕。*人工智能*缘起达特茅斯会议，在...](https://devpress.csdn.net/v1/article/detail/79386740)

[*AI**发展史*：从图灵机到*AI*大时代](https://devpress.csdn.net/v1/article/detail/139102029)

[m0\_59164304的博客](https://blog.csdn.net/m0_59164304)

05-21

5806

[*AI*无疑是近年来最热门的话题了，它以一种前所末有的速度影响我们的生活。然而,*AI*的发展历程并非一蹴而就,它经历了漫长的探索和曲折。本期,我们将回顾*AI*的发展历程。](https://devpress.csdn.net/v1/article/detail/139102029)

[*人工智能*与*深度学习*发展*简史*：从感知器到多模态*大模型*的技术演进](https://wenku.csdn.net/doc/68jgfmwztf)

[*人工智能*（*AI*）与*深度学习*的*发展史*，是一部融合数学、计算机科学、神经科学、认知心理学与工程实践的宏大叙事，其演进不仅体现了人类对智能本质的持续追问，更深刻重塑了技术范式、产业格局与社会运行逻辑。...](https://wenku.csdn.net/doc/68jgfmwztf)

[*人工智能*发展*简史*：从1943年M-P模型到21世纪*深度学习*爆发](https://wenku.csdn.net/doc/7utf0qazuj)

[资源摘要信息*:**人工智能*发展*简史*1所涵盖的知识点，系统性地勾勒出*人工智能*学科从思想萌芽到学科正式确立的关键演进脉络，其核心在于揭示人类如何在数学、逻辑学、神经生理学、计算机科学与认知科学的交叉融合中，逐步...](https://wenku.csdn.net/doc/7utf0qazuj)

[*AI**发展史*：从*神经网络*到*大模型*的演进之路](https://wenku.csdn.net/doc/706v1gz262)

[本文以“*AI* *简史*：从*神经元*到*现代**大模型*”为题，系统梳理了从早期人工*神经网络*到当前主流*深度学习*架构的关键节点，涵盖了从理论奠基到实际应用的完整脉络，并提供了可运行的源码示例，使得开发者不仅能理解原理，还...](https://wenku.csdn.net/doc/706v1gz262)

[【引言与*神经网络*基础】*人工智能**简史*：从逻辑机器到*深度学习*的演变](https://wenku.csdn.net/column/2fcd64w82r)

[[【引言与*神经网络*基础】*人工智能**简史*：从逻辑机器到*深度学习*的演变](https*:*//online.visual-paradigm.com/repository/images/06393536-dbad-4462-982f-7661c65029ea/timeline-diagram-design/.png) # 1. *人工智能*...](https://wenku.csdn.net/column/2fcd64w82r)

[*人工智能*发展*简史*：从图灵机、达特茅斯会议到*神经网络*与控制论的演进](https://wenku.csdn.net/doc/2qeezgdjyz)

[而麦克洛奇与皮茨于1943年提出的MP*神经元*模型，则是首次用数学微分方程与阈值逻辑模拟生物*神经元*电生理活动的开创性尝试，它虽高度简化，却构建起连接主义范式的原始框架，成为后世感知机、反向传播算法及深度神经...](https://wenku.csdn.net/doc/2qeezgdjyz)

[赫布理论](https://blog.csdn.net/qq_31374615/article/details/48623221)

[qq\_31374615的博客](https://blog.csdn.net/qq_31374615)

09-21

3958

[赫布理论
本词条缺少名片图，补充相关内容使词条更完整，还能快速升级，赶紧来编辑吧！
赫布理论（英语：Hebbian theory）描述了突触可塑性的基本原理，即突触前*神经元*向突触后*神经元*的持续重复的刺激可以导致突触传递效能的增加。这一理论由唐纳德·赫布于1949年提出，又被称为赫布定律（Hebb's
rule）、赫布假说（Hebb's postulate）、细胞结集理论（cel](https://blog.csdn.net/qq_31374615/article/details/48623221)

[*人工智能**简史*\_*人工智能**简史*](https://devpress.csdn.net/v1/article/detail/107257372)

[科技博客的分析“工具人”](https://blog.csdn.net/cxq8989)

07-10

387

[*人工智能**简史* 在*人工智能*的早期，计算机科学家试图在计算机中重建人类思维的各个方面。 这就是科幻小说中的智力类型，即或多或少像我们一样思考的机器。 毫无疑问，这种类型的智能称为可理解性。 具有可理解性的计算机可用于探索我们如何推理，学习，判断，感知和执行脑力活动。
可懂度的早期研究集中于在计算机中对现实世界和思维（来自认知科学家的领域）的部分进行建模。 当您考虑到这些实验是在60年前进行的时...](https://devpress.csdn.net/v1/article/detail/107257372)

[*人工智能*（*AI*）的发展历程](https://devpress.csdn.net/v1/article/detail/141714348)

[juzhi14plus的博客](https://blog.csdn.net/juzhi14plus)

08-30

2797

[综上所述，人类在创造*人工智能*这一新物种的过程中，必须从伦理道德、法律监管、技术创新、教育和培训等方面进行应对，以确保*人工智能*的发展符合人类的利益和价值观，为人类社会的发展带来更多的机遇和福祉。十年后的 1966 年，麻省理工学院的约瑟夫・魏泽鲍姆开发了一款名为 ELIZA 的聊天机器人，这款机器人能够与人类进行简单的对话，为以后突破人类与机器之间的沟通障碍迈出了重要一步。*人工智能*的发展给人类带来了巨大的机遇和挑战，人类在创造这一新物种的过程中，必须采取积极有效的措施进行应对。](https://devpress.csdn.net/v1/article/detail/141714348)

[四张图片道清*AI**大模型*的*发展史*(1943-2023)

热门推荐](https://keziyi.blog.csdn.net/article/details/132310317)

[weixin\_47567401的博客](https://blog.csdn.net/weixin_47567401)

08-16

1万+

[快速了解大规模语言模型的发展历程](https://keziyi.blog.csdn.net/article/details/132310317)

* [关于我们](//www.csdn.net/company/index.html#about)
* [招贤纳士](//www.csdn.net/company/index.html#recruit)
* [商务合作](https://fsc-p05.txscrm.com/T8PN8SFII7W)
* [寻求报道](//marketing.csdn.net/questions/Q2202181748074189855)
* 400-660-0108
* [kefu@csdn.net](mailto:webmaster@csdn.net)
* [在线客服](https://csdn.s2.udesk.cn/im_client/?web_plugin_id=29181)
* 工作时间 8:30-22:00

* [公安备案号11010502030143](http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010502030143)
* [京ICP备19004658号](http://beian.miit.gov.cn/publish/query/indexFirst.action)
* [京网文〔2020〕1039-165号](https://csdnimg.cn/release/live_fe/culture_license.png)
* [经营性网站备案信息](https://csdnimg.cn/cdn/content-toolbar/csdn-ICP.png)
* [北京互联网违法和不良信息举报中心](http://www.bjjubao.org/)
* [家长监护](https://download.csdn.net/tutelage/home)
* [网络110报警服务](https://cyberpolice.mps.gov.cn/)
* [中国互联网举报中心](http://www.12377.cn/)
* [Chrome商店下载](https://chrome.google.com/webstore/detail/csdn%E5%BC%80%E5%8F%91%E8%80%85%E5%8A%A9%E6%89%8B/kfkdboecolemdjodhmhmcibjocfopejo?hl=zh-CN)
* [账号管理规范](https://blog.csdn.net/blogdevteam/article/details/126135357)
* [版权与免责声明](https://www.csdn.net/company/index.html#statement)
* [版权申诉](https://blog.csdn.net/blogdevteam/article/details/90369522)
* [出版物许可证](https://img-home.csdnimg.cn/images/20250103023206.png)
* [营业执照](https://img-home.csdnimg.cn/images/20250103023201.png)
* ©1999-2026北京创新乐知网络技术有限公司

登录后您可以享受以下权益：

* 免费复制代码
* 和博主大V互动
* 下载海量资源
* 发动态/写文章/加入社区

×

评论 2

被折叠的  条评论
[为什么被折叠?](https://blogdev.blog.csdn.net/article/details/122245662)
[到【灌水乐园】发言](https://bbs.csdn.net/forums/FreeZone)

查看更多评论

添加红包

发出的红包

JarodYv

¥1
¥2
¥4
¥6
¥10
¥20

扫码支付：¥1

您的余额不足，请更换扫码支付或[充值](https://i.csdn.net/#/wallet/balance/recharge?utm_source=RewardVip)

打赏作者

实付元

扫码支付

钱包余额
0

1.余额是钱包充值的虚拟货币，按照1:1的比例进行支付金额的抵扣。  
 2.余额无法直接购买下载，可以购买VIP、付费专栏及课程。

[余额充值](https://i.csdn.net/#/wallet/balance/recharge)

确定取消

举报

* 包含不实信息
* 涉及个人隐私

请选择具体原因（必选）

* 侮辱谩骂
* 诽谤

请选择具体原因（必选）

* 搬家样式
* 博文样式

[点击体验  
DeepSeekR1满血版](https://ai.csdn.net/chat?utm_source=cknow_pc_blogdetail&spm=1001.2101.3001.10583) 
专业的中文 IT 技术社区，与千万技术人共成长
客服
返回顶部