# AI 简史：从神经元到现代大模型 - CSDN博客

**URL**:
https://blog.csdn.net/jarodyv/article/details/144699658

## 元数据
- 发布日期: 2024-12-25T00:00:00+00:00

## 完整内容
---
# AI 简史：从神经元到现代大模型

原创已于 2024-12-25 16:28:52 修改·1.4w 阅读

·46

·86
·

CC 4.0 BY-SA版权

版权声明：本文为博主原创文章，遵循 [CC 4.0 BY-NC-SA] 版权协议，转载请附上原文出处链接和本声明。

文章标签：

[#深度学习] [#人工智能] [#ai] [#神经网络] [#transformer] [#卷积神经网络] [#机器学习] 

于 2024-12-25 10:54:28 首次发布

[生成AI专栏收录该内容] 

45 篇文章

订阅专栏

## AI 简史：从神经元到现代大模型

人工智能 (AI) 和深度学习 (DL) 在过去的几十年中飞速发展，推动了计算机视觉、自然语言处理和机器人等领域的进步。今年的诺贝尔物理学奖更是颁给了美国科学家约翰·霍普菲尔德 (John Hopfield）和英国科学家杰弗里·辛顿（Geoffrey Hinton），表彰他们“在人工神经网络机器学习方面的基础性发现和发明”。本文将为大家概述 AI 的发展历程，梳理出从早期神经网络模型到现代大型语言模型发展过程中的重要里程碑。

图 1\. AI 发展全景图

#### 文章目录

- [1\. 人工智能诞生 (1956)] 
- [2\. AI 的演进：从基于规则的系统到深度神经网络] 
- [3\. 早期人工神经网络 (1940s – 1960s)] 
 - [3.1 McCulloch-Pitts 神经元 (1943)] 
 - [3.2 Rosenblatt 感知机模型 (1957)] 
 - [3.3 ADALINE (1959)] 
 - [3.4 异或（XOR）问题 (1969)] 
- [4\. 多层感知机 (1960)] 
 - [4.1 隐藏层 (Hidden Layers)] 
 - [4.2 多层感知机的历史背景与挑战] 
- [5\. 反向传播 (1970s – 1980s)] 
 - [5.1 早期发展 (1970 年代)] 
 - [5.2 强化与普及（1980 年代）] 
 - [5.3 通用逼近定理 (1989)] 
 - [5.4 第二次黄金时代 (1980 年代末 – 1990 年代初)] 
 - [5.5 第二次黑暗时代 (1990 年代初 – 2000 年代初)] 
 - [深度学习的复兴 (2000 年代末 – 现在)] 
- [6\. 卷积神经网络 (1980s – 2010s)] 
 - [6.1 早期发展 (1980 – 1998)] 
 - [6.2 CNN 的崛起：AlexNet (2012)] 
 - [6.3 AlexNet 开启神经网络的第三次黄金时代（2010 年代至今）] 
 - [6.4 后续架构改进] 
 - [6.5 CNN 的应用] 
- [7\. 循环神经网络 (1986 – 2017)] 
 - [7.1 早期发展 (1980s – 1990s)] 
 - [7.2 LSTM, GRU 和 Seq2Seq 模型 (1997 – 2014)] 
 - [7.3 RNN 的应用] 
 - [7.4 RNN 的挑战] 
- [8\. Transformer (2017 – 现在)] 
 - [8.1 Transformer 简介] 
 - [8.2 Transformer 的衍生模型] 
 - [8.3 OpenAI GPT 的发展历程] 
 - [8.4 其他知名大语言模型] 
- [9\. 多模态模型 (2023 – 现在)] 
 - [9.1 GPT-4V (2023) 和 GPT-4o (2024)] 
 - [9.2 Google’s Gemini (2023 – 现在)] 
 - [9.3 Claude 3.0 和 Claude 3.5 (2023 – 现在)] 
 - [9.4 LLaVA (2023)] 
- [10\. 扩散模型 (2015 – 现在)] 
 - [10.1 扩散模型简介 (2015)] 
 - [10.2 扩散模型的发展 (2020 – 现在)] 
 - [10.3 文生图模型] 
 - [10.4 文生视频模型] 
- [11\. 尾声] 

### 1\. 人工智能诞生 (1956)

人工智能（AI）的概念由来已久，但现代 AI 的雏形是在 20 世纪中期逐渐形成的。“人工智能”这个术语是由计算机科学家和认知科学家约翰·麦卡锡 (John McCarthy) 在 1956 年召开的达特茅斯人工智能夏季研讨项目上首次提出并被大家接受，AI 从此走上历史舞台。

图 2.
[A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence] (1955)

达特茅斯会议通常被视为 AI 研究的发源地。这次会议汇聚了计算机科学家、数学家和认知科学家，共同探讨创造能够模拟人类智能的机器的可能性。与会者中大佬云集，包括：

- **约翰·麦卡锡 (John McCarthy)** ：计算机科学家、Lisp 编程语言发明人之一。

- **马文·明斯基 (Marvin Minsky)**：计算机科学家、框架理论的创立者。

- **雷·索洛莫诺夫 (Ray Solomonoff)**：算法概率论创始人，通用概率分布之父，通用归纳推理理论的创建者。

- **纳撒尼尔·罗切斯特 (Nathaniel Rochester)** ：IBM 701 的首席设计师，编写了世界上第一个汇编程序。

- **克劳德·香农 (Claude Shannon)** ：数学家、发明家、密码学家，信息论创始人。

- **奥利弗·塞弗里奇 (Oliver Selfridge)**：模式识别的奠基人、人工智能的先驱，被誉为“机器知觉之父”。


图 3\. 参加达特茅斯会议的部分重量级人物

达特茅斯会议对计算机科学的发展产生了深远的影响，它为计算机科学的发展指明了方向，推动了计算机科学的快速发展。会议的成果为后来的计算机科学研究提供了重要的思想和方法支持，为计算机科学的教育和培训提供了重要的参考。达特茅斯会议也为跨学科合作和交流提供了一个成功的范例，为后来的跨学科研究提供了重要的经验和启示。

### 2\. AI 的演进：从基于规则的系统到深度神经网络

纵观整个 AI 的发展史，有一条清晰的发展脉络，那就是从基于规则的系统向深度神经网络的不断进化。

人工智能 (AI) 的发展始于上个世纪 50 年代，那时人们开始开发用于国际象棋和问题求解的算法。第一个 AI 程序 Logical Theorist 于 1956 年诞生。到了 1960 和 1970 年代，基于规则的专家系统如 MYCIN 被引入，它们可以帮助进行复杂的决策。1980 年代，机器学习开始兴起，使 AI 系统能够从数据中学习并不断改进，为现代深度学习技术奠定了基础。

今天，大多数最前沿的 AI 技术都由深度学习驱动，深刻改变了 AI 的发展格局。深度学习是机器学习的一个独立分支，它通过多层人工神经网络从原始数据中提取复杂特征。在本文中，我们将探讨 AI 的发展历史，并重点介绍深度学习在其中的关键作用。

图 4\. 人工智能、机器学习、神经网络、深度学习之间的关系

### 3\. 早期人工神经网络 (1940s – 1960s)

#### 3.1 McCulloch-Pitts 神经元 (1943)

神经网络的概念可以追溯到 1943 年，当时 Warren McCulloch 和 Walter Pitts 提出了第一个人工神经元模型。McCulloch-Pitts (MP) 神经元模型是对生物神经元的一种突破性简化。这个模型通过聚合二进制输入，并利用阈值激活函数来做出决策，从而为人工神经网络奠定了基础，输出结果为二进制 {
0
,
1
}
\\{0, 1\\}
{0,1}。

图 5\. 人工神经元的结构与原理

这种简化的模型抓住了神经元行为的核心特征——接收多个输入，整合这些输入，并根据是否超过阈值来产生二进制输出。尽管MP神经元模型非常简单，但它能够实现基本的逻辑运算，展示了神经计算的潜力。

#### 3.2 Rosenblatt 感知机模型 (1957)

Frank Rosenblatt 在 1957 年引入了感知机，这是一种能够学习和识别模式的单层神经网络。感知机模型比 MP 神经元更为通用，设计用于处理实数值输入，并通过调整权重来最小化分类错误。

图 6\. 感知机模型

Rosenblatt 还为感知机开发了一种监督学习算法，使得网络能够直接从训练数据中进行学习。 L
(
W
)
=
−
∑
i
∈
M
W
T
X
i
y
i
\\mathcal{L}(W) = - \\sum\_{i \\in M} W^T X\_i y\_i
L(W)=−i∈M∑​WTXi​yi​

图 7\. Mark I 感知机，是一台实现了图像识别感知机算法的机器

Rosenblatt 的感知机展示出识别个人和在不同语言间翻译语音的潜力，这在当时引发了公众对 AI 的极大兴趣。感知机模型及其相关的学习算法成为神经网络发展历程中的重要里程碑。然而，很快就显现出一个关键限制：当训练数据是非线性可分时，感知机的学习规则无法收敛。

#### 3.3 ADALINE (1959)

Widrow 和 Hoff 在 1959 年引入了 ADALINE（自适应线性神经元，也称 Delta 学习规则），对感知机学习规则进行了改进。ADALINE 解决了二进制输出和噪声敏感性等限制，并能够学习并收敛非线性可分的数据，这是神经网络发展中的一大突破。

图 8\. ADALINE VS. 感知机

ADALINE 的主要特点包括：

- **线性激活函数**：不同于感知器的阶跃函数，ADALINE 使用线性激活函数，因此适用于回归任务和连续输出。
- **最小均方（LMS）算法**：ADALINE 采用 LMS 算法，该算法通过最小化预测输出与实际输出之间的均方误差，提供更高效和稳定的学习过程。
- **自适应权重**：LMS 算法根据输出误差自适应调整权重，使 ADALINE 即使在有噪声的情况下也能有效地学习和收敛。

**ADALINE 的引入标志着神经网络第一次黄金时代的开始**，它克服了 Rosenblatt 感知机学习的限制。这一突破实现了高效学习、连续输出和对噪声数据的适应能力，推动了该领域的创新和快速发展。

图 9\. ADALINE 开启了神经网络第一次黄金时代

然而，与感知机类似，ADALINE 仍然无法解决线性可分的问题，无法应对更复杂的非线性任务。这一局限集中体现在异或（XOR）问题上，也促进了更高级神经网络架构的发展。

#### 3.4 异或（XOR）问题 (1969)

1969年，Marvin Minsky 和 Seymour Papert 在他们的著作《Perceptrons》中揭示了单层感知机的一个重要局限：由于其线性决策边界，感知机无法解决异或 (XOR) 问题，而这是一个简单的二元分类任务。异或问题不是线性可分的，也就是说，没有一个单一的线性边界能够正确地将所有的输入模式分类。

图 10\. Marvin Minsky 和 Seymour Papert 合著的《Perceptrons: An introduction to computational geometry》

这一发现强调了需要开发更复杂的神经网络架构，以便能够学习非线性的决策边界。感知机的局限性被揭露后，人们对神经网络的信心减弱，转而研究符号人工智能方法， **这标志着从 20 世纪 70 年代初到 80 年代中期的“神经网络的第一次黑暗时代”的开始**。

图 11\. 异或问题将神经网络代入第一次黑暗时代

然而，研究人员从解决异或问题中获得的见解促使他们意识到需要更复杂的模型来捕捉非线性关系。这种认识最终推动了多层感知机和其他先进神经网络模型的发展，为神经网络和深度学习在后来的复兴奠定了基础。

### 4\. 多层感知机 (1960)

多层感知机 (MLP) 最早于 20 世纪 60 年代提出，作为对单层感知机的改进。MLP 由多个层次的相互连接的神经元组成，能够克服单层模型的局限性。苏联科学家 A. G. Ivakhnenko 和 V. Lapa 在感知机基础上进行研究，对多层感知机的发展中做出了重要贡献。

图 12\. 多层感知机模型

#### 4.1 隐藏层 (Hidden Layers)

增加隐藏层使得 MLP (多层感知器) 可以捕捉和表达数据中的复杂非线性关系。这些隐藏层极大地增强了网络的学习能力，使其能够解决诸如异或问题这样非线性可分的问题。

图 13\. 隐藏层解决异或问题

#### 4.2 多层感知机的历史背景与挑战

MLP 的出现标志着神经网络的研究向前迈出了重大一步，展示了深度学习架构在解决复杂问题方面的潜力。然而，在 1960 年代和 1970 年代，MLP 的发展面临若干挑战：

- **缺乏训练算法**：早期的 MLP 模型缺乏高效的训练算法，无法有效地调整网络权重。此时反向传播算法还未诞生，训练多层深度网络非常困难。
- **算力限制**：当时的算力不足以应对训练深度神经网络所需的复杂计算。这一限制拖慢了 MLP 的研究和发展进程。

神经网络的第一个黑暗时代在 1986 年结束， **随着反向传播算法的诞生，开启了神经网络的第二个黄金时代**。

### 5\. 反向传播 (1970s – 1980s)

1969 年，异或问题揭示了感知机（单层神经网络）的局限性。研究人员意识到，多层神经网络能够克服这些限制，但缺乏有效的训练算法。17年后，反向传播算法的开发使得神经网络在理论上可以逼近任何函数。值得注意的是，该算法实际上在发表之前就已被发明。如今，反向传播已成为深度学习的核心组件，自 20 世纪 60 年代和70 年代以来经历了显著的发展和完善。

图 14\. 反向传播原理示意图

反向传播的关键特性：

- **梯度下降**：反向传播与梯度下降联合使用以降低误差函数。该算法计算每个权重相对于误差的梯度，从而逐步调整权重以减少误差。
- **链式法则**：反向传播算法的核心在于应用微积分的链式法则。此法则使得误差的梯度可以被分解为一系列偏导数，并通过网络的反向传递高效计算。
- **分层计算**：反向传播逐层运作，从输出层向输入层反向传递。这种分层计算确保梯度在网络中正确传播，使得深度架构的训练成为可能。

#### 5.1 早期发展 (1970 年代)

- **Seppo Linnainmaa (1970)**: 提出了自动微分的概念，这是反向传播算法的重要组成部分。
- **Paul Werbos (1974)**: 提议使用微积分的链式法则计算误差函数对网络权重的梯度，从而能够训练多层神经网络。

#### 5.2 强化与普及（1980 年代）

- **David Rumelhart, Geoffrey Hinton 和 Ronald Williams (1986)**: 将 **反向传播** 这一高效实用的方法，用于训练深度神经网络，并展示了其在多种问题中的应用。

图 15\. 反向传播算法的三位主要贡献者

其中 Geoffrey Hinton 因其在人工神经网络和机器学习领域的贡献获得了 2018 年图灵奖和 2024 诺贝尔物理学奖，称为继 Herbert Simon 后第二位图灵奖-诺贝尔奖双料得主。

#### 5.3 通用逼近定理 (1989)

George Cybenko 在 1989 年提出的通用逼近定理，为多层神经网络的功能提供了数学基础。该定理表明，只要神经元数量足够，并且使用非线性激活函数，具有单个隐藏层的前馈神经网络就能够以任意精度逼近任意连续函数。这个定理突显了神经网络的强大能力和灵活性，使其能够应用于各种领域。

图 16\. 具有单个隐藏层的神经网络可以将任意连续函数逼近到任意所需的精度，从而在各个领域解决复杂的问题

#### 5.4 第二次黄金时代 (1980 年代末 – 1990 年代初)

\*\*反向传播算法的出现和通用逼近定理的提出，开启了神经网络研究的第二个黄金时代。\*\*反向传播提供了一种高效的多层神经网络训练方法，使研究人员能够构建更深层次和更复杂的模型。通用逼近定理则为使用多层神经网络提供了理论支持，并增强了人们对其解决复杂问题能力的信心。在 1980 年代末至 1990 年代初，这一时期见证了对神经网络领域的兴趣回升和显著的进步。

图 17\. 反向传播和通用逼近定理开启了神经网络研究的第二个黄金时代

#### 5.5 第二次黑暗时代 (1990 年代初 – 2000 年代初)

然而，由于一系列因素，神经网络领域在 1990 年代初至 2000 年代初经历了“第二个黑暗时代”：

- **支持向量机 (SVM) 的兴起**：支持向量机为分类和回归任务提供了更优雅的数学方法。
- **算力限制**：由于训练深度神经网络仍然耗时且对硬件要求高，计算能力受到限制。
- **过拟合和泛化问题**：这两个问题导致早期神经网络在训练数据上表现良好，但在新数据上表现不佳，限制了其实用性。

这些挑战使得许多研究人员转而关注其他领域，导致神经网络研究的停滞。

图 18\. 随着 SVM 的兴起，神经网络进入第二个黑暗时代

#### 深度学习的复兴 (2000 年代末 – 现在)

在 2000 年代末和 2010 年代初，神经网络领域经历了复兴，这得益于以下方面的进步：

- **深度学习架构的发展**（如 CNNs、RNNs、Transformers、Diffusion Models）
- **硬件的改进**（如 GPUs、TPUs、LPUs）
- **大规模数据集的可用性**（如 ImageNet、COCO、OpenWebText、WikiText 等）
- **训练算法的优化**（如 SGD、Adam、dropout）

这些进展带来了计算机视觉、自然语言处理、语音识别和强化学习的重大突破。通用逼近定理与实际技术的进步相结合，为深度学习技术的广泛应用和成功奠定了基础。

### 6\. 卷积神经网络 (1980s – 2010s)

卷积神经网络 (CNN) 在深度学习领域，尤其是计算机视觉和图像处理方面，带来了革命性的变化。从上个世纪 80 年代到本世纪最初的 10 年，CNN 在架构、训练技术和应用等方面取得了显著的进步。

卷积神经网络由以下三个主要组件构成：

- **卷积层 (Convolutional Layers)**：这些层通过一组可调整的滤波器，从输入图像中自动学习和提取特征的空间层次结构。
- **池化层 (Pooling Layers)**：池化层通过缩小输入的空间尺寸，来提高对输入变化的适应性，并减少计算量。
- **全连接层 (Fully Connected Layers)**：在卷积层和池化层之后，全连接层用于分类任务，负责整合之前层中提取的特征。

卷积神经网络的主要特性

- **局部感受野**：CNN 利用局部感受野来捕捉输入数据中的局部特征，使其在处理图像和其他视觉任务时表现出色。
- **权重共享**：通过在卷积层中共享权重，CNN 能够减少网络中参数的数量，从而提高训练效率。
- **平移不变性**：池化层赋予网络平移不变性，使其能够识别输入图像中不同位置的相同模式。

#### 6.1 早期发展 (1980 – 1998)

1980 年代，福岛邦彦 (Kunihiko Fukushima) 首次提出了 CNN 的概念，他设计了一种称为神经认知机 (Neocognitron) 的分层神经网络，这种网络模仿了人类视觉皮层的结构。这项开创性的研究为之后 CNN 的发展奠定了基础。

图 19\. 福岛邦彦与他的神经认知机

到了 1980 年代末和 1990 年代初，Yann LeCun 和他的团队在此基础上进一步发展了 CNN，并推出了 LeNet-5 架构，该架构专为手写数字识别而设计。

图 20\. Yann LeCun 与他的 LeNet-5

#### 6.2 CNN 的崛起：AlexNet (2012)

2012 年，AlexNet 在 ImageNet 大规模视觉识别挑战赛（ILSVRC）中取得了重大胜利，这是 CNN 发展中的一个重要里程碑。这次胜利不仅以压倒性优势赢得了比赛，也在图像分类领域取得了重大突破。

图 21\. ILSVRC 历年冠军及其表现

ILSVRC 是一个年度图像识别基准测试，用于评估算法在一个包含 1000 万多张注释图像的数据集上的表现，这些图像被划分为 1000 个类别。AlexNet 的创新之处包括：

- **ReLU 激活函数**：为解决传统激活函数的问题而引入，ReLU 提高了训练速度并改善了性能。
- **Dropout 正则化**：这种技术通过在训练过程中随机丢弃神经元来减少过拟合现象。
- **数据增强**：通过人为增加训练数据的多样性，增强了数据集的丰富性，从而改善了模型的泛化能力。

AlexNet 的成功成为 CNN 发展中的一个转折点，为图像分类和物体检测的进一步发展奠定了基础。

图 22\. AlexNet 架构

#### 6.3 AlexNet 开启神经网络的第三次黄金时代（2010 年代至今）

自 2010 年代直至今天，当前的科技发展黄金时代以深度学习、大数据和强大计算平台的结合为特征。在这一时期，图像识别、自然语言处理和机器人技术等领域取得了显著的突破。持续的研究不断推动着人工智能（AI）能力的边界。

图 23\. AlexNet 开启神经网络的第三次黄金时代

#### 6.4 后续架构改进

继 AlexNet 之后，又相继出现了几个有影响力的架构：

- **VGGNet (2014)**：由牛津大学的视觉几何组开发，VGGNet 强调使用更深的网络架构，并采用较小的卷积滤波器 (
3
×
3
3 \\times 3
3×3)，从而取得了显著的准确率。
图 24\. 原始 VGGNet 架构
- **GoogLeNet/Inception (2014)**：引入了 inception 模块，使得网络能够以更高效的方式捕捉不同尺度的特征。
图 25\. GooLeNet 架构
- **ResNet (2015)**：


---
*数据来源: Exa搜索 | 获取时间: 2026-02-02 20:35:42*